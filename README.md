# Machine Learning

Understanding modern LLMs from first principles. ~30 interlinked documents covering mathematical foundations through transformers and modern training techniques.

## Documents

### Mathematical Foundations
| Document | Topic |
|----------|-------|
| [Linear Algebra](math-foundations/linear-algebra.md) | Vectors, matrices, eigenvalues, PCA, embeddings |
| [Calculus](math-foundations/calculus.md) | Derivatives, chain rule, gradients, Jacobians |
| [Probability](math-foundations/probability.md) | Distributions, Bayes, entropy, softmax, cross-entropy |
| [Optimization](math-foundations/optimization.md) | Gradient descent, convexity, saddle points, momentum |
| [Information Theory](math-foundations/information-theory.md) | KL divergence, mutual information, loss functions |

### Neural Networks
| Document | Topic |
|----------|-------|
| [Perceptron](neural-networks/perceptron.md) | Single neuron, linear classifiers, decision boundaries |
| [Multilayer Networks](neural-networks/multilayer-networks.md) | Universal approximation, depth vs width |
| [Backpropagation](neural-networks/backpropagation.md) | Chain rule, computational graphs, automatic differentiation |
| [Activation Functions](neural-networks/activation-functions.md) | Sigmoid, ReLU, GELU, vanishing gradients |
| [Regularization](neural-networks/regularization.md) | Dropout, weight decay, batch normalization |

### Architectures
| Document | Topic |
|----------|-------|
| [CNNs](architectures/cnns.md) | Convolution, pooling, translation invariance |
| [RNNs](architectures/rnns.md) | Sequential processing, hidden states, vanishing gradients |
| [LSTMs](architectures/lstms.md) | Gates, memory cells, gradient flow |
| [Sequence-to-Sequence](architectures/sequence-to-sequence.md) | Encoder-decoder, the road to attention |

### Transformers
| Document | Topic |
|----------|-------|
| [Attention](transformers/attention.md) | Dot-product attention, query-key-value |
| [Self-Attention](transformers/self-attention.md) | Attending to yourself, positional encoding |
| [Multi-Head Attention](transformers/multi-head-attention.md) | Parallel attention heads, different subspaces |
| [Transformer Architecture](transformers/transformer-architecture.md) | Full stack: embeddings to output |
| [BERT](transformers/bert.md) | Encoder-only, masked language modeling, fine-tuning |
| [GPT](transformers/gpt.md) | Decoder-only, autoregressive language models |

### Training
| Document | Topic |
|----------|-------|
| [Loss Functions](training/loss-functions.md) | Cross-entropy, perplexity, contrastive losses |
| [Optimizers](training/optimizers.md) | SGD, Adam, learning rate schedules |
| [Batching](training/batching.md) | Mini-batch, gradient accumulation |
| [Distributed Training](training/distributed-training.md) | Data parallel, model parallel, ZeRO |

### Modern LLMs
| Document | Topic |
|----------|-------|
| [Pretraining](modern-llms/pretraining.md) | Next-token prediction at scale |
| [Fine-tuning](modern-llms/fine-tuning.md) | Task adaptation, catastrophic forgetting |
| [RLHF](modern-llms/rlhf.md) | Reward models, PPO, alignment |
| [Instruction Tuning](modern-llms/instruction-tuning.md) | Supervised fine-tuning, chat formatting |
| [Efficient Adaptation](modern-llms/efficient-adaptation.md) | LoRA, adapters, prompt tuning |
| [Inference](modern-llms/inference.md) | KV caching, speculative decoding, quantization |

## Notebooks

Runnable implementations progressing from NumPy to PyTorch:

| Notebook | Focus | Status |
|----------|-------|--------|
| [01-numpy-neural-net](notebooks/01-numpy-neural-net.ipynb) | 2-layer net from scratch | âœ… |
| [02-backprop-from-scratch](notebooks/02-backprop-from-scratch.ipynb) | Manual gradient computation | âœ… |
| [03-activations-visualized](notebooks/03-activations-visualized.ipynb) | Plot activations, dead ReLU | ğŸ“‹ |
| [04-optimization-landscape](notebooks/04-optimization-landscape.ipynb) | Visualize SGD, Adam | ğŸ“‹ |
| [05-rnn-from-scratch](notebooks/05-rnn-from-scratch.ipynb) | Character-level RNN | ğŸ“‹ |
| [06-lstm-from-scratch](notebooks/06-lstm-from-scratch.ipynb) | LSTM cell implementation | ğŸ“‹ |
| [07-attention-from-scratch](notebooks/07-attention-from-scratch.ipynb) | Dot-product attention | ğŸ“‹ |
| [08-minimal-transformer](notebooks/08-minimal-transformer.ipynb) | Single-layer transformer | ğŸ“‹ |
| [09-minimal-gpt](notebooks/09-minimal-gpt.ipynb) | Train tiny GPT on Shakespeare | ğŸ“‹ |
| [10-fine-tuning-basics](notebooks/10-fine-tuning-basics.ipynb) | Fine-tune on classification | ğŸ“‹ |
| [11-lora-from-scratch](notebooks/11-lora-from-scratch.ipynb) | Implement LoRA | ğŸ“‹ |
| [12-kv-cache-demo](notebooks/12-kv-cache-demo.ipynb) | KV caching speedup | ğŸ“‹ |

## Folder Structure

```
machine-learning/
â”œâ”€â”€ math-foundations/       # Linear algebra, calculus, probability, optimization
â”œâ”€â”€ neural-networks/        # Perceptrons, backprop, activations, regularization
â”œâ”€â”€ architectures/          # CNNs, RNNs, LSTMs (historical context)
â”œâ”€â”€ transformers/           # Attention, self-attention, BERT, GPT
â”œâ”€â”€ training/               # Loss functions, optimizers, distributed
â”œâ”€â”€ modern-llms/            # Pretraining, RLHF, LoRA, inference
â””â”€â”€ notebooks/              # Runnable implementations
```

## How Documents Connect

```
MATH FOUNDATIONS                           NEURAL NETWORKS                    TRANSFORMERS â†’ LLMs
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Linear Algebra â”€â”€â”¬â”€â”€â–º Perceptron â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Multilayer Networks
                 â”‚         â”‚                        â”‚
Calculus â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â–º Backpropagation â—„â”€â”€â”€â”˜
                 â”‚                    â”‚
Probability â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â–º Loss Functions
                 â”‚                    â”‚          â”‚
Optimization â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â–º Optimizers
                                             â”‚
Information Theory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                                             â–¼
                 CNNs â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Activation Functions
                   â”‚                         â”‚
                   â”‚              Regularization
                   â”‚                         â”‚
                   â””â”€â”€â”€â”€â–º RNNs â”€â”€â–º LSTMs â”€â”€â”€â”€â”¤
                                    â”‚        â”‚
                         Seq2Seq â—„â”€â”€â”˜        â”‚
                            â”‚                â”‚
                            â–¼                â–¼
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚       ATTENTION         â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                      Self-Attention + Positional Encoding
                                 â”‚
                      Multi-Head Attention
                                 â”‚
                      Transformer Architecture
                            â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
                            â–¼         â–¼
                          BERT       GPT
                            â”‚         â”‚
                            â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                                 â–¼
                           Pretraining
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼            â–¼            â–¼
              Fine-tuning      RLHF    Instruction Tuning
                    â”‚            â”‚            â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â–¼
                      Efficient Adaptation (LoRA)
                                 â”‚
                              Inference
```

## Reading Order

**New to ML math?** Start with foundations:
- [Linear Algebra](math-foundations/linear-algebra.md) â†’ [Calculus](math-foundations/calculus.md) â†’ [Probability](math-foundations/probability.md) for the core math
- [Optimization](math-foundations/optimization.md) for gradient descent intuition

**Ready for neural networks?** Choose a track:

| Track | Path |
|-------|------|
| **Quickstart** | Linear Algebra â†’ Perceptron â†’ Backprop â†’ Attention â†’ Transformer Architecture â†’ GPT |
| **Foundations-first** | All math-foundations â†’ Neural networks in order â†’ Architectures â†’ Transformers |
| **Practitioner deep-dive** | Attention â†’ Transformer Architecture â†’ GPT â†’ All modern-llms |
| **Historical** | Perceptron â†’ CNNs â†’ RNNs â†’ LSTMs â†’ Seq2Seq â†’ Attention (the research arc) |

## Code Examples

Documents include inline code snippets showing math translated to NumPy/PyTorch:

```python
def attention(Q, K, V):
    """The core attention mechanism in 4 lines."""
    d_k = K.shape[-1]
    scores = Q @ K.T / np.sqrt(d_k)  # similarity
    weights = softmax(scores)         # normalize
    return weights @ V                # weighted sum
```

Notebooks provide complete, runnable implementations for hands-on learning.

## Rendering

GitHub renders LaTeX natively. For local viewing, use VS Code with a math-preview extension.

Example: $\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$
