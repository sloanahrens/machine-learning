{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal Transformer from Scratch\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Build a complete transformer encoder block in PyTorch\n",
    "2. Implement multi-head attention, feedforward network, and layer normalization\n",
    "3. Add sinusoidal positional encoding\n",
    "4. Train on simple sequence tasks (copy, reverse)\n",
    "5. Visualize attention patterns during training\n",
    "\n",
    "**Prerequisites:** [transformer-architecture](../transformers/transformer-architecture.md), [multi-head-attention](../transformers/multi-head-attention.md)\n",
    "\n",
    "**Key Insight:** A transformer block is just: `x + Attention(LayerNorm(x))` followed by `x + FFN(LayerNorm(x))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Positional Encoding\n",
    "\n",
    "Transformers have no inherent notion of position. We add sinusoidal encodings:\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}})$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{model}})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Register as buffer (not a parameter, but saved with model)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Visualize positional encoding\n",
    "pe = PositionalEncoding(d_model=64, max_len=100, dropout=0.0)\n",
    "encoding = pe.pe[0, :50, :].numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.imshow(encoding, cmap='RdBu', aspect='auto')\n",
    "plt.xlabel('Dimension')\n",
    "plt.ylabel('Position')\n",
    "plt.title('Sinusoidal Positional Encoding')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-Head Attention\n",
    "\n",
    "Multiple attention heads learn different patterns:\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n",
    "\n",
    "where each head is: $\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = np.sqrt(self.d_k)\n",
    "        \n",
    "        # Store attention weights for visualization\n",
    "        self.attn_weights = None\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, d_model)\n",
    "            mask: (seq_len, seq_len) or None\n",
    "        Returns:\n",
    "            output: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        Q = self.W_q(x)  # (batch, seq_len, d_model)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # Reshape for multi-head: (batch, n_heads, seq_len, d_k)\n",
    "        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Compute attention scores: (batch, n_heads, seq_len, seq_len)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # Softmax to get attention weights\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Store for visualization\n",
    "        self.attn_weights = attn_weights.detach()\n",
    "        \n",
    "        # Apply attention to values: (batch, n_heads, seq_len, d_k)\n",
    "        context = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Concatenate heads: (batch, seq_len, d_model)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        # Final projection\n",
    "        output = self.W_o(context)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test multi-head attention\n",
    "mha = MultiHeadAttention(d_model=64, n_heads=8)\n",
    "x = torch.randn(2, 10, 64)  # batch=2, seq_len=10, d_model=64\n",
    "out = mha(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(f\"Attention weights shape: {mha.attn_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Position-wise Feed-Forward Network\n",
    "\n",
    "Two linear layers with activation in between:\n",
    "\n",
    "$$\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "The inner dimension is typically 4x the model dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise feed-forward network.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, d_ff=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        \n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        x = self.linear1(x)\n",
    "        x = F.gelu(x)  # GELU is common in modern transformers\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "# Test feed-forward\n",
    "ff = FeedForward(d_model=64)\n",
    "x = torch.randn(2, 10, 64)\n",
    "out = ff(x)\n",
    "print(f\"FFN input shape: {x.shape}\")\n",
    "print(f\"FFN output shape: {out.shape}\")\n",
    "print(f\"FFN parameters: {sum(p.numel() for p in ff.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transformer Encoder Block\n",
    "\n",
    "A single transformer block combines attention and FFN with residual connections and layer norm:\n",
    "\n",
    "```\n",
    "x -> LayerNorm -> MultiHeadAttention -> + -> LayerNorm -> FFN -> + -> output\n",
    "     |____________________________|        |_________________|\n",
    "              residual                          residual\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Single transformer encoder block.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads, d_ff=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Pre-norm architecture (more stable training):\n",
    "        x = x + Attention(LayerNorm(x))\n",
    "        x = x + FFN(LayerNorm(x))\n",
    "        \"\"\"\n",
    "        # Self-attention with residual\n",
    "        attn_out = self.attention(self.norm1(x), mask)\n",
    "        x = x + self.dropout1(attn_out)\n",
    "        \n",
    "        # Feed-forward with residual\n",
    "        ff_out = self.feed_forward(self.norm2(x))\n",
    "        x = x + self.dropout2(ff_out)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test transformer block\n",
    "block = TransformerBlock(d_model=64, n_heads=8)\n",
    "x = torch.randn(2, 10, 64)\n",
    "out = block(x)\n",
    "print(f\"Block input shape: {x.shape}\")\n",
    "print(f\"Block output shape: {out.shape}\")\n",
    "print(f\"Block parameters: {sum(p.numel() for p in block.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Complete Transformer Encoder\n",
    "\n",
    "Stack multiple blocks with embedding and output layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Complete transformer encoder.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers, d_ff=None, \n",
    "                 max_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output layer\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.output = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len) token indices\n",
    "            mask: optional attention mask\n",
    "        Returns:\n",
    "            logits: (batch_size, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        # Embed tokens and add positional encoding\n",
    "        x = self.embedding(x) * np.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "        \n",
    "        # Final norm and output projection\n",
    "        x = self.norm(x)\n",
    "        logits = self.output(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Test complete model\n",
    "model = TransformerEncoder(\n",
    "    vocab_size=100,\n",
    "    d_model=64,\n",
    "    n_heads=4,\n",
    "    n_layers=2,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "x = torch.randint(0, 100, (2, 10))  # batch=2, seq_len=10\n",
    "logits = model(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {logits.shape}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Task: Sequence Copy\n",
    "\n",
    "Train the transformer to copy input sequences. A simple task that tests whether the model can learn to pass information through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CopyDataset(Dataset):\n",
    "    \"\"\"Dataset for sequence copying task.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, seq_len, num_samples):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "        # Reserve 0 for padding, 1 for separator\n",
    "        self.data_vocab_size = vocab_size - 2\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Generate random sequence (values 2 to vocab_size-1)\n",
    "        seq = torch.randint(2, self.vocab_size, (self.seq_len,))\n",
    "        \n",
    "        # Input: [seq, SEP, zeros]\n",
    "        # Target: [zeros, zeros, seq]\n",
    "        sep = torch.tensor([1])  # Separator token\n",
    "        padding = torch.zeros(self.seq_len, dtype=torch.long)\n",
    "        \n",
    "        input_seq = torch.cat([seq, sep, padding])\n",
    "        target_seq = torch.cat([padding, torch.tensor([0]), seq])\n",
    "        \n",
    "        return input_seq, target_seq\n",
    "\n",
    "# Create dataset\n",
    "vocab_size = 20\n",
    "seq_len = 8\n",
    "dataset = CopyDataset(vocab_size, seq_len, num_samples=10000)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Show example\n",
    "x, y = dataset[0]\n",
    "print(f\"Input:  {x.tolist()}\")\n",
    "print(f\"Target: {y.tolist()}\")\n",
    "print(f\"\\nThe model should learn to copy the first {seq_len} tokens after the separator.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_x, batch_y in dataloader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(batch_x)\n",
    "        \n",
    "        # Compute loss (only on target positions, not padding)\n",
    "        # Reshape for cross entropy\n",
    "        logits_flat = logits.view(-1, logits.size(-1))\n",
    "        targets_flat = batch_y.view(-1)\n",
    "        \n",
    "        # Mask out padding positions (target == 0)\n",
    "        mask = targets_flat != 0\n",
    "        loss = F.cross_entropy(logits_flat[mask], targets_flat[mask])\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Compute accuracy\n",
    "        preds = logits_flat.argmax(dim=-1)\n",
    "        correct += (preds[mask] == targets_flat[mask]).sum().item()\n",
    "        total += mask.sum().item()\n",
    "    \n",
    "    return total_loss / len(dataloader), correct / total\n",
    "\n",
    "def test_model(model, dataset, device, n_samples=5):\n",
    "    \"\"\"Test the model on a few examples.\"\"\"\n",
    "    model.train(False)  # Set to inference mode\n",
    "    \n",
    "    print(\"\\nTest examples:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        x, y = dataset[i]\n",
    "        x = x.unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(x)\n",
    "            preds = logits.argmax(dim=-1)\n",
    "        \n",
    "        x_list = x[0].cpu().tolist()\n",
    "        y_list = y.tolist()\n",
    "        p_list = preds[0].cpu().tolist()\n",
    "        \n",
    "        # Extract the relevant part (after separator)\n",
    "        sep_idx = seq_len\n",
    "        input_seq = x_list[:sep_idx]\n",
    "        target_seq = y_list[sep_idx+1:]\n",
    "        pred_seq = p_list[sep_idx+1:]\n",
    "        \n",
    "        match = \"Correct!\" if target_seq == pred_seq else \"Wrong\"\n",
    "        print(f\"Input:  {input_seq}\")\n",
    "        print(f\"Target: {target_seq}\")\n",
    "        print(f\"Output: {pred_seq} ({match})\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = TransformerEncoder(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=64,\n",
    "    n_heads=4,\n",
    "    n_layers=2,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Training\n",
    "n_epochs = 20\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    loss, acc = train_epoch(model, dataloader, optimizer, device)\n",
    "    losses.append(loss)\n",
    "    accuracies.append(acc)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d} | Loss: {loss:.4f} | Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(losses)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(accuracies)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Copy Accuracy')\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Test on examples\n",
    "test_model(model, dataset, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Attention Patterns\n",
    "\n",
    "Let's see what the model learned to attend to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(model, x, layer_idx=0):\n",
    "    \"\"\"Visualize attention weights for a given input.\"\"\"\n",
    "    model.train(False)  # Set to inference mode\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        _ = model(x.unsqueeze(0).to(device))\n",
    "    \n",
    "    # Get attention weights from specified layer\n",
    "    attn = model.blocks[layer_idx].attention.attn_weights[0]  # (n_heads, seq_len, seq_len)\n",
    "    attn = attn.cpu().numpy()\n",
    "    \n",
    "    n_heads = attn.shape[0]\n",
    "    seq_len_full = attn.shape[1]\n",
    "    \n",
    "    # Plot attention for each head\n",
    "    fig, axes = plt.subplots(1, n_heads, figsize=(3 * n_heads, 3))\n",
    "    \n",
    "    for head in range(n_heads):\n",
    "        ax = axes[head] if n_heads > 1 else axes\n",
    "        im = ax.imshow(attn[head], cmap='Blues', aspect='auto')\n",
    "        ax.set_title(f'Head {head}')\n",
    "        ax.set_xlabel('Key')\n",
    "        if head == 0:\n",
    "            ax.set_ylabel('Query')\n",
    "    \n",
    "    plt.suptitle(f'Attention Weights - Layer {layer_idx}')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Visualize on a test example\n",
    "x, y = dataset[0]\n",
    "print(f\"Input sequence: {x.tolist()}\")\n",
    "print(f\"Format: [data tokens] [SEP=1] [padding=0]\")\n",
    "print()\n",
    "\n",
    "# Layer 0 attention\n",
    "visualize_attention(model, x, layer_idx=0)\n",
    "plt.show()\n",
    "\n",
    "# Layer 1 attention\n",
    "visualize_attention(model, x, layer_idx=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Task: Sequence Reversal\n",
    "\n",
    "A harder task - reverse the input sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseDataset(Dataset):\n",
    "    \"\"\"Dataset for sequence reversal task.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, seq_len, num_samples):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.num_samples = num_samples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Generate random sequence\n",
    "        seq = torch.randint(2, self.vocab_size, (self.seq_len,))\n",
    "        \n",
    "        # Reversed sequence\n",
    "        rev_seq = seq.flip(0)\n",
    "        \n",
    "        # Input: [seq, SEP, zeros]\n",
    "        # Target: [zeros, zeros, reversed_seq]\n",
    "        sep = torch.tensor([1])\n",
    "        padding = torch.zeros(self.seq_len, dtype=torch.long)\n",
    "        \n",
    "        input_seq = torch.cat([seq, sep, padding])\n",
    "        target_seq = torch.cat([padding, torch.tensor([0]), rev_seq])\n",
    "        \n",
    "        return input_seq, target_seq\n",
    "\n",
    "# Create reverse dataset\n",
    "reverse_dataset = ReverseDataset(vocab_size, seq_len, num_samples=10000)\n",
    "reverse_dataloader = DataLoader(reverse_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Show example\n",
    "x, y = reverse_dataset[0]\n",
    "print(f\"Input:  {x.tolist()}\")\n",
    "print(f\"Target: {y.tolist()}\")\n",
    "print(f\"\\nThe model should output the first {seq_len} tokens in reverse order.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on reverse task\n",
    "reverse_model = TransformerEncoder(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=64,\n",
    "    n_heads=4,\n",
    "    n_layers=3,  # Slightly deeper for harder task\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(reverse_model.parameters(), lr=1e-3)\n",
    "\n",
    "n_epochs = 30\n",
    "rev_losses = []\n",
    "rev_accuracies = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    loss, acc = train_epoch(reverse_model, reverse_dataloader, optimizer, device)\n",
    "    rev_losses.append(loss)\n",
    "    rev_accuracies.append(acc)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d} | Loss: {loss:.4f} | Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(rev_losses)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Reverse Task - Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(rev_accuracies)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Reverse Task - Accuracy')\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Test reverse task\n",
    "print(\"\\nReverse task test examples:\")\n",
    "print(\"-\" * 50)\n",
    "reverse_model.train(False)  # Set to inference mode\n",
    "\n",
    "for i in range(5):\n",
    "    x, y = reverse_dataset[i]\n",
    "    x_in = x.unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = reverse_model(x_in)\n",
    "        preds = logits.argmax(dim=-1)\n",
    "    \n",
    "    input_seq = x[:seq_len].tolist()\n",
    "    target_seq = y[seq_len+1:].tolist()\n",
    "    pred_seq = preds[0, seq_len+1:].cpu().tolist()\n",
    "    \n",
    "    match = \"Correct!\" if target_seq == pred_seq else \"Wrong\"\n",
    "    print(f\"Input:    {input_seq}\")\n",
    "    print(f\"Expected: {target_seq}\")\n",
    "    print(f\"Got:      {pred_seq} ({match})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention for reverse task\n",
    "x, y = reverse_dataset[0]\n",
    "print(f\"Input: {x[:seq_len].tolist()} (reversed: {x[:seq_len].flip(0).tolist()})\")\n",
    "print()\n",
    "\n",
    "visualize_attention(reverse_model, x, layer_idx=0)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNotice: The model learns to attend to positions in reverse order!\")\n",
    "print(\"Output position i attends strongly to input position (seq_len - 1 - i)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Architecture Summary\n",
    "\n",
    "```\n",
    "TransformerEncoder\n",
    "├── Embedding (vocab_size -> d_model)\n",
    "├── PositionalEncoding (sinusoidal)\n",
    "├── TransformerBlock x N\n",
    "│   ├── LayerNorm\n",
    "│   ├── MultiHeadAttention\n",
    "│   │   ├── Linear (Q projection)\n",
    "│   │   ├── Linear (K projection)\n",
    "│   │   ├── Linear (V projection)\n",
    "│   │   └── Linear (output projection)\n",
    "│   ├── Residual Connection\n",
    "│   ├── LayerNorm\n",
    "│   ├── FeedForward\n",
    "│   │   ├── Linear (d_model -> 4*d_model)\n",
    "│   │   ├── GELU\n",
    "│   │   └── Linear (4*d_model -> d_model)\n",
    "│   └── Residual Connection\n",
    "├── LayerNorm (final)\n",
    "└── Linear (d_model -> vocab_size)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter breakdown\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count parameters in each component.\"\"\"\n",
    "    counts = {}\n",
    "    \n",
    "    counts['embedding'] = model.embedding.weight.numel()\n",
    "    counts['output'] = sum(p.numel() for p in model.output.parameters())\n",
    "    counts['layer_norm'] = sum(p.numel() for p in model.norm.parameters())\n",
    "    \n",
    "    block_params = sum(p.numel() for p in model.blocks.parameters())\n",
    "    counts['transformer_blocks'] = block_params\n",
    "    \n",
    "    # Breakdown per block\n",
    "    block = model.blocks[0]\n",
    "    counts['  - attention'] = sum(p.numel() for p in block.attention.parameters())\n",
    "    counts['  - feed_forward'] = sum(p.numel() for p in block.feed_forward.parameters())\n",
    "    counts['  - norms'] = sum(p.numel() for p in block.norm1.parameters()) + \\\n",
    "                          sum(p.numel() for p in block.norm2.parameters())\n",
    "    \n",
    "    return counts\n",
    "\n",
    "counts = count_parameters(model)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(\"Parameter Breakdown:\")\n",
    "print(\"-\" * 40)\n",
    "for name, count in counts.items():\n",
    "    pct = count / total * 100\n",
    "    print(f\"{name:25s} {count:8,d} ({pct:5.1f}%)\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Total':25s} {total:8,d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|--------|\n",
    "| Positional Encoding | Inject position information |\n",
    "| Multi-Head Attention | Learn multiple attention patterns |\n",
    "| Feed-Forward Network | Non-linear transformation per position |\n",
    "| Layer Normalization | Stabilize training |\n",
    "| Residual Connections | Enable gradient flow |\n",
    "\n",
    "**Key Insights:**\n",
    "1. The transformer is surprisingly simple: attention + FFN + residuals\n",
    "2. Pre-norm (LayerNorm before attention/FFN) is more stable than post-norm\n",
    "3. The model learns task-specific attention patterns (copy = identity, reverse = anti-diagonal)\n",
    "4. Most parameters are in FFN layers, not attention\n",
    "\n",
    "**Next:** [09-minimal-gpt.ipynb](09-minimal-gpt.ipynb) builds a decoder-only (autoregressive) transformer for text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Sorting Task:** Create a dataset where the model must output the input sequence in sorted order. How does this compare to copy/reverse?\n",
    "\n",
    "2. **Causal Masking:** Modify MultiHeadAttention to support causal (autoregressive) masking. Verify that position i cannot attend to positions > i.\n",
    "\n",
    "3. **Depth Ablation:** Train models with 1, 2, 4, and 6 layers on the reverse task. How does depth affect learning speed and final accuracy?\n",
    "\n",
    "4. **Head Analysis:** After training, analyze what each attention head has learned. Do different heads specialize in different patterns?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
