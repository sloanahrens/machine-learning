{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation from Scratch\n",
    "\n",
    "This notebook explores backpropagation in detail. We'll derive gradients by hand, implement them, and verify our understanding with numerical checks.\n",
    "\n",
    "**Goal:** Deep understanding of how gradients flow through a neural network.\n",
    "\n",
    "**Prerequisites:** [backpropagation.md](../neural-networks/backpropagation.md), [calculus.md](../math-foundations/calculus.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Chain Rule\n",
    "\n",
    "Backprop is just the chain rule applied efficiently. Let's start simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example: f(x) = (x + 2)^2\n",
    "# Composed: g(x) = x + 2, f(g) = g^2\n",
    "\n",
    "def f(x):\n",
    "    return (x + 2) ** 2\n",
    "\n",
    "def f_derivative_numerical(x, h=1e-5):\n",
    "    \"\"\"Numerical derivative.\"\"\"\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)\n",
    "\n",
    "def f_derivative_analytical(x):\n",
    "    \"\"\"Analytical derivative via chain rule.\n",
    "    \n",
    "    df/dx = df/dg * dg/dx\n",
    "          = 2g * 1\n",
    "          = 2(x + 2)\n",
    "    \"\"\"\n",
    "    return 2 * (x + 2)\n",
    "\n",
    "# Verify\n",
    "x = 3.0\n",
    "print(f\"f({x}) = {f(x)}\")\n",
    "print(f\"Numerical derivative: {f_derivative_numerical(x):.6f}\")\n",
    "print(f\"Analytical derivative: {f_derivative_analytical(x):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Computational Graphs\n",
    "\n",
    "Neural networks are compositions of simple operations. Let's represent them as graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute: f(x, y, z) = (x + y) * z\n",
    "#\n",
    "# Graph:\n",
    "#   x ───┐\n",
    "#        ├──[+]──→ q ──┐\n",
    "#   y ───┘              ├──[*]──→ f\n",
    "#   z ─────────────────┘\n",
    "#\n",
    "# Forward: q = x + y, then f = q * z\n",
    "# Backward: df/dq = z, df/dz = q\n",
    "#           df/dx = df/dq * dq/dx = z * 1 = z\n",
    "#           df/dy = df/dq * dq/dy = z * 1 = z\n",
    "\n",
    "def forward_graph(x, y, z):\n",
    "    \"\"\"Forward pass through computational graph.\"\"\"\n",
    "    q = x + y        # Addition\n",
    "    f = q * z        # Multiplication\n",
    "    return f, q      # Return intermediate for backprop\n",
    "\n",
    "def backward_graph(x, y, z, q):\n",
    "    \"\"\"Backward pass: compute gradients.\"\"\"\n",
    "    # df/df = 1 (starting point)\n",
    "    df = 1.0\n",
    "    \n",
    "    # f = q * z\n",
    "    dq = df * z      # df/dq = z\n",
    "    dz = df * q      # df/dz = q\n",
    "    \n",
    "    # q = x + y\n",
    "    dx = dq * 1      # dq/dx = 1\n",
    "    dy = dq * 1      # dq/dy = 1\n",
    "    \n",
    "    return dx, dy, dz\n",
    "\n",
    "# Test\n",
    "x, y, z = 2.0, 3.0, 4.0\n",
    "f_val, q = forward_graph(x, y, z)\n",
    "dx, dy, dz = backward_graph(x, y, z, q)\n",
    "\n",
    "print(f\"f = (x + y) * z = ({x} + {y}) * {z} = {f_val}\")\n",
    "print(f\"Gradients: dx={dx}, dy={dy}, dz={dz}\")\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"  df/dx should be z = {z}\")\n",
    "print(f\"  df/dy should be z = {z}\")\n",
    "print(f\"  df/dz should be (x+y) = {x+y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradients of Common Operations\n",
    "\n",
    "Let's derive and verify gradients for operations used in neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x, h=1e-5):\n",
    "    \"\"\"Compute gradient numerically for any function.\"\"\"\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=['multi_index'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        old_val = x[idx]\n",
    "        \n",
    "        x[idx] = old_val + h\n",
    "        f_plus = f(x)\n",
    "        \n",
    "        x[idx] = old_val - h\n",
    "        f_minus = f(x)\n",
    "        \n",
    "        grad[idx] = (f_plus - f_minus) / (2 * h)\n",
    "        x[idx] = old_val\n",
    "        it.iternext()\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiply: z = X @ W\n",
    "# Forward: z_ij = sum_k X_ik * W_kj\n",
    "# Backward: dL/dX_ik = sum_j dL/dz_ij * W_kj = (dL/dz @ W.T)_ik\n",
    "#           dL/dW_kj = sum_i X_ik * dL/dz_ij = (X.T @ dL/dz)_kj\n",
    "\n",
    "class MatMul:\n",
    "    \"\"\"Matrix multiply with gradients.\"\"\"\n",
    "    \n",
    "    def forward(self, X, W):\n",
    "        self.X = X\n",
    "        self.W = W\n",
    "        return X @ W\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"dout is dL/dz.\"\"\"\n",
    "        dX = dout @ self.W.T\n",
    "        dW = self.X.T @ dout\n",
    "        return dX, dW\n",
    "\n",
    "# Verify\n",
    "X = np.random.randn(3, 4)\n",
    "W = np.random.randn(4, 5)\n",
    "\n",
    "matmul = MatMul()\n",
    "z = matmul.forward(X, W)\n",
    "\n",
    "# Pretend upstream gradient is random\n",
    "dout = np.random.randn(*z.shape)\n",
    "dX, dW = matmul.backward(dout)\n",
    "\n",
    "# Numerical check for dW\n",
    "def loss_wrt_W(W_test):\n",
    "    return np.sum((X @ W_test) * dout)  # Surrogate loss\n",
    "\n",
    "dW_numerical = numerical_gradient(loss_wrt_W, W.copy())\n",
    "\n",
    "print(\"Matrix Multiply Gradient Check (dW):\")\n",
    "print(f\"  Max difference: {np.abs(dW - dW_numerical).max():.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU: y = max(0, x)\n",
    "# Gradient: dy/dx = 1 if x > 0 else 0\n",
    "\n",
    "class ReLU:\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return dout * (self.x > 0)\n",
    "\n",
    "# Verify\n",
    "x = np.array([-2, -1, 0, 1, 2], dtype=float)\n",
    "relu = ReLU()\n",
    "y = relu.forward(x)\n",
    "dx = relu.backward(np.ones_like(y))\n",
    "\n",
    "print(\"ReLU:\")\n",
    "print(f\"  x = {x}\")\n",
    "print(f\"  y = ReLU(x) = {y}\")\n",
    "print(f\"  dy/dx = {dx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid: y = 1 / (1 + exp(-x))\n",
    "# Gradient: dy/dx = y * (1 - y)\n",
    "\n",
    "class Sigmoid:\n",
    "    def forward(self, x):\n",
    "        self.y = 1 / (1 + np.exp(-x))\n",
    "        return self.y\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        return dout * self.y * (1 - self.y)\n",
    "\n",
    "# Verify\n",
    "x = np.linspace(-5, 5, 11)\n",
    "sigmoid = Sigmoid()\n",
    "y = sigmoid.forward(x)\n",
    "\n",
    "# Numerical gradient\n",
    "def sigmoid_fn(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "dx_numerical = numerical_gradient(lambda x: sigmoid_fn(x).sum(), x.copy())\n",
    "dx_analytical = sigmoid.backward(np.ones_like(y))\n",
    "\n",
    "print(\"Sigmoid Gradient Check:\")\n",
    "print(f\"  Max difference: {np.abs(dx_analytical - dx_numerical).max():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Softmax + Cross-Entropy\n",
    "\n",
    "This is the most important gradient to understand for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"Numerically stable softmax.\"\"\"\n",
    "    exp_z = np.exp(z - z.max(axis=-1, keepdims=True))\n",
    "    return exp_z / exp_z.sum(axis=-1, keepdims=True)\n",
    "\n",
    "def cross_entropy(probs, y):\n",
    "    \"\"\"Cross-entropy loss. y is integer labels.\"\"\"\n",
    "    n = len(y)\n",
    "    return -np.log(probs[np.arange(n), y] + 1e-10).mean()\n",
    "\n",
    "class SoftmaxCrossEntropy:\n",
    "    \"\"\"\n",
    "    Combined softmax + cross-entropy for efficient gradient.\n",
    "    \n",
    "    The beautiful result: dL/dz = probs - one_hot(y)\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, z, y):\n",
    "        self.probs = softmax(z)\n",
    "        self.y = y\n",
    "        return cross_entropy(self.probs, y)\n",
    "    \n",
    "    def backward(self):\n",
    "        n = len(self.y)\n",
    "        dz = self.probs.copy()\n",
    "        dz[np.arange(n), self.y] -= 1\n",
    "        dz /= n\n",
    "        return dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive and verify the gradient\n",
    "# \n",
    "# Loss: L = -log(p_y) where p = softmax(z)\n",
    "# \n",
    "# For the correct class k = y:\n",
    "#   dL/dz_k = -1/p_y * d(p_y)/dz_k\n",
    "#           = -1/p_y * p_y(1 - p_y)  [softmax derivative for same index]\n",
    "#           = -(1 - p_y)\n",
    "#           = p_y - 1\n",
    "#\n",
    "# For incorrect class j != y:\n",
    "#   dL/dz_j = -1/p_y * d(p_y)/dz_j\n",
    "#           = -1/p_y * (-p_y * p_j)  [softmax derivative for different index]\n",
    "#           = p_j\n",
    "#\n",
    "# Combined: dL/dz = probs - one_hot(y)\n",
    "\n",
    "# Test\n",
    "z = np.array([[1.0, 2.0, 3.0], [3.0, 2.0, 1.0]])\n",
    "y = np.array([2, 0])  # True classes\n",
    "\n",
    "sce = SoftmaxCrossEntropy()\n",
    "loss = sce.forward(z, y)\n",
    "dz = sce.backward()\n",
    "\n",
    "print(f\"Logits z:\\n{z}\")\n",
    "print(f\"\\nProbabilities (softmax):\\n{sce.probs.round(3)}\")\n",
    "print(f\"\\nTrue labels: {y}\")\n",
    "print(f\"Loss: {loss:.4f}\")\n",
    "print(f\"\\nGradient dL/dz:\\n{dz.round(4)}\")\n",
    "\n",
    "# Numerical verification\n",
    "def loss_fn(z_flat):\n",
    "    z_test = z_flat.reshape(z.shape)\n",
    "    return cross_entropy(softmax(z_test), y)\n",
    "\n",
    "dz_numerical = numerical_gradient(loss_fn, z.flatten().copy()).reshape(z.shape)\n",
    "print(f\"\\nNumerical gradient:\\n{dz_numerical.round(4)}\")\n",
    "print(f\"\\nMax difference: {np.abs(dz - dz_numerical).max():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Full Network Backprop\n",
    "\n",
    "Now let's trace gradients through a complete network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "    \"\"\"Linear layer with gradient computation.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        self.W = np.random.randn(in_features, out_features) * np.sqrt(2 / in_features)\n",
    "        self.b = np.zeros(out_features)\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        return X @ self.W + self.b\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        dout: gradient from upstream, shape (batch, out_features)\n",
    "        Returns: gradient for input X, shape (batch, in_features)\n",
    "        \"\"\"\n",
    "        # Parameter gradients\n",
    "        self.dW = self.X.T @ dout\n",
    "        self.db = dout.sum(axis=0)\n",
    "        \n",
    "        # Input gradient (to pass backwards)\n",
    "        dX = dout @ self.W.T\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNetwork:\n",
    "    \"\"\"\n",
    "    Simple two-layer network with explicit gradient computation.\n",
    "    \n",
    "    Architecture: Input → Linear → ReLU → Linear → Softmax → Loss\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        self.linear1 = LinearLayer(input_dim, hidden_dim)\n",
    "        self.relu = ReLU()\n",
    "        self.linear2 = LinearLayer(hidden_dim, output_dim)\n",
    "        self.softmax_ce = SoftmaxCrossEntropy()\n",
    "    \n",
    "    def forward(self, X, y):\n",
    "        \"\"\"Forward pass, returns loss.\"\"\"\n",
    "        # Store shapes for visualization\n",
    "        self.shapes = []\n",
    "        \n",
    "        # Layer 1: Linear\n",
    "        z1 = self.linear1.forward(X)\n",
    "        self.shapes.append(('z1', z1.shape))\n",
    "        \n",
    "        # ReLU\n",
    "        a1 = self.relu.forward(z1)\n",
    "        self.shapes.append(('a1', a1.shape))\n",
    "        \n",
    "        # Layer 2: Linear\n",
    "        z2 = self.linear2.forward(a1)\n",
    "        self.shapes.append(('z2', z2.shape))\n",
    "        \n",
    "        # Softmax + Cross-Entropy Loss\n",
    "        loss = self.softmax_ce.forward(z2, y)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Backward pass, compute all gradients.\"\"\"\n",
    "        # Start from loss\n",
    "        # dL/dz2\n",
    "        dz2 = self.softmax_ce.backward()\n",
    "        \n",
    "        # dL/da1 (through linear2)\n",
    "        da1 = self.linear2.backward(dz2)\n",
    "        \n",
    "        # dL/dz1 (through ReLU)\n",
    "        dz1 = self.relu.backward(da1)\n",
    "        \n",
    "        # dL/dX (through linear1)\n",
    "        dX = self.linear1.backward(dz1)\n",
    "        \n",
    "        return dX\n",
    "    \n",
    "    def get_params_and_grads(self):\n",
    "        \"\"\"Return all parameters and their gradients.\"\"\"\n",
    "        return [\n",
    "            ('W1', self.linear1.W, self.linear1.dW),\n",
    "            ('b1', self.linear1.b, self.linear1.db),\n",
    "            ('W2', self.linear2.W, self.linear2.dW),\n",
    "            ('b2', self.linear2.b, self.linear2.db),\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network and test data\n",
    "net = SimpleNetwork(input_dim=4, hidden_dim=8, output_dim=3)\n",
    "X = np.random.randn(2, 4)  # 2 samples, 4 features\n",
    "y = np.array([0, 2])       # Labels\n",
    "\n",
    "# Forward\n",
    "loss = net.forward(X, y)\n",
    "print(f\"Loss: {loss:.4f}\")\n",
    "print(f\"\\nForward pass shapes:\")\n",
    "for name, shape in net.shapes:\n",
    "    print(f\"  {name}: {shape}\")\n",
    "\n",
    "# Backward\n",
    "dX = net.backward()\n",
    "print(f\"\\ndX shape: {dX.shape}\")\n",
    "\n",
    "# Check gradients\n",
    "print(f\"\\nGradient shapes:\")\n",
    "for name, param, grad in net.get_params_and_grads():\n",
    "    print(f\"  {name}: param {param.shape}, grad {grad.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify gradients numerically\n",
    "def check_gradient(net, X, y, param_name, threshold=1e-5):\n",
    "    \"\"\"Check analytical gradient against numerical.\"\"\"\n",
    "    \n",
    "    # Get the parameter\n",
    "    if param_name == 'W1':\n",
    "        param = net.linear1.W\n",
    "    elif param_name == 'b1':\n",
    "        param = net.linear1.b\n",
    "    elif param_name == 'W2':\n",
    "        param = net.linear2.W\n",
    "    elif param_name == 'b2':\n",
    "        param = net.linear2.b\n",
    "    \n",
    "    # Compute analytical gradient\n",
    "    net.forward(X, y)\n",
    "    net.backward()\n",
    "    \n",
    "    for name, p, g in net.get_params_and_grads():\n",
    "        if name == param_name:\n",
    "            analytical = g.copy()\n",
    "            break\n",
    "    \n",
    "    # Compute numerical gradient\n",
    "    numerical = np.zeros_like(param)\n",
    "    h = 1e-5\n",
    "    \n",
    "    it = np.nditer(param, flags=['multi_index'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        old_val = param[idx]\n",
    "        \n",
    "        param[idx] = old_val + h\n",
    "        loss_plus = net.forward(X, y)\n",
    "        \n",
    "        param[idx] = old_val - h\n",
    "        loss_minus = net.forward(X, y)\n",
    "        \n",
    "        numerical[idx] = (loss_plus - loss_minus) / (2 * h)\n",
    "        param[idx] = old_val\n",
    "        it.iternext()\n",
    "    \n",
    "    # Compare\n",
    "    diff = np.abs(analytical - numerical).max()\n",
    "    status = \"✓\" if diff < threshold else \"✗\"\n",
    "    print(f\"{param_name}: max diff = {diff:.2e} {status}\")\n",
    "    return diff < threshold\n",
    "\n",
    "print(\"Gradient Check:\")\n",
    "check_gradient(net, X, y, 'W1')\n",
    "check_gradient(net, X, y, 'b1')\n",
    "check_gradient(net, X, y, 'W2')\n",
    "check_gradient(net, X, y, 'b2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizing Gradient Flow\n",
    "\n",
    "Let's trace how gradients flow through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deeper network to see gradient flow\n",
    "class DeepNetwork:\n",
    "    def __init__(self, layer_sizes):\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.layers.append(LinearLayer(layer_sizes[i], layer_sizes[i+1]))\n",
    "            if i < len(layer_sizes) - 2:  # No activation on last layer\n",
    "                self.activations.append(ReLU())\n",
    "        \n",
    "        self.softmax_ce = SoftmaxCrossEntropy()\n",
    "    \n",
    "    def forward(self, X, y):\n",
    "        self.z_values = []  # Store pre-activation values\n",
    "        self.a_values = [X]  # Store post-activation values\n",
    "        \n",
    "        h = X\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            z = layer.forward(h)\n",
    "            self.z_values.append(z)\n",
    "            \n",
    "            if i < len(self.activations):\n",
    "                h = self.activations[i].forward(z)\n",
    "                self.a_values.append(h)\n",
    "            else:\n",
    "                h = z\n",
    "        \n",
    "        return self.softmax_ce.forward(h, y)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.gradient_norms = []\n",
    "        \n",
    "        dout = self.softmax_ce.backward()\n",
    "        self.gradient_norms.append(np.linalg.norm(dout))\n",
    "        \n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            dout = self.layers[i].backward(dout)\n",
    "            if i > 0:\n",
    "                dout = self.activations[i-1].backward(dout)\n",
    "            self.gradient_norms.append(np.linalg.norm(dout))\n",
    "        \n",
    "        self.gradient_norms.reverse()\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 5-layer network\n",
    "deep_net = DeepNetwork([10, 20, 20, 20, 20, 5])\n",
    "\n",
    "X = np.random.randn(32, 10)\n",
    "y = np.random.randint(0, 5, 32)\n",
    "\n",
    "loss = deep_net.forward(X, y)\n",
    "deep_net.backward()\n",
    "\n",
    "# Plot gradient norms through the network\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(range(len(deep_net.gradient_norms)), deep_net.gradient_norms)\n",
    "plt.xlabel('Layer (0 = input, last = output)')\n",
    "plt.ylabel('Gradient Norm')\n",
    "plt.title('Gradient Flow Through Network')\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGradient norms by layer:\")\n",
    "for i, norm in enumerate(deep_net.gradient_norms):\n",
    "    print(f\"  Layer {i}: {norm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Vanishing Gradients\n",
    "\n",
    "Let's demonstrate the vanishing gradient problem with sigmoid activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNetworkSigmoid:\n",
    "    \"\"\"Deep network with sigmoid activations (to show vanishing gradients).\"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes):\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.layers.append(LinearLayer(layer_sizes[i], layer_sizes[i+1]))\n",
    "            if i < len(layer_sizes) - 2:\n",
    "                self.activations.append(Sigmoid())  # Using sigmoid!\n",
    "        \n",
    "        self.softmax_ce = SoftmaxCrossEntropy()\n",
    "    \n",
    "    def forward(self, X, y):\n",
    "        h = X\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            z = layer.forward(h)\n",
    "            if i < len(self.activations):\n",
    "                h = self.activations[i].forward(z)\n",
    "            else:\n",
    "                h = z\n",
    "        return self.softmax_ce.forward(h, y)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.gradient_norms = []\n",
    "        \n",
    "        dout = self.softmax_ce.backward()\n",
    "        self.gradient_norms.append(np.linalg.norm(dout))\n",
    "        \n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            dout = self.layers[i].backward(dout)\n",
    "            if i > 0:\n",
    "                dout = self.activations[i-1].backward(dout)\n",
    "            self.gradient_norms.append(np.linalg.norm(dout))\n",
    "        \n",
    "        self.gradient_norms.reverse()\n",
    "\n",
    "# Compare ReLU vs Sigmoid\n",
    "sizes = [10, 50, 50, 50, 50, 50, 50, 50, 5]  # 8 layers\n",
    "\n",
    "relu_net = DeepNetwork(sizes)\n",
    "sigmoid_net = DeepNetworkSigmoid(sizes)\n",
    "\n",
    "X = np.random.randn(32, 10)\n",
    "y = np.random.randint(0, 5, 32)\n",
    "\n",
    "relu_net.forward(X, y)\n",
    "relu_net.backward()\n",
    "\n",
    "sigmoid_net.forward(X, y)\n",
    "sigmoid_net.backward()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(relu_net.gradient_norms, 'b-o', label='ReLU')\n",
    "plt.plot(sigmoid_net.gradient_norms, 'r-s', label='Sigmoid')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Gradient Norm')\n",
    "plt.title('Vanishing Gradients: ReLU vs Sigmoid')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nReLU gradient ratio (first/last): {relu_net.gradient_norms[0]/relu_net.gradient_norms[-1]:.2e}\")\n",
    "print(f\"Sigmoid gradient ratio (first/last): {sigmoid_net.gradient_norms[0]/sigmoid_net.gradient_norms[-1]:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exercises\n",
    "\n",
    "1. **Implement Tanh:** Add a Tanh activation class and verify the gradient.\n",
    "\n",
    "2. **Batch Normalization:** Implement BatchNorm with its gradient.\n",
    "\n",
    "3. **Dropout:** Add dropout during forward pass (with proper scaling).\n",
    "\n",
    "4. **Residual Connection:** Implement a residual block and trace gradients.\n",
    "\n",
    "5. **Different Loss Functions:** Implement MSE loss and its gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Implement Tanh\n",
    "# tanh(x) = (e^x - e^-x) / (e^x + e^-x)\n",
    "# d/dx tanh(x) = 1 - tanh(x)^2\n",
    "\n",
    "class Tanh:\n",
    "    def forward(self, x):\n",
    "        self.y = np.tanh(x)\n",
    "        return self.y\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        # TODO: Implement\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key insights from this notebook:\n",
    "\n",
    "1. **Backprop = Chain Rule:** It's just calculus applied systematically.\n",
    "\n",
    "2. **Local Gradients:** Each operation only needs to know its local gradient.\n",
    "\n",
    "3. **Gradient Flow:** Gradients multiply as they flow backward. Sigmoid squashes gradients (max derivative 0.25), while ReLU preserves them (derivative 1 for positive inputs).\n",
    "\n",
    "4. **Softmax + CE:** The combined gradient is elegantly simple: `probs - one_hot(y)`.\n",
    "\n",
    "5. **Numerical Verification:** Always check gradients during implementation.\n",
    "\n",
    "**Next:** Apply this understanding to build [attention mechanisms](../transformers/attention.md)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
