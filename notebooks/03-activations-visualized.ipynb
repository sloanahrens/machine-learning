{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Activation Functions Visualized\n",
    "\n",
    "This notebook explores activation functions and their impact on neural network training. We'll visualize the functions, their gradients, and demonstrate key problems like vanishing gradients and dead ReLU.\n",
    "\n",
    "**Goal:** Build intuition for why activation function choice matters.\n",
    "\n",
    "**Prerequisites:** [activation-functions.md](../neural-networks/activation-functions.md), [backpropagation.md](../neural-networks/backpropagation.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. The Activation Functions\n",
    "\n",
    "Let's implement and visualize the most common activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Logistic sigmoid: squashes to (0, 1)\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"Hyperbolic tangent: squashes to (-1, 1)\"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"Rectified Linear Unit: max(0, x)\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    \"\"\"Leaky ReLU: allows small negative slope\"\"\"\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def gelu(x):\n",
    "    \"\"\"Gaussian Error Linear Unit: smooth approximation of ReLU\"\"\"\n",
    "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "def swish(x):\n",
    "    \"\"\"Swish/SiLU: x * sigmoid(x)\"\"\"\n",
    "    return x * sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivatives (for backpropagation)\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def leaky_relu_derivative(x, alpha=0.01):\n",
    "    return np.where(x > 0, 1, alpha)\n",
    "\n",
    "def gelu_derivative(x):\n",
    "    # Approximate derivative\n",
    "    cdf = 0.5 * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
    "    pdf = np.exp(-x**2 / 2) / np.sqrt(2 * np.pi)\n",
    "    return cdf + x * pdf\n",
    "\n",
    "def swish_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s + x * s * (1 - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all activation functions\n",
    "x = np.linspace(-4, 4, 400)\n",
    "\n",
    "activations = [\n",
    "    ('Sigmoid', sigmoid, sigmoid_derivative, 'C0'),\n",
    "    ('Tanh', tanh, tanh_derivative, 'C1'),\n",
    "    ('ReLU', relu, relu_derivative, 'C2'),\n",
    "    ('Leaky ReLU', leaky_relu, leaky_relu_derivative, 'C3'),\n",
    "    ('GELU', gelu, gelu_derivative, 'C4'),\n",
    "    ('Swish', swish, swish_derivative, 'C5'),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "\n",
    "for ax, (name, func, deriv, color) in zip(axes.flat, activations):\n",
    "    ax.plot(x, func(x), color=color, linewidth=2, label=f'{name}')\n",
    "    ax.plot(x, deriv(x), color=color, linewidth=2, linestyle='--', alpha=0.6, label=f'{name} derivative')\n",
    "    ax.axhline(y=0, color='gray', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='gray', linewidth=0.5)\n",
    "    ax.set_xlim(-4, 4)\n",
    "    ax.set_ylim(-1.5, 2.5)\n",
    "    ax.set_title(name, fontsize=12)\n",
    "    ax.legend(loc='upper left', fontsize=9)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "\n",
    "plt.suptitle('Activation Functions and Their Derivatives', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 2. The Vanishing Gradient Problem\n",
    "\n",
    "Sigmoid and tanh have derivatives that approach zero for large inputs. In deep networks, gradients multiply through layers, causing them to vanish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate vanishing gradients through multiple layers\n",
    "\n",
    "def gradient_through_layers(activation_deriv, n_layers, input_value):\n",
    "    \"\"\"\n",
    "    Simulate gradient flow through n layers.\n",
    "    \n",
    "    Gradient = derivative(layer_n) * derivative(layer_n-1) * ... * derivative(layer_1)\n",
    "    \"\"\"\n",
    "    gradient = 1.0\n",
    "    gradients = [gradient]\n",
    "    \n",
    "    # Assume each layer receives similar input\n",
    "    for _ in range(n_layers):\n",
    "        gradient *= activation_deriv(input_value)\n",
    "        gradients.append(gradient)\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "# Compare gradient flow for different activations\n",
    "n_layers = 20\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: gradients at x=0 (best case for sigmoid/tanh)\n",
    "ax = axes[0]\n",
    "for name, _, deriv, color in activations:\n",
    "    grads = gradient_through_layers(deriv, n_layers, 0)\n",
    "    ax.plot(grads, marker='o', markersize=3, color=color, label=name)\n",
    "\n",
    "ax.set_xlabel('Layer depth')\n",
    "ax.set_ylabel('Gradient magnitude')\n",
    "ax.set_title('Gradient Flow (x = 0, best case)')\n",
    "ax.legend()\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim(1e-10, 10)\n",
    "\n",
    "# Right: gradients at x=2 (common case)\n",
    "ax = axes[1]\n",
    "for name, _, deriv, color in activations:\n",
    "    grads = gradient_through_layers(deriv, n_layers, 2)\n",
    "    ax.plot(grads, marker='o', markersize=3, color=color, label=name)\n",
    "\n",
    "ax.set_xlabel('Layer depth')\n",
    "ax.set_ylabel('Gradient magnitude')\n",
    "ax.set_title('Gradient Flow (x = 2, common case)')\n",
    "ax.legend()\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim(1e-10, 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"After 20 layers with x=2:\")\n",
    "print(f\"  Sigmoid gradient: {gradient_through_layers(sigmoid_derivative, 20, 2)[-1]:.2e}\")\n",
    "print(f\"  Tanh gradient:    {gradient_through_layers(tanh_derivative, 20, 2)[-1]:.2e}\")\n",
    "print(f\"  ReLU gradient:    {gradient_through_layers(relu_derivative, 20, 2)[-1]:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "**Key insight:** Sigmoid and tanh gradients shrink exponentially with depth. ReLU maintains gradients for positive inputs, enabling training of very deep networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 3. The Dead ReLU Problem\n",
    "\n",
    "ReLU neurons can \"die\" during training: if their input is always negative, the gradient is always zero and the neuron never updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNetwork:\n",
    "    \"\"\"Simple network to demonstrate dead ReLU.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, activation='relu'):\n",
    "        self.W1 = np.random.randn(input_dim, hidden_dim) * 0.5\n",
    "        self.b1 = np.zeros(hidden_dim)\n",
    "        self.W2 = np.random.randn(hidden_dim, 1) * 0.5\n",
    "        self.b2 = np.zeros(1)\n",
    "        \n",
    "        if activation == 'relu':\n",
    "            self.activation = relu\n",
    "            self.activation_deriv = relu_derivative\n",
    "        elif activation == 'leaky_relu':\n",
    "            self.activation = leaky_relu\n",
    "            self.activation_deriv = leaky_relu_derivative\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {activation}\")\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.a1 = self.activation(self.z1)\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        return self.z2\n",
    "    \n",
    "    def backward(self, y):\n",
    "        batch_size = len(y)\n",
    "        dz2 = (self.z2 - y) / batch_size\n",
    "        \n",
    "        dW2 = self.a1.T @ dz2\n",
    "        db2 = dz2.sum(axis=0)\n",
    "        \n",
    "        da1 = dz2 @ self.W2.T\n",
    "        dz1 = da1 * self.activation_deriv(self.z1)\n",
    "        \n",
    "        dW1 = self.X.T @ dz1\n",
    "        db1 = dz1.sum(axis=0)\n",
    "        \n",
    "        return {'W1': dW1, 'b1': db1, 'W2': dW2, 'b2': db2}\n",
    "    \n",
    "    def update(self, grads, lr):\n",
    "        self.W1 -= lr * grads['W1']\n",
    "        self.b1 -= lr * grads['b1']\n",
    "        self.W2 -= lr * grads['W2']\n",
    "        self.b2 -= lr * grads['b2']\n",
    "    \n",
    "    def count_dead_neurons(self, X):\n",
    "        \"\"\"Count neurons that output zero for all inputs.\"\"\"\n",
    "        self.forward(X)\n",
    "        # A neuron is \"dead\" if it's zero for all samples\n",
    "        dead = (self.a1.max(axis=0) == 0)\n",
    "        return dead.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple regression problem\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(1000, 10)\n",
    "y = (X[:, 0] * 2 + X[:, 1] - X[:, 2] * 0.5 + np.random.randn(1000) * 0.1).reshape(-1, 1)\n",
    "\n",
    "# Train with ReLU - track dead neurons\n",
    "model_relu = SimpleNetwork(10, 100, activation='relu')\n",
    "dead_counts_relu = []\n",
    "\n",
    "# Train with Leaky ReLU for comparison\n",
    "model_leaky = SimpleNetwork(10, 100, activation='leaky_relu')\n",
    "# Copy initial weights\n",
    "model_leaky.W1 = model_relu.W1.copy()\n",
    "model_leaky.b1 = model_relu.b1.copy()\n",
    "model_leaky.W2 = model_relu.W2.copy()\n",
    "model_leaky.b2 = model_relu.b2.copy()\n",
    "dead_counts_leaky = []\n",
    "\n",
    "# Use high learning rate to induce dead neurons\n",
    "lr = 0.5\n",
    "epochs = 200\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # ReLU\n",
    "    pred = model_relu.forward(X)\n",
    "    grads = model_relu.backward(y)\n",
    "    model_relu.update(grads, lr)\n",
    "    dead_counts_relu.append(model_relu.count_dead_neurons(X))\n",
    "    \n",
    "    # Leaky ReLU\n",
    "    pred = model_leaky.forward(X)\n",
    "    grads = model_leaky.backward(y)\n",
    "    model_leaky.update(grads, lr)\n",
    "    dead_counts_leaky.append(model_leaky.count_dead_neurons(X))\n",
    "\n",
    "# Plot dead neuron counts\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(dead_counts_relu, label='ReLU', color='C2')\n",
    "plt.plot(dead_counts_leaky, label='Leaky ReLU', color='C3')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Number of Dead Neurons (out of 100)')\n",
    "plt.title('Dead ReLU Problem: Neurons That Never Activate')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final dead neurons - ReLU: {dead_counts_relu[-1]}, Leaky ReLU: {dead_counts_leaky[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "**Key insight:** High learning rates or poor initialization can push ReLU neurons into the negative regime permanently. Leaky ReLU prevents this by allowing small gradients even for negative inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 4. Activation Statistics During Training\n",
    "\n",
    "Let's watch how activations evolve during training and how different functions affect the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic classification data\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X_class, y_class = make_classification(\n",
    "    n_samples=1000, n_features=20, n_informative=10,\n",
    "    n_redundant=5, n_classes=2, random_state=42\n",
    ")\n",
    "y_class = y_class.reshape(-1, 1).astype(float)\n",
    "\n",
    "def train_and_record_activations(X, y, activation_name, epochs=100):\n",
    "    \"\"\"Train a network and record activation statistics.\"\"\"\n",
    "    \n",
    "    activation_funcs = {\n",
    "        'sigmoid': (sigmoid, sigmoid_derivative),\n",
    "        'tanh': (tanh, tanh_derivative),\n",
    "        'relu': (relu, relu_derivative),\n",
    "        'gelu': (gelu, gelu_derivative),\n",
    "    }\n",
    "    \n",
    "    act_func, act_deriv = activation_funcs[activation_name]\n",
    "    \n",
    "    # Simple 2-layer network\n",
    "    np.random.seed(42)\n",
    "    W1 = np.random.randn(20, 50) * np.sqrt(2/20)\n",
    "    b1 = np.zeros(50)\n",
    "    W2 = np.random.randn(50, 1) * np.sqrt(2/50)\n",
    "    b2 = np.zeros(1)\n",
    "    \n",
    "    activation_means = []\n",
    "    activation_stds = []\n",
    "    losses = []\n",
    "    \n",
    "    lr = 0.1\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward\n",
    "        z1 = X @ W1 + b1\n",
    "        a1 = act_func(z1)\n",
    "        z2 = a1 @ W2 + b2\n",
    "        pred = sigmoid(z2)  # Output sigmoid for classification\n",
    "        \n",
    "        # Record activation stats\n",
    "        activation_means.append(a1.mean())\n",
    "        activation_stds.append(a1.std())\n",
    "        \n",
    "        # Loss (binary cross-entropy)\n",
    "        loss = -np.mean(y * np.log(pred + 1e-10) + (1-y) * np.log(1-pred + 1e-10))\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # Backward\n",
    "        dz2 = (pred - y) / len(y)\n",
    "        dW2 = a1.T @ dz2\n",
    "        db2 = dz2.sum(axis=0)\n",
    "        \n",
    "        da1 = dz2 @ W2.T\n",
    "        dz1 = da1 * act_deriv(z1)\n",
    "        dW1 = X.T @ dz1\n",
    "        db1 = dz1.sum(axis=0)\n",
    "        \n",
    "        # Update\n",
    "        W1 -= lr * dW1\n",
    "        b1 -= lr * db1\n",
    "        W2 -= lr * dW2\n",
    "        b2 -= lr * db2\n",
    "    \n",
    "    return {\n",
    "        'means': activation_means,\n",
    "        'stds': activation_stds,\n",
    "        'losses': losses\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with different activations\n",
    "results = {}\n",
    "for name in ['sigmoid', 'tanh', 'relu', 'gelu']:\n",
    "    results[name] = train_and_record_activations(X_class, y_class, name)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "colors = {'sigmoid': 'C0', 'tanh': 'C1', 'relu': 'C2', 'gelu': 'C4'}\n",
    "\n",
    "# Loss curves\n",
    "ax = axes[0]\n",
    "for name, res in results.items():\n",
    "    ax.plot(res['losses'], label=name, color=colors[name])\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training Loss')\n",
    "ax.legend()\n",
    "\n",
    "# Activation means\n",
    "ax = axes[1]\n",
    "for name, res in results.items():\n",
    "    ax.plot(res['means'], label=name, color=colors[name])\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Mean Activation')\n",
    "ax.set_title('Activation Mean Over Training')\n",
    "ax.legend()\n",
    "\n",
    "# Activation stds\n",
    "ax = axes[2]\n",
    "for name, res in results.items():\n",
    "    ax.plot(res['stds'], label=name, color=colors[name])\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Std of Activations')\n",
    "ax.set_title('Activation Spread Over Training')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 5. Comparing Network Training\n",
    "\n",
    "Let's train identical networks with different activations on MNIST and compare convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load MNIST (smaller subset for speed)\n",
    "print(\"Loading MNIST...\")\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "X_mnist, y_mnist = mnist.data[:10000] / 255.0, mnist.target[:10000].astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mnist, y_mnist, test_size=2000, random_state=42)\n",
    "print(f\"Train: {len(X_train)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    exp_z = np.exp(z - z.max(axis=1, keepdims=True))\n",
    "    return exp_z / exp_z.sum(axis=1, keepdims=True)\n",
    "\n",
    "class MNISTNetwork:\n",
    "    \"\"\"Network for comparing activations on MNIST.\"\"\"\n",
    "    \n",
    "    def __init__(self, activation_name='relu', hidden_dim=128):\n",
    "        self.activation_name = activation_name\n",
    "        \n",
    "        activations = {\n",
    "            'sigmoid': (sigmoid, sigmoid_derivative),\n",
    "            'tanh': (tanh, tanh_derivative),\n",
    "            'relu': (relu, relu_derivative),\n",
    "            'leaky_relu': (leaky_relu, leaky_relu_derivative),\n",
    "            'gelu': (gelu, gelu_derivative),\n",
    "        }\n",
    "        self.act, self.act_deriv = activations[activation_name]\n",
    "        \n",
    "        # He initialization for ReLU variants, Xavier for others\n",
    "        if activation_name in ['relu', 'leaky_relu', 'gelu']:\n",
    "            scale = np.sqrt(2 / 784)\n",
    "        else:\n",
    "            scale = np.sqrt(1 / 784)\n",
    "        \n",
    "        np.random.seed(42)\n",
    "        self.W1 = np.random.randn(784, hidden_dim) * scale\n",
    "        self.b1 = np.zeros(hidden_dim)\n",
    "        self.W2 = np.random.randn(hidden_dim, 10) * np.sqrt(2 / hidden_dim)\n",
    "        self.b2 = np.zeros(10)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.a1 = self.act(self.z1)\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        self.probs = softmax(self.z2)\n",
    "        return self.probs\n",
    "    \n",
    "    def backward(self, y):\n",
    "        batch_size = len(y)\n",
    "        \n",
    "        dz2 = self.probs.copy()\n",
    "        dz2[np.arange(batch_size), y] -= 1\n",
    "        dz2 /= batch_size\n",
    "        \n",
    "        dW2 = self.a1.T @ dz2\n",
    "        db2 = dz2.sum(axis=0)\n",
    "        \n",
    "        da1 = dz2 @ self.W2.T\n",
    "        dz1 = da1 * self.act_deriv(self.z1)\n",
    "        \n",
    "        dW1 = self.X.T @ dz1\n",
    "        db1 = dz1.sum(axis=0)\n",
    "        \n",
    "        return {'W1': dW1, 'b1': db1, 'W2': dW2, 'b2': db2}\n",
    "    \n",
    "    def update(self, grads, lr):\n",
    "        self.W1 -= lr * grads['W1']\n",
    "        self.b1 -= lr * grads['b1']\n",
    "        self.W2 -= lr * grads['W2']\n",
    "        self.b2 -= lr * grads['b2']\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        probs = self.forward(X)\n",
    "        preds = np.argmax(probs, axis=1)\n",
    "        return (preds == y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mnist(activation_name, epochs=20, batch_size=64, lr=0.1):\n",
    "    \"\"\"Train on MNIST and return history.\"\"\"\n",
    "    model = MNISTNetwork(activation_name)\n",
    "    \n",
    "    history = {'train_acc': [], 'test_acc': []}\n",
    "    n_batches = len(X_train) // batch_size\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle\n",
    "        indices = np.random.permutation(len(X_train))\n",
    "        X_shuffled = X_train[indices]\n",
    "        y_shuffled = y_train[indices]\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            \n",
    "            model.forward(X_shuffled[start:end])\n",
    "            grads = model.backward(y_shuffled[start:end])\n",
    "            model.update(grads, lr)\n",
    "        \n",
    "        train_acc = model.accuracy(X_train, y_train)\n",
    "        test_acc = model.accuracy(X_test, y_test)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_acc'].append(test_acc)\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Train with each activation\n",
    "print(\"Training networks with different activations...\")\n",
    "histories = {}\n",
    "for name in ['sigmoid', 'tanh', 'relu', 'leaky_relu', 'gelu']:\n",
    "    print(f\"  Training with {name}...\")\n",
    "    histories[name] = train_mnist(name)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors = {'sigmoid': 'C0', 'tanh': 'C1', 'relu': 'C2', 'leaky_relu': 'C3', 'gelu': 'C4'}\n",
    "\n",
    "for name, hist in histories.items():\n",
    "    ax1.plot(hist['train_acc'], label=name, color=colors[name])\n",
    "    ax2.plot(hist['test_acc'], label=name, color=colors[name])\n",
    "\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Training Accuracy')\n",
    "ax1.legend()\n",
    "ax1.set_ylim(0.5, 1.0)\n",
    "\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Test Accuracy')\n",
    "ax2.legend()\n",
    "ax2.set_ylim(0.5, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFinal test accuracies:\")\n",
    "for name, hist in sorted(histories.items(), key=lambda x: -x[1]['test_acc'][-1]):\n",
    "    print(f\"  {name:12s}: {hist['test_acc'][-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 6. Deeper Networks: Where Activation Choice Really Matters\n",
    "\n",
    "With shallow networks, all activations work reasonably well. Let's see what happens with deeper networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNetwork:\n",
    "    \"\"\"Deeper network to show activation function impact.\"\"\"\n",
    "    \n",
    "    def __init__(self, activation_name='relu', n_hidden_layers=5, hidden_dim=64):\n",
    "        self.n_layers = n_hidden_layers + 1  # +1 for output\n",
    "        \n",
    "        activations = {\n",
    "            'sigmoid': (sigmoid, sigmoid_derivative),\n",
    "            'tanh': (tanh, tanh_derivative),\n",
    "            'relu': (relu, relu_derivative),\n",
    "        }\n",
    "        self.act, self.act_deriv = activations[activation_name]\n",
    "        \n",
    "        # Initialize layers\n",
    "        np.random.seed(42)\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        # Input layer\n",
    "        scale = np.sqrt(2 / 784) if activation_name == 'relu' else np.sqrt(1 / 784)\n",
    "        self.weights.append(np.random.randn(784, hidden_dim) * scale)\n",
    "        self.biases.append(np.zeros(hidden_dim))\n",
    "        \n",
    "        # Hidden layers\n",
    "        scale = np.sqrt(2 / hidden_dim) if activation_name == 'relu' else np.sqrt(1 / hidden_dim)\n",
    "        for _ in range(n_hidden_layers - 1):\n",
    "            self.weights.append(np.random.randn(hidden_dim, hidden_dim) * scale)\n",
    "            self.biases.append(np.zeros(hidden_dim))\n",
    "        \n",
    "        # Output layer\n",
    "        self.weights.append(np.random.randn(hidden_dim, 10) * np.sqrt(2 / hidden_dim))\n",
    "        self.biases.append(np.zeros(10))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.activations = [X]\n",
    "        self.pre_activations = []\n",
    "        \n",
    "        current = X\n",
    "        for i, (W, b) in enumerate(zip(self.weights[:-1], self.biases[:-1])):\n",
    "            z = current @ W + b\n",
    "            self.pre_activations.append(z)\n",
    "            current = self.act(z)\n",
    "            self.activations.append(current)\n",
    "        \n",
    "        # Output layer (no activation before softmax)\n",
    "        z = current @ self.weights[-1] + self.biases[-1]\n",
    "        self.pre_activations.append(z)\n",
    "        self.probs = softmax(z)\n",
    "        return self.probs\n",
    "    \n",
    "    def backward(self, y):\n",
    "        batch_size = len(y)\n",
    "        grads = {'W': [], 'b': []}\n",
    "        \n",
    "        # Output layer\n",
    "        dz = self.probs.copy()\n",
    "        dz[np.arange(batch_size), y] -= 1\n",
    "        dz /= batch_size\n",
    "        \n",
    "        for i in range(len(self.weights) - 1, -1, -1):\n",
    "            grads['W'].insert(0, self.activations[i].T @ dz)\n",
    "            grads['b'].insert(0, dz.sum(axis=0))\n",
    "            \n",
    "            if i > 0:\n",
    "                da = dz @ self.weights[i].T\n",
    "                dz = da * self.act_deriv(self.pre_activations[i-1])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def update(self, grads, lr):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= lr * grads['W'][i]\n",
    "            self.biases[i] -= lr * grads['b'][i]\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        probs = self.forward(X)\n",
    "        preds = np.argmax(probs, axis=1)\n",
    "        return (preds == y).mean()\n",
    "    \n",
    "    def get_gradient_norms(self, X, y):\n",
    "        \"\"\"Return gradient norms for each layer.\"\"\"\n",
    "        self.forward(X)\n",
    "        grads = self.backward(y)\n",
    "        return [np.linalg.norm(g) for g in grads['W']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare gradient flow in deep networks\n",
    "print(\"Training deep networks (5 hidden layers)...\\n\")\n",
    "\n",
    "deep_histories = {}\n",
    "gradient_histories = {}\n",
    "\n",
    "for name in ['sigmoid', 'tanh', 'relu']:\n",
    "    print(f\"Training with {name}...\")\n",
    "    model = DeepNetwork(name, n_hidden_layers=5, hidden_dim=64)\n",
    "    \n",
    "    history = {'test_acc': []}\n",
    "    grad_norms = []\n",
    "    \n",
    "    epochs = 20\n",
    "    batch_size = 64\n",
    "    lr = 0.1 if name == 'relu' else 0.5  # Sigmoid/tanh need higher LR\n",
    "    n_batches = len(X_train) // batch_size\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        indices = np.random.permutation(len(X_train))\n",
    "        X_shuffled = X_train[indices]\n",
    "        y_shuffled = y_train[indices]\n",
    "        \n",
    "        epoch_grads = []\n",
    "        for i in range(n_batches):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            \n",
    "            batch_X = X_shuffled[start:end]\n",
    "            batch_y = y_shuffled[start:end]\n",
    "            \n",
    "            model.forward(batch_X)\n",
    "            grads = model.backward(batch_y)\n",
    "            model.update(grads, lr)\n",
    "            \n",
    "            # Record gradient norms periodically\n",
    "            if i == 0:\n",
    "                epoch_grads.append(model.get_gradient_norms(batch_X, batch_y))\n",
    "        \n",
    "        grad_norms.append(np.mean(epoch_grads, axis=0))\n",
    "        test_acc = model.accuracy(X_test, y_test)\n",
    "        history['test_acc'].append(test_acc)\n",
    "    \n",
    "    deep_histories[name] = history\n",
    "    gradient_histories[name] = grad_norms\n",
    "    print(f\"  Final accuracy: {history['test_acc'][-1]:.3f}\")\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy\n",
    "ax = axes[0]\n",
    "for name, hist in deep_histories.items():\n",
    "    ax.plot(hist['test_acc'], label=name, color=colors[name], linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Test Accuracy')\n",
    "ax.set_title('Deep Network (5 Hidden Layers) - Test Accuracy')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Gradient norms per layer (final epoch)\n",
    "ax = axes[1]\n",
    "for name, grads in gradient_histories.items():\n",
    "    final_grads = grads[-1]  # Last epoch\n",
    "    layers = range(1, len(final_grads) + 1)\n",
    "    ax.plot(layers, final_grads, marker='o', label=name, color=colors[name], linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Gradient Norm')\n",
    "ax.set_title('Gradient Magnitude by Layer (Final Epoch)')\n",
    "ax.legend()\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "| Activation | Pros | Cons | Best For |\n",
    "|------------|------|------|----------|\n",
    "| **Sigmoid** | Output in (0,1), smooth | Vanishing gradients, slow | Output layers (binary) |\n",
    "| **Tanh** | Zero-centered, stronger gradients | Still vanishes for large inputs | RNNs (historically) |\n",
    "| **ReLU** | Fast, no vanishing gradient for x>0 | Dead neurons, not zero-centered | Hidden layers (default) |\n",
    "| **Leaky ReLU** | No dead neurons | Slight computational overhead | When dead ReLU is a problem |\n",
    "| **GELU** | Smooth, best empirical results | More computation | Transformers |\n",
    "\n",
    "**Key takeaways:**\n",
    "\n",
    "1. **Vanishing gradients** killed sigmoid/tanh for deep networks\n",
    "2. **ReLU** enabled training of deep networks (2012 breakthrough)\n",
    "3. **Dead ReLU** can be mitigated with Leaky ReLU or careful initialization\n",
    "4. **GELU** is now standard in transformers (GPT, BERT)\n",
    "5. Proper **initialization** matters as much as activation choice\n",
    "\n",
    "**Next:** [04-optimization-landscape.ipynb](04-optimization-landscape.ipynb) visualizes how optimizers navigate loss surfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 8. Exercises\n",
    "\n",
    "1. **Implement ELU:** Add Exponential Linear Unit and compare to ReLU/Leaky ReLU.\n",
    "\n",
    "2. **Batch normalization:** Add batch norm after activations. Does it help sigmoid/tanh in deep networks?\n",
    "\n",
    "3. **Initialization experiment:** Try Xavier vs He initialization with each activation. Which combinations work best?\n",
    "\n",
    "4. **Learning rate sensitivity:** Which activations are most sensitive to learning rate choice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 starter: Implement ELU\n",
    "def elu(x, alpha=1.0):\n",
    "    \"\"\"Exponential Linear Unit\"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "\n",
    "def elu_derivative(x, alpha=1.0):\n",
    "    # Your implementation here\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
