{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network from Scratch with NumPy\n",
    "\n",
    "This notebook builds a complete neural network using only NumPy. No frameworks, no magic—just matrix operations.\n",
    "\n",
    "**Goal:** Train a network to classify handwritten digits (MNIST).\n",
    "\n",
    "**Prerequisites:** [perceptron.md](../neural-networks/perceptron.md), [multilayer-networks.md](../neural-networks/multilayer-networks.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load MNIST Data\n",
    "\n",
    "MNIST contains 70,000 handwritten digits (28×28 pixels each)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "X, y = mnist.data, mnist.target.astype(int)\n",
    "\n",
    "# Normalize to [0, 1]\n",
    "X = X / 255.0\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=10000, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Input shape: {X_train[0].shape}\")\n",
    "print(f\"Classes: {np.unique(y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X_train[i].reshape(28, 28), cmap='gray')\n",
    "    ax.set_title(f\"Label: {y_train[i]}\")\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building Blocks\n",
    "\n",
    "Let's define the core components: activation functions, their derivatives, and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"ReLU activation.\"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    \"\"\"Gradient of ReLU.\"\"\"\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"Softmax activation (numerically stable).\"\"\"\n",
    "    exp_z = np.exp(z - z.max(axis=1, keepdims=True))\n",
    "    return exp_z / exp_z.sum(axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(probs, y):\n",
    "    \"\"\"\n",
    "    Cross-entropy loss.\n",
    "    \n",
    "    Args:\n",
    "        probs: (batch_size, num_classes) softmax probabilities\n",
    "        y: (batch_size,) integer class labels\n",
    "    \"\"\"\n",
    "    batch_size = len(y)\n",
    "    correct_probs = probs[np.arange(batch_size), y]\n",
    "    return -np.log(correct_probs + 1e-10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test softmax\n",
    "z = np.array([[1, 2, 3], [1, 1, 1]])\n",
    "probs = softmax(z)\n",
    "print(\"Softmax output:\")\n",
    "print(probs)\n",
    "print(f\"Row sums: {probs.sum(axis=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Neural Network Class\n",
    "\n",
    "Architecture: Input (784) → Hidden (128, ReLU) → Output (10, Softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    Two-layer neural network for classification.\n",
    "    \n",
    "    Architecture:\n",
    "        Input → Linear → ReLU → Linear → Softmax → Output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Initialize with He initialization for ReLU.\n",
    "        \"\"\"\n",
    "        # Layer 1: input → hidden\n",
    "        self.W1 = np.random.randn(input_dim, hidden_dim) * np.sqrt(2 / input_dim)\n",
    "        self.b1 = np.zeros(hidden_dim)\n",
    "        \n",
    "        # Layer 2: hidden → output\n",
    "        self.W2 = np.random.randn(hidden_dim, output_dim) * np.sqrt(2 / hidden_dim)\n",
    "        self.b2 = np.zeros(output_dim)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass. Store intermediates for backprop.\n",
    "        \n",
    "        Args:\n",
    "            X: (batch_size, input_dim)\n",
    "            \n",
    "        Returns:\n",
    "            probs: (batch_size, output_dim) softmax probabilities\n",
    "        \"\"\"\n",
    "        # Store input for backprop\n",
    "        self.X = X\n",
    "        \n",
    "        # Layer 1\n",
    "        self.z1 = X @ self.W1 + self.b1  # (batch, hidden)\n",
    "        self.a1 = relu(self.z1)           # (batch, hidden)\n",
    "        \n",
    "        # Layer 2\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2  # (batch, output)\n",
    "        self.probs = softmax(self.z2)          # (batch, output)\n",
    "        \n",
    "        return self.probs\n",
    "    \n",
    "    def backward(self, y):\n",
    "        \"\"\"\n",
    "        Backward pass. Compute gradients.\n",
    "        \n",
    "        Args:\n",
    "            y: (batch_size,) integer class labels\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of gradients\n",
    "        \"\"\"\n",
    "        batch_size = len(y)\n",
    "        \n",
    "        # Output layer gradient (softmax + cross-entropy combined)\n",
    "        # dL/dz2 = probs - one_hot(y)\n",
    "        dz2 = self.probs.copy()\n",
    "        dz2[np.arange(batch_size), y] -= 1\n",
    "        dz2 /= batch_size  # Average over batch\n",
    "        \n",
    "        # Layer 2 gradients\n",
    "        dW2 = self.a1.T @ dz2        # (hidden, output)\n",
    "        db2 = dz2.sum(axis=0)        # (output,)\n",
    "        \n",
    "        # Backprop through layer 2\n",
    "        da1 = dz2 @ self.W2.T        # (batch, hidden)\n",
    "        \n",
    "        # Backprop through ReLU\n",
    "        dz1 = da1 * relu_derivative(self.z1)  # (batch, hidden)\n",
    "        \n",
    "        # Layer 1 gradients\n",
    "        dW1 = self.X.T @ dz1         # (input, hidden)\n",
    "        db1 = dz1.sum(axis=0)        # (hidden,)\n",
    "        \n",
    "        return {'W1': dW1, 'b1': db1, 'W2': dW2, 'b2': db2}\n",
    "    \n",
    "    def update(self, grads, learning_rate):\n",
    "        \"\"\"\n",
    "        Update parameters with gradient descent.\n",
    "        \"\"\"\n",
    "        self.W1 -= learning_rate * grads['W1']\n",
    "        self.b1 -= learning_rate * grads['b1']\n",
    "        self.W2 -= learning_rate * grads['W2']\n",
    "        self.b2 -= learning_rate * grads['b2']\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels.\n",
    "        \"\"\"\n",
    "        probs = self.forward(X)\n",
    "        return np.argmax(probs, axis=1)\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute classification accuracy.\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return (predictions == y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Loop\n",
    "\n",
    "Train with mini-batch gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, y_train, X_val, y_val, \n",
    "          epochs=10, batch_size=32, learning_rate=0.1):\n",
    "    \"\"\"\n",
    "    Train the neural network.\n",
    "    \"\"\"\n",
    "    n_samples = len(X_train)\n",
    "    n_batches = n_samples // batch_size\n",
    "    \n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle training data\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        X_shuffled = X_train[indices]\n",
    "        y_shuffled = y_train[indices]\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            # Get batch\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            X_batch = X_shuffled[start:end]\n",
    "            y_batch = y_shuffled[start:end]\n",
    "            \n",
    "            # Forward pass\n",
    "            probs = model.forward(X_batch)\n",
    "            loss = cross_entropy_loss(probs, y_batch)\n",
    "            epoch_loss += loss\n",
    "            \n",
    "            # Backward pass\n",
    "            grads = model.backward(y_batch)\n",
    "            \n",
    "            # Update\n",
    "            model.update(grads, learning_rate)\n",
    "        \n",
    "        # Compute metrics\n",
    "        avg_loss = epoch_loss / n_batches\n",
    "        train_acc = model.accuracy(X_train[:5000], y_train[:5000])  # Subset for speed\n",
    "        val_acc = model.accuracy(X_val, y_val)\n",
    "        \n",
    "        history['train_loss'].append(avg_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:2d}: Loss = {avg_loss:.4f}, \"\n",
    "              f\"Train Acc = {train_acc:.3f}, Val Acc = {val_acc:.3f}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the network\n",
    "model = NeuralNetwork(\n",
    "    input_dim=784,   # 28x28 pixels\n",
    "    hidden_dim=128,  # Hidden layer size\n",
    "    output_dim=10    # 10 digit classes\n",
    ")\n",
    "\n",
    "# Split off a validation set\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=5000, random_state=42\n",
    ")\n",
    "\n",
    "# Train!\n",
    "history = train(\n",
    "    model, X_tr, y_tr, X_val, y_val,\n",
    "    epochs=15,\n",
    "    batch_size=64,\n",
    "    learning_rate=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss\n",
    "ax1.plot(history['train_loss'])\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy\n",
    "ax2.plot(history['train_acc'], label='Train')\n",
    "ax2.plot(history['val_acc'], label='Validation')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = model.accuracy(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.3f}\")\n",
    "print(f\"That's {int(test_acc * len(y_test))} / {len(y_test)} correct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions on test samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "\n",
    "# Get some random test samples\n",
    "indices = np.random.choice(len(X_test), 10, replace=False)\n",
    "\n",
    "for i, (ax, idx) in enumerate(zip(axes.flat, indices)):\n",
    "    img = X_test[idx].reshape(28, 28)\n",
    "    pred = model.predict(X_test[idx:idx+1])[0]\n",
    "    true = y_test[idx]\n",
    "    \n",
    "    ax.imshow(img, cmap='gray')\n",
    "    color = 'green' if pred == true else 'red'\n",
    "    ax.set_title(f\"Pred: {pred}, True: {true}\", color=color)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Understanding What the Network Learned\n",
    "\n",
    "Let's visualize what features the hidden layer learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first layer weights\n",
    "# Each column of W1 is a \"feature detector\"\n",
    "\n",
    "fig, axes = plt.subplots(4, 8, figsize=(12, 6))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < model.W1.shape[1]:\n",
    "        weight = model.W1[:, i].reshape(28, 28)\n",
    "        ax.imshow(weight, cmap='RdBu_r', vmin=-0.5, vmax=0.5)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('First 32 Hidden Unit Weights\\n(Red = positive, Blue = negative)', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Get predictions on test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=range(10), yticklabels=range(10))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Which digits are most confused?\n",
    "print(\"\\nMost common confusions:\")\n",
    "errors = []\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if i != j and cm[i, j] > 0:\n",
    "            errors.append((cm[i, j], i, j))\n",
    "\n",
    "errors.sort(reverse=True)\n",
    "for count, true, pred in errors[:5]:\n",
    "    print(f\"  True {true} → Predicted {pred}: {count} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Gradient Checking (Optional)\n",
    "\n",
    "Verify our gradients are correct by comparing with numerical gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(model, X, y, param_name, epsilon=1e-5):\n",
    "    \"\"\"\n",
    "    Compute gradient numerically for verification.\n",
    "    \"\"\"\n",
    "    param = getattr(model, param_name)\n",
    "    grad = np.zeros_like(param)\n",
    "    \n",
    "    # Only check a subset for speed\n",
    "    indices = np.random.choice(param.size, min(20, param.size), replace=False)\n",
    "    \n",
    "    for idx in indices:\n",
    "        flat_idx = np.unravel_index(idx, param.shape)\n",
    "        \n",
    "        # f(x + epsilon)\n",
    "        param[flat_idx] += epsilon\n",
    "        probs_plus = model.forward(X)\n",
    "        loss_plus = cross_entropy_loss(probs_plus, y)\n",
    "        \n",
    "        # f(x - epsilon)\n",
    "        param[flat_idx] -= 2 * epsilon\n",
    "        probs_minus = model.forward(X)\n",
    "        loss_minus = cross_entropy_loss(probs_minus, y)\n",
    "        \n",
    "        # Restore\n",
    "        param[flat_idx] += epsilon\n",
    "        \n",
    "        # Gradient\n",
    "        grad[flat_idx] = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "    \n",
    "    return grad, indices\n",
    "\n",
    "# Quick gradient check on a small batch\n",
    "X_check = X_train[:32]\n",
    "y_check = y_train[:32]\n",
    "\n",
    "# Get analytical gradient\n",
    "model.forward(X_check)\n",
    "analytical = model.backward(y_check)\n",
    "\n",
    "# Get numerical gradient for W2\n",
    "numerical, indices = numerical_gradient(model, X_check, y_check, 'W2')\n",
    "\n",
    "# Compare\n",
    "analytical_subset = analytical['W2'].flatten()[indices]\n",
    "numerical_subset = numerical.flatten()[indices]\n",
    "\n",
    "diff = np.abs(analytical_subset - numerical_subset).max()\n",
    "print(f\"Max gradient difference (W2): {diff:.2e}\")\n",
    "print(\"(Should be < 1e-5 for correct implementation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exercises\n",
    "\n",
    "Try modifying the network:\n",
    "\n",
    "1. **Change hidden layer size:** What happens with 32, 64, 256, 512 hidden units?\n",
    "\n",
    "2. **Add a second hidden layer:** Modify the class to have two hidden layers.\n",
    "\n",
    "3. **Try different activations:** Replace ReLU with sigmoid or tanh.\n",
    "\n",
    "4. **Add regularization:** Implement L2 regularization (weight decay).\n",
    "\n",
    "5. **Implement dropout:** Add dropout between layers.\n",
    "\n",
    "6. **Learning rate schedule:** Decay the learning rate during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Try a larger hidden layer\n",
    "# model_large = NeuralNetwork(784, 256, 10)\n",
    "# history = train(model_large, X_tr, y_tr, X_val, y_val, epochs=15, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we built:\n",
    "\n",
    "1. **Forward pass:** Matrix multiply → activation → matrix multiply → softmax\n",
    "\n",
    "2. **Backward pass:** Apply chain rule to compute gradients layer by layer\n",
    "\n",
    "3. **Training loop:** Batch data, forward, backward, update, repeat\n",
    "\n",
    "Key insights:\n",
    "- The whole network is just matrix operations\n",
    "- Backprop reuses intermediate values from the forward pass\n",
    "- He initialization + ReLU works well in practice\n",
    "- A simple 2-layer network achieves ~97% on MNIST\n",
    "\n",
    "**Next:** [02-backprop-from-scratch.ipynb](02-backprop-from-scratch.ipynb) for a deeper dive into the math."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
