{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10: Fine-Tuning Basics\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Understand how fine-tuning adapts pretrained models to specific tasks\n",
    "2. Load pretrained models and add task-specific heads\n",
    "3. Observe catastrophic forgetting during aggressive fine-tuning\n",
    "4. Compare frozen vs unfrozen backbone approaches\n",
    "5. Implement learning rate schedules for stable fine-tuning\n",
    "\n",
    "**Prerequisites:** [pretraining](../modern-llms/pretraining.md), [fine-tuning](../modern-llms/fine-tuning.md)\n",
    "\n",
    "**Framework:** PyTorch + HuggingFace Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import copy\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading a Pretrained Model\n",
    "\n",
    "We'll use GPT-2 small (124M parameters) as our base model. This model was pretrained on WebText to predict the next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained GPT-2\n",
    "model_name = \"gpt2\"  # 124M parameters\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have a pad token\n",
    "\n",
    "# Load the model\n",
    "pretrained_model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in pretrained_model.parameters()):,}\")\n",
    "\n",
    "# Show architecture\n",
    "print(f\"\\nModel config:\")\n",
    "print(f\"  Hidden size: {pretrained_model.config.n_embd}\")\n",
    "print(f\"  Layers: {pretrained_model.config.n_layer}\")\n",
    "print(f\"  Heads: {pretrained_model.config.n_head}\")\n",
    "print(f\"  Vocabulary: {pretrained_model.config.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate that it already knows language\n",
    "def generate_text(model, prompt, max_tokens=50, temperature=0.8):\n",
    "    \"\"\"Generate text from a prompt.\"\"\"\n",
    "    model.train(False)  # Set to inference mode\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test generation\n",
    "print(\"Pretrained GPT-2 generation:\")\n",
    "print(\"-\" * 50)\n",
    "print(generate_text(pretrained_model, \"The meaning of life is\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Creating a Fine-Tuning Task\n",
    "\n",
    "We'll create a sentiment classification task. Fine-tuning means adapting the pretrained model to classify text as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple sentiment dataset\n",
    "SENTIMENT_DATA = [\n",
    "    # Positive\n",
    "    (\"This movie was absolutely amazing and wonderful!\", 1),\n",
    "    (\"I loved every moment of this incredible experience.\", 1),\n",
    "    (\"What a fantastic product, exceeded all expectations!\", 1),\n",
    "    (\"The service was excellent and the staff were friendly.\", 1),\n",
    "    (\"Best purchase I've ever made, highly recommend!\", 1),\n",
    "    (\"This made my day so much better, thank you!\", 1),\n",
    "    (\"Absolutely brilliant work, I'm impressed!\", 1),\n",
    "    (\"Outstanding quality and great value for money.\", 1),\n",
    "    (\"The food was delicious and beautifully presented.\", 1),\n",
    "    (\"I had an amazing time, would definitely return!\", 1),\n",
    "    (\"Such a wonderful surprise, I'm so happy!\", 1),\n",
    "    (\"This exceeded my expectations in every way.\", 1),\n",
    "    (\"Perfect in every way, couldn't ask for more.\", 1),\n",
    "    (\"What an incredible achievement, well done!\", 1),\n",
    "    (\"I'm thrilled with the results, fantastic!\", 1),\n",
    "    # Negative\n",
    "    (\"This was a complete waste of time and money.\", 0),\n",
    "    (\"Terrible experience, I would never recommend this.\", 0),\n",
    "    (\"The quality was awful and disappointing.\", 0),\n",
    "    (\"Worst service I have ever encountered.\", 0),\n",
    "    (\"I regret buying this, total disappointment.\", 0),\n",
    "    (\"What a disaster, nothing worked as expected.\", 0),\n",
    "    (\"Horrible product, broke after one day.\", 0),\n",
    "    (\"The food was cold and tasted terrible.\", 0),\n",
    "    (\"I'm very unhappy with this purchase.\", 0),\n",
    "    (\"Waste of money, don't bother buying this.\", 0),\n",
    "    (\"Extremely frustrating experience, avoid at all costs.\", 0),\n",
    "    (\"This ruined my entire day, so disappointed.\", 0),\n",
    "    (\"Poor quality and even worse customer service.\", 0),\n",
    "    (\"I've never been more let down by a product.\", 0),\n",
    "    (\"Absolute garbage, want my money back.\", 0),\n",
    "]\n",
    "\n",
    "# Split into train/test\n",
    "np.random.shuffle(SENTIMENT_DATA)\n",
    "train_data = SENTIMENT_DATA[:24]\n",
    "test_data = SENTIMENT_DATA[24:]\n",
    "\n",
    "print(f\"Training examples: {len(train_data)}\")\n",
    "print(f\"Test examples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    \"\"\"Dataset for sentiment classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, data, tokenizer, max_length=64):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text, label = self.data[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_dataset = SentimentDataset(train_data, tokenizer)\n",
    "test_dataset = SentimentDataset(test_data, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Adding a Classification Head\n",
    "\n",
    "For classification, we need to:\n",
    "1. Get the hidden states from GPT-2 (use last token's representation)\n",
    "2. Add a linear layer to project to class logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2ForSentiment(nn.Module):\n",
    "    \"\"\"GPT-2 with a classification head for sentiment analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self, pretrained_model, num_classes=2, freeze_backbone=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Copy the transformer backbone\n",
    "        self.transformer = copy.deepcopy(pretrained_model.transformer)\n",
    "        self.config = pretrained_model.config\n",
    "        \n",
    "        # Optionally freeze backbone weights\n",
    "        if freeze_backbone:\n",
    "            for param in self.transformer.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Add classification head (randomly initialized)\n",
    "        self.classifier = nn.Linear(self.config.n_embd, num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Get transformer outputs\n",
    "        outputs = self.transformer(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        hidden_states = outputs.last_hidden_state  # [batch, seq, hidden]\n",
    "        \n",
    "        # Pool: use last non-padding token\n",
    "        # For each sequence, find the position of the last real token\n",
    "        if attention_mask is not None:\n",
    "            # Get index of last 1 in attention mask\n",
    "            sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "            batch_size = hidden_states.shape[0]\n",
    "            pooled = hidden_states[torch.arange(batch_size, device=hidden_states.device), \n",
    "                                   sequence_lengths]\n",
    "        else:\n",
    "            pooled = hidden_states[:, -1]  # Just use last token\n",
    "        \n",
    "        # Classify\n",
    "        pooled = self.dropout(pooled)\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits\n",
    "    \n",
    "    def count_trainable_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "# Create model with unfrozen backbone\n",
    "sentiment_model = GPT2ForSentiment(pretrained_model, freeze_backbone=False).to(device)\n",
    "print(f\"Trainable parameters (unfrozen): {sentiment_model.count_trainable_params():,}\")\n",
    "\n",
    "# Create model with frozen backbone\n",
    "frozen_model = GPT2ForSentiment(pretrained_model, freeze_backbone=True).to(device)\n",
    "print(f\"Trainable parameters (frozen): {frozen_model.count_trainable_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Training and Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, scheduler=None):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    return total_loss / len(train_loader), correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    \"\"\"Evaluate on test set.\"\"\"\n",
    "    model.train(False)  # Inference mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Demonstrating Catastrophic Forgetting\n",
    "\n",
    "Let's see what happens when we fine-tune aggressively (high learning rate). The model will overfit to our task and \"forget\" its language modeling abilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_language_modeling(model_transformer, tokenizer, test_prompts):\n",
    "    \"\"\"Measure language modeling quality by computing perplexity on test prompts.\"\"\"\n",
    "    # Rebuild an LM head model from the transformer\n",
    "    config = GPT2Config.from_pretrained('gpt2')\n",
    "    lm_model = GPT2LMHeadModel(config).to(device)\n",
    "    lm_model.transformer = model_transformer\n",
    "    lm_model.train(False)\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for prompt in test_prompts:\n",
    "            inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "            outputs = lm_model(**inputs, labels=inputs['input_ids'])\n",
    "            total_loss += outputs.loss.item() * inputs['input_ids'].shape[1]\n",
    "            total_tokens += inputs['input_ids'].shape[1]\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    return perplexity\n",
    "\n",
    "# Test prompts to measure language modeling quality\n",
    "LM_TEST_PROMPTS = [\n",
    "    \"The capital of France is Paris, which is known for\",\n",
    "    \"In the beginning, there was nothing but darkness and\",\n",
    "    \"Machine learning is a field of artificial intelligence that\",\n",
    "    \"The quick brown fox jumps over the lazy dog while\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with HIGH learning rate (will cause forgetting)\n",
    "print(\"Training with HIGH learning rate (1e-3) - demonstrates catastrophic forgetting\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "aggressive_model = GPT2ForSentiment(pretrained_model, freeze_backbone=False).to(device)\n",
    "aggressive_optimizer = torch.optim.AdamW(aggressive_model.parameters(), lr=1e-3)  # Too high!\n",
    "\n",
    "aggressive_metrics = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_acc': [],\n",
    "    'lm_perplexity': []\n",
    "}\n",
    "\n",
    "# Measure initial perplexity\n",
    "initial_ppl = measure_language_modeling(aggressive_model.transformer, tokenizer, LM_TEST_PROMPTS)\n",
    "print(f\"Initial LM perplexity: {initial_ppl:.2f}\")\n",
    "\n",
    "for epoch in range(10):\n",
    "    train_loss, train_acc = train_epoch(aggressive_model, train_loader, aggressive_optimizer)\n",
    "    test_acc = evaluate(aggressive_model, test_loader)\n",
    "    lm_ppl = measure_language_modeling(aggressive_model.transformer, tokenizer, LM_TEST_PROMPTS)\n",
    "    \n",
    "    aggressive_metrics['train_loss'].append(train_loss)\n",
    "    aggressive_metrics['train_acc'].append(train_acc)\n",
    "    aggressive_metrics['test_acc'].append(test_acc)\n",
    "    aggressive_metrics['lm_perplexity'].append(lm_ppl)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Train Acc={train_acc:.3f}, Test Acc={test_acc:.3f}, LM PPL={lm_ppl:.1f}\")\n",
    "\n",
    "print(f\"\\nFinal LM perplexity: {aggressive_metrics['lm_perplexity'][-1]:.2f}\")\n",
    "print(f\"Perplexity increased by {aggressive_metrics['lm_perplexity'][-1]/initial_ppl:.1f}x (forgetting!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with PROPER low learning rate\n",
    "print(\"\\nTraining with LOW learning rate (2e-5) - proper fine-tuning\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "careful_model = GPT2ForSentiment(pretrained_model, freeze_backbone=False).to(device)\n",
    "careful_optimizer = torch.optim.AdamW(careful_model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "# Learning rate schedule with warmup\n",
    "total_steps = len(train_loader) * 10\n",
    "careful_scheduler = get_linear_schedule_with_warmup(\n",
    "    careful_optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "careful_metrics = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_acc': [],\n",
    "    'lm_perplexity': []\n",
    "}\n",
    "\n",
    "initial_ppl = measure_language_modeling(careful_model.transformer, tokenizer, LM_TEST_PROMPTS)\n",
    "print(f\"Initial LM perplexity: {initial_ppl:.2f}\")\n",
    "\n",
    "for epoch in range(10):\n",
    "    train_loss, train_acc = train_epoch(careful_model, train_loader, careful_optimizer, careful_scheduler)\n",
    "    test_acc = evaluate(careful_model, test_loader)\n",
    "    lm_ppl = measure_language_modeling(careful_model.transformer, tokenizer, LM_TEST_PROMPTS)\n",
    "    \n",
    "    careful_metrics['train_loss'].append(train_loss)\n",
    "    careful_metrics['train_acc'].append(train_acc)\n",
    "    careful_metrics['test_acc'].append(test_acc)\n",
    "    careful_metrics['lm_perplexity'].append(lm_ppl)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Train Acc={train_acc:.3f}, Test Acc={test_acc:.3f}, LM PPL={lm_ppl:.1f}\")\n",
    "\n",
    "print(f\"\\nFinal LM perplexity: {careful_metrics['lm_perplexity'][-1]:.2f}\")\n",
    "print(f\"Perplexity change: {careful_metrics['lm_perplexity'][-1]/initial_ppl:.2f}x (minimal forgetting!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize catastrophic forgetting\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "epochs = range(1, 11)\n",
    "\n",
    "# Training loss\n",
    "axes[0].plot(epochs, aggressive_metrics['train_loss'], 'r-', label='High LR (1e-3)', linewidth=2)\n",
    "axes[0].plot(epochs, careful_metrics['train_loss'], 'b-', label='Low LR (2e-5)', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Training Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Test accuracy\n",
    "axes[1].plot(epochs, aggressive_metrics['test_acc'], 'r-', label='High LR (1e-3)', linewidth=2)\n",
    "axes[1].plot(epochs, careful_metrics['test_acc'], 'b-', label='Low LR (2e-5)', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Test Accuracy')\n",
    "axes[1].set_title('Task Performance')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# LM perplexity (forgetting indicator)\n",
    "axes[2].plot(epochs, aggressive_metrics['lm_perplexity'], 'r-', label='High LR (1e-3)', linewidth=2)\n",
    "axes[2].plot(epochs, careful_metrics['lm_perplexity'], 'b-', label='Low LR (2e-5)', linewidth=2)\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('LM Perplexity')\n",
    "axes[2].set_title('Language Modeling Quality\\n(Lower = Better)')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('catastrophic_forgetting.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight: High learning rate achieves similar task accuracy but destroys\")\n",
    "print(\"the model's language understanding (high perplexity = catastrophic forgetting)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Frozen vs Unfrozen Backbone\n",
    "\n",
    "Another strategy to prevent forgetting: freeze the pretrained weights entirely and only train the classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with frozen backbone (feature extraction)\n",
    "print(\"Training with FROZEN backbone (only train classifier head)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "frozen_model = GPT2ForSentiment(pretrained_model, freeze_backbone=True).to(device)\n",
    "# Can use higher LR since we're not touching pretrained weights\n",
    "frozen_optimizer = torch.optim.AdamW(frozen_model.parameters(), lr=1e-3)\n",
    "\n",
    "frozen_metrics = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_acc': [],\n",
    "    'lm_perplexity': []\n",
    "}\n",
    "\n",
    "initial_ppl = measure_language_modeling(frozen_model.transformer, tokenizer, LM_TEST_PROMPTS)\n",
    "print(f\"Initial LM perplexity: {initial_ppl:.2f}\")\n",
    "\n",
    "for epoch in range(10):\n",
    "    train_loss, train_acc = train_epoch(frozen_model, train_loader, frozen_optimizer)\n",
    "    test_acc = evaluate(frozen_model, test_loader)\n",
    "    lm_ppl = measure_language_modeling(frozen_model.transformer, tokenizer, LM_TEST_PROMPTS)\n",
    "    \n",
    "    frozen_metrics['train_loss'].append(train_loss)\n",
    "    frozen_metrics['train_acc'].append(train_acc)\n",
    "    frozen_metrics['test_acc'].append(test_acc)\n",
    "    frozen_metrics['lm_perplexity'].append(lm_ppl)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Train Acc={train_acc:.3f}, Test Acc={test_acc:.3f}, LM PPL={lm_ppl:.1f}\")\n",
    "\n",
    "print(f\"\\nFinal LM perplexity: {frozen_metrics['lm_perplexity'][-1]:.2f}\")\n",
    "print(\"No forgetting possible - backbone weights unchanged!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three approaches\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "epochs = range(1, 11)\n",
    "\n",
    "# Test accuracy comparison\n",
    "axes[0].plot(epochs, aggressive_metrics['test_acc'], 'r-', label='Unfrozen + High LR', linewidth=2)\n",
    "axes[0].plot(epochs, careful_metrics['test_acc'], 'b-', label='Unfrozen + Low LR', linewidth=2)\n",
    "axes[0].plot(epochs, frozen_metrics['test_acc'], 'g-', label='Frozen backbone', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Test Accuracy')\n",
    "axes[0].set_title('Task Performance Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# LM perplexity comparison\n",
    "axes[1].plot(epochs, aggressive_metrics['lm_perplexity'], 'r-', label='Unfrozen + High LR', linewidth=2)\n",
    "axes[1].plot(epochs, careful_metrics['lm_perplexity'], 'b-', label='Unfrozen + Low LR', linewidth=2)\n",
    "axes[1].plot(epochs, frozen_metrics['lm_perplexity'], 'g-', label='Frozen backbone', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('LM Perplexity')\n",
    "axes[1].set_title('Language Modeling Quality')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('finetuning_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"Summary: Final Test Accuracy and LM Perplexity\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Method':<30} {'Test Acc':>12} {'LM PPL':>12}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Unfrozen + High LR (1e-3)':<30} {aggressive_metrics['test_acc'][-1]:>12.3f} {aggressive_metrics['lm_perplexity'][-1]:>12.1f}\")\n",
    "print(f\"{'Unfrozen + Low LR (2e-5)':<30} {careful_metrics['test_acc'][-1]:>12.3f} {careful_metrics['lm_perplexity'][-1]:>12.1f}\")\n",
    "print(f\"{'Frozen backbone':<30} {frozen_metrics['test_acc'][-1]:>12.3f} {frozen_metrics['lm_perplexity'][-1]:>12.1f}\")\n",
    "print(\"-\" * 60)\n",
    "print(\"\\nTrade-offs:\")\n",
    "print(\"- High LR: Fastest training but destroys pretrained knowledge\")\n",
    "print(\"- Low LR: Good task performance while preserving general abilities\")\n",
    "print(\"- Frozen: Zero forgetting but may underfit complex tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Learning Rate Schedules\n",
    "\n",
    "Proper fine-tuning uses warmup + decay schedules to stabilize training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize different learning rate schedules\n",
    "def visualize_lr_schedules():\n",
    "    \"\"\"Compare different learning rate schedules.\"\"\"\n",
    "    total_steps = 1000\n",
    "    warmup_steps = 100\n",
    "    base_lr = 2e-5\n",
    "    \n",
    "    # Create dummy models and optimizers for schedule visualization\n",
    "    schedules = {}\n",
    "    \n",
    "    # Constant LR\n",
    "    lrs = [base_lr] * total_steps\n",
    "    schedules['Constant'] = lrs\n",
    "    \n",
    "    # Linear warmup + decay\n",
    "    dummy_param = nn.Parameter(torch.zeros(1))\n",
    "    opt = torch.optim.AdamW([dummy_param], lr=base_lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        opt, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",
    "    )\n",
    "    lrs = []\n",
    "    for _ in range(total_steps):\n",
    "        lrs.append(opt.param_groups[0]['lr'])\n",
    "        scheduler.step()\n",
    "    schedules['Linear warmup + decay'] = lrs\n",
    "    \n",
    "    # Cosine with warmup\n",
    "    dummy_param = nn.Parameter(torch.zeros(1))\n",
    "    opt = torch.optim.AdamW([dummy_param], lr=base_lr)\n",
    "    from transformers import get_cosine_schedule_with_warmup\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        opt, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",
    "    )\n",
    "    lrs = []\n",
    "    for _ in range(total_steps):\n",
    "        lrs.append(opt.param_groups[0]['lr'])\n",
    "        scheduler.step()\n",
    "    schedules['Cosine warmup + decay'] = lrs\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    steps = range(total_steps)\n",
    "    colors = ['gray', 'blue', 'green']\n",
    "    \n",
    "    for (name, lrs), color in zip(schedules.items(), colors):\n",
    "        plt.plot(steps, lrs, label=name, color=color, linewidth=2)\n",
    "    \n",
    "    plt.axvline(x=warmup_steps, color='red', linestyle='--', alpha=0.5, label='Warmup ends')\n",
    "    plt.xlabel('Training Step')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.title('Learning Rate Schedules for Fine-Tuning')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('lr_schedules.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nWhy warmup?\")\n",
    "    print(\"- Early gradients are noisy (random classifier head)\")\n",
    "    print(\"- Small LR during warmup prevents large initial updates\")\n",
    "    print(\"- Gradual increase lets model stabilize\")\n",
    "    print(\"\\nWhy decay?\")\n",
    "    print(\"- Reduces LR as we approach convergence\")\n",
    "    print(\"- Helps model settle into a good minimum\")\n",
    "    print(\"- Prevents overshooting late in training\")\n",
    "\n",
    "visualize_lr_schedules()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Layer-wise Learning Rates\n",
    "\n",
    "Advanced technique: use different learning rates for different layers. Lower layers (closer to input) should change less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_layerwise_optimizer(model, base_lr=2e-5, decay_factor=0.9):\n",
    "    \"\"\"\n",
    "    Create optimizer with layer-wise learning rate decay.\n",
    "    \n",
    "    Later layers get higher LR, earlier layers get lower LR.\n",
    "    This preserves lower-level features while adapting higher-level ones.\n",
    "    \"\"\"\n",
    "    # Separate parameters into groups\n",
    "    param_groups = []\n",
    "    \n",
    "    # Embeddings get lowest LR\n",
    "    embed_params = []\n",
    "    for name, param in model.transformer.named_parameters():\n",
    "        if 'wte' in name or 'wpe' in name:  # Token and position embeddings\n",
    "            embed_params.append(param)\n",
    "    if embed_params:\n",
    "        param_groups.append({\n",
    "            'params': embed_params,\n",
    "            'lr': base_lr * (decay_factor ** 12)  # Lowest LR\n",
    "        })\n",
    "    \n",
    "    # Each transformer layer gets progressively higher LR\n",
    "    n_layers = model.config.n_layer\n",
    "    for layer_idx in range(n_layers):\n",
    "        layer_params = []\n",
    "        for name, param in model.transformer.named_parameters():\n",
    "            if f'.h.{layer_idx}.' in name:\n",
    "                layer_params.append(param)\n",
    "        \n",
    "        # Earlier layers get lower LR\n",
    "        layer_lr = base_lr * (decay_factor ** (n_layers - 1 - layer_idx))\n",
    "        if layer_params:\n",
    "            param_groups.append({\n",
    "                'params': layer_params,\n",
    "                'lr': layer_lr\n",
    "            })\n",
    "    \n",
    "    # Final layer norm\n",
    "    ln_params = []\n",
    "    for name, param in model.transformer.named_parameters():\n",
    "        if 'ln_f' in name:\n",
    "            ln_params.append(param)\n",
    "    if ln_params:\n",
    "        param_groups.append({\n",
    "            'params': ln_params,\n",
    "            'lr': base_lr  # Full LR for final norm\n",
    "        })\n",
    "    \n",
    "    # Classification head gets highest LR\n",
    "    param_groups.append({\n",
    "        'params': model.classifier.parameters(),\n",
    "        'lr': base_lr * 10  # Higher LR for new layers\n",
    "    })\n",
    "    \n",
    "    return torch.optim.AdamW(param_groups, weight_decay=0.01)\n",
    "\n",
    "\n",
    "# Visualize layer-wise learning rates\n",
    "def visualize_layerwise_lr(base_lr=2e-5, decay_factor=0.9, n_layers=12):\n",
    "    \"\"\"Show how learning rate varies by layer.\"\"\"\n",
    "    layers = ['Embed'] + [f'Layer {i}' for i in range(n_layers)] + ['LN_f', 'Head']\n",
    "    lrs = [\n",
    "        base_lr * (decay_factor ** n_layers),  # Embeddings\n",
    "        *[base_lr * (decay_factor ** (n_layers - 1 - i)) for i in range(n_layers)],  # Layers\n",
    "        base_lr,  # Final LN\n",
    "        base_lr * 10  # Head\n",
    "    ]\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    bars = plt.bar(range(len(layers)), lrs, color='steelblue')\n",
    "    bars[-1].set_color('green')  # Highlight classifier head\n",
    "    bars[0].set_color('orange')  # Highlight embeddings\n",
    "    \n",
    "    plt.xticks(range(len(layers)), layers, rotation=45, ha='right')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.title('Layer-wise Learning Rates\\n(Earlier layers learn slower to preserve pretrained features)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('layerwise_lr.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Embeddings LR: {lrs[0]:.2e}\")\n",
    "    print(f\"Layer 0 LR: {lrs[1]:.2e}\")\n",
    "    print(f\"Layer 11 LR: {lrs[-3]:.2e}\")\n",
    "    print(f\"Classifier LR: {lrs[-1]:.2e}\")\n",
    "\n",
    "visualize_layerwise_lr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Practical Fine-Tuning Checklist\n",
    "\n",
    "A summary of best practices for fine-tuning pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_finetuning_checklist():\n",
    "    \"\"\"Print a checklist for fine-tuning.\"\"\"\n",
    "    checklist = \"\"\"\n",
    "    FINE-TUNING BEST PRACTICES\n",
    "    ==========================\n",
    "    \n",
    "    1. LEARNING RATE\n",
    "       [ ] Use small LR: 1e-5 to 5e-5 for full fine-tuning\n",
    "       [ ] Use warmup: 6-10% of total steps\n",
    "       [ ] Use decay: linear or cosine schedule\n",
    "       [ ] Consider layer-wise LR decay\n",
    "    \n",
    "    2. REGULARIZATION\n",
    "       [ ] Weight decay: 0.01 typical\n",
    "       [ ] Dropout: keep or slightly increase\n",
    "       [ ] Early stopping: monitor validation loss\n",
    "    \n",
    "    3. DATA\n",
    "       [ ] Clean, high-quality task data\n",
    "       [ ] Balanced classes if classification\n",
    "       [ ] Reasonable train/val/test split (e.g., 80/10/10)\n",
    "    \n",
    "    4. TRAINING\n",
    "       [ ] Small batch size: 8-32 typical\n",
    "       [ ] Few epochs: 2-4 often enough\n",
    "       [ ] Gradient clipping: max_norm=1.0\n",
    "       [ ] Mixed precision if available\n",
    "    \n",
    "    5. EVALUATION\n",
    "       [ ] Monitor task metrics (accuracy, F1, etc.)\n",
    "       [ ] Check for overfitting (train vs val gap)\n",
    "       [ ] Optionally track forgetting metrics\n",
    "    \n",
    "    6. ALTERNATIVES\n",
    "       [ ] Frozen backbone for limited data\n",
    "       [ ] LoRA/adapters for efficient training\n",
    "       [ ] Few-shot prompting before fine-tuning\n",
    "    \"\"\"\n",
    "    print(checklist)\n",
    "\n",
    "print_finetuning_checklist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Data size experiment**: Try fine-tuning with 5, 10, 20, 50 training examples. How does performance scale?\n",
    "\n",
    "2. **Epoch selection**: Train for 20 epochs and plot train/val loss. When does overfitting begin?\n",
    "\n",
    "3. **Different pooling**: Instead of last-token pooling, try mean pooling. Does it help?\n",
    "\n",
    "4. **Partial freezing**: Freeze only the first N layers. Find the best N.\n",
    "\n",
    "5. **Multi-task**: Add a second task (e.g., topic classification). Train on both simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Concept | Key Point |\n",
    "|---------|----------|\n",
    "| Transfer learning | Pretrained models have useful general representations |\n",
    "| Classification head | Add task-specific layers on top of frozen/unfrozen backbone |\n",
    "| Catastrophic forgetting | High LR destroys pretrained knowledge |\n",
    "| Learning rate | Use 1e-5 to 5e-5 with warmup + decay |\n",
    "| Frozen backbone | Prevents forgetting but may underfit |\n",
    "| Layer-wise LR | Earlier layers learn slower to preserve features |\n",
    "\n",
    "**Key insight:** Fine-tuning works because pretrained representations transfer. The challenge is adapting to new tasks without destroying what was learned. Use small learning rates, warmup schedules, and consider which layers to update."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
