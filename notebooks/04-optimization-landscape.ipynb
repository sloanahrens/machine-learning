{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Optimization Landscape Visualized\n",
    "\n",
    "This notebook visualizes how different optimizers navigate loss surfaces. We'll implement SGD, Momentum, RMSprop, and Adam from scratch and animate their paths on 2D functions.\n",
    "\n",
    "**Goal:** Build intuition for optimizer behavior on different loss landscapes.\n",
    "\n",
    "**Prerequisites:** [optimization.md](../math-foundations/optimization.md), [optimizers.md](../training/optimizers.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Optimizers from Scratch\n",
    "\n",
    "Let's implement each optimizer with clear, minimal code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    Vanilla Stochastic Gradient Descent.\n",
    "    \n",
    "    Update: θ = θ - lr * gradient\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "    \n",
    "    def step(self, params, grads):\n",
    "        return params - self.lr * grads\n",
    "\n",
    "\n",
    "class Momentum:\n",
    "    \"\"\"\n",
    "    SGD with Momentum.\n",
    "    \n",
    "    Accumulates velocity from past gradients.\n",
    "    \n",
    "    v = β * v + gradient\n",
    "    θ = θ - lr * v\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01, beta=0.9):\n",
    "        self.lr = lr\n",
    "        self.beta = beta\n",
    "        self.v = None\n",
    "    \n",
    "    def step(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = np.zeros_like(params)\n",
    "        \n",
    "        self.v = self.beta * self.v + grads\n",
    "        return params - self.lr * self.v\n",
    "\n",
    "\n",
    "class RMSprop:\n",
    "    \"\"\"\n",
    "    RMSprop: Adapts learning rate per parameter.\n",
    "    \n",
    "    Divides by running average of squared gradients.\n",
    "    \n",
    "    s = β * s + (1-β) * gradient²\n",
    "    θ = θ - lr * gradient / √(s + ε)\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01, beta=0.9, eps=1e-8):\n",
    "        self.lr = lr\n",
    "        self.beta = beta\n",
    "        self.eps = eps\n",
    "        self.s = None\n",
    "    \n",
    "    def step(self, params, grads):\n",
    "        if self.s is None:\n",
    "            self.s = np.zeros_like(params)\n",
    "        \n",
    "        self.s = self.beta * self.s + (1 - self.beta) * grads**2\n",
    "        return params - self.lr * grads / (np.sqrt(self.s) + self.eps)\n",
    "\n",
    "\n",
    "class Adam:\n",
    "    \"\"\"\n",
    "    Adam: Combines momentum and adaptive learning rates.\n",
    "    \n",
    "    m = β1 * m + (1-β1) * gradient       (momentum)\n",
    "    v = β2 * v + (1-β2) * gradient²      (RMSprop)\n",
    "    \n",
    "    # Bias correction (important early in training)\n",
    "    m_hat = m / (1 - β1^t)\n",
    "    v_hat = v / (1 - β2^t)\n",
    "    \n",
    "    θ = θ - lr * m_hat / √(v_hat + ε)\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.t = 0\n",
    "    \n",
    "    def step(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m = np.zeros_like(params)\n",
    "            self.v = np.zeros_like(params)\n",
    "        \n",
    "        self.t += 1\n",
    "        \n",
    "        # Update biased first and second moment estimates\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * grads\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * grads**2\n",
    "        \n",
    "        # Bias correction\n",
    "        m_hat = self.m / (1 - self.beta1**self.t)\n",
    "        v_hat = self.v / (1 - self.beta2**self.t)\n",
    "        \n",
    "        return params - self.lr * m_hat / (np.sqrt(v_hat) + self.eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Test Functions\n",
    "\n",
    "Classic optimization test functions with different characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test functions and their gradients\n",
    "\n",
    "def quadratic(x):\n",
    "    \"\"\"Simple convex quadratic bowl. Minimum at (0, 0).\"\"\"\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "def quadratic_grad(x):\n",
    "    return np.array([2*x[0], 2*x[1]])\n",
    "\n",
    "\n",
    "def elongated_quadratic(x):\n",
    "    \"\"\"Elongated quadratic - different curvature in each direction.\"\"\"\n",
    "    return x[0]**2 + 10*x[1]**2\n",
    "\n",
    "def elongated_quadratic_grad(x):\n",
    "    return np.array([2*x[0], 20*x[1]])\n",
    "\n",
    "\n",
    "def rosenbrock(x):\n",
    "    \"\"\"\n",
    "    Rosenbrock function: classic difficult optimization problem.\n",
    "    Minimum at (1, 1). Has a narrow curved valley.\n",
    "    \"\"\"\n",
    "    return (1 - x[0])**2 + 100*(x[1] - x[0]**2)**2\n",
    "\n",
    "def rosenbrock_grad(x):\n",
    "    dx = -2*(1 - x[0]) - 400*x[0]*(x[1] - x[0]**2)\n",
    "    dy = 200*(x[1] - x[0]**2)\n",
    "    return np.array([dx, dy])\n",
    "\n",
    "\n",
    "def saddle_point(x):\n",
    "    \"\"\"Saddle point at origin. Goes up in x, down in y.\"\"\"\n",
    "    return x[0]**2 - x[1]**2\n",
    "\n",
    "def saddle_point_grad(x):\n",
    "    return np.array([2*x[0], -2*x[1]])\n",
    "\n",
    "\n",
    "def beale(x):\n",
    "    \"\"\"\n",
    "    Beale function: multiple local minima.\n",
    "    Global minimum at (3, 0.5).\n",
    "    \"\"\"\n",
    "    term1 = (1.5 - x[0] + x[0]*x[1])**2\n",
    "    term2 = (2.25 - x[0] + x[0]*x[1]**2)**2\n",
    "    term3 = (2.625 - x[0] + x[0]*x[1]**3)**2\n",
    "    return term1 + term2 + term3\n",
    "\n",
    "def beale_grad(x):\n",
    "    # Numerical gradient for simplicity\n",
    "    eps = 1e-5\n",
    "    grad = np.zeros(2)\n",
    "    for i in range(2):\n",
    "        x_plus = x.copy()\n",
    "        x_plus[i] += eps\n",
    "        x_minus = x.copy()\n",
    "        x_minus[i] -= eps\n",
    "        grad[i] = (beale(x_plus) - beale(x_minus)) / (2*eps)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the test functions\n",
    "\n",
    "def plot_surface(func, xlim, ylim, title, ax=None, levels=50):\n",
    "    \"\"\"Plot contour of a 2D function.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    x = np.linspace(xlim[0], xlim[1], 200)\n",
    "    y = np.linspace(ylim[0], ylim[1], 200)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = np.array([[func(np.array([xi, yi])) for xi, yi in zip(row_x, row_y)] \n",
    "                  for row_x, row_y in zip(X, Y)])\n",
    "    \n",
    "    # Use log scale for better visualization of valleys\n",
    "    Z_plot = np.log(Z + 1)\n",
    "    \n",
    "    contour = ax.contour(X, Y, Z_plot, levels=levels, cmap='viridis', alpha=0.7)\n",
    "    ax.contourf(X, Y, Z_plot, levels=levels, cmap='viridis', alpha=0.3)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(title)\n",
    "    return ax\n",
    "\n",
    "\n",
    "# Plot all test functions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "plot_surface(quadratic, (-3, 3), (-3, 3), 'Quadratic (Easy)', axes[0, 0])\n",
    "axes[0, 0].plot(0, 0, 'r*', markersize=15, label='Minimum')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "plot_surface(elongated_quadratic, (-3, 3), (-3, 3), 'Elongated Quadratic', axes[0, 1])\n",
    "axes[0, 1].plot(0, 0, 'r*', markersize=15, label='Minimum')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "plot_surface(rosenbrock, (-2, 2), (-1, 3), 'Rosenbrock (Hard)', axes[1, 0], levels=30)\n",
    "axes[1, 0].plot(1, 1, 'r*', markersize=15, label='Minimum')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "plot_surface(saddle_point, (-3, 3), (-3, 3), 'Saddle Point', axes[1, 1])\n",
    "axes[1, 1].plot(0, 0, 'ro', markersize=10, label='Saddle')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Optimizer Trajectories\n",
    "\n",
    "Run each optimizer on the test functions and visualize their paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(optimizer, func, grad_func, start, n_steps=100):\n",
    "    \"\"\"\n",
    "    Run optimizer for n_steps.\n",
    "    Returns trajectory of positions.\n",
    "    \"\"\"\n",
    "    x = start.copy()\n",
    "    trajectory = [x.copy()]\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        grad = grad_func(x)\n",
    "        x = optimizer.step(x, grad)\n",
    "        trajectory.append(x.copy())\n",
    "    \n",
    "    return np.array(trajectory)\n",
    "\n",
    "\n",
    "def plot_trajectories(func, grad_func, xlim, ylim, title, start, n_steps=100,\n",
    "                      optimizers=None, lr_scale=1.0):\n",
    "    \"\"\"\n",
    "    Plot optimizer trajectories on a contour plot.\n",
    "    \"\"\"\n",
    "    if optimizers is None:\n",
    "        optimizers = {\n",
    "            'SGD': SGD(lr=0.1 * lr_scale),\n",
    "            'Momentum': Momentum(lr=0.1 * lr_scale, beta=0.9),\n",
    "            'RMSprop': RMSprop(lr=0.1 * lr_scale),\n",
    "            'Adam': Adam(lr=0.1 * lr_scale),\n",
    "        }\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    plot_surface(func, xlim, ylim, title, ax)\n",
    "    \n",
    "    colors = {'SGD': 'red', 'Momentum': 'blue', 'RMSprop': 'green', 'Adam': 'orange'}\n",
    "    \n",
    "    for name, opt in optimizers.items():\n",
    "        traj = optimize(opt, func, grad_func, start.copy(), n_steps)\n",
    "        ax.plot(traj[:, 0], traj[:, 1], '-', color=colors[name], \n",
    "                linewidth=2, label=name, alpha=0.8)\n",
    "        ax.plot(traj[0, 0], traj[0, 1], 'o', color=colors[name], markersize=8)\n",
    "        ax.plot(traj[-1, 0], traj[-1, 1], 's', color=colors[name], markersize=8)\n",
    "    \n",
    "    ax.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "    return optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quadratic - all optimizers should work well\n",
    "start = np.array([2.5, 2.5])\n",
    "plot_trajectories(quadratic, quadratic_grad, (-3, 3), (-3, 3), \n",
    "                  'Quadratic Function', start, n_steps=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elongated quadratic - momentum helps, SGD oscillates\n",
    "start = np.array([2.5, 2.5])\n",
    "plot_trajectories(elongated_quadratic, elongated_quadratic_grad, (-3, 3), (-3, 3),\n",
    "                  'Elongated Quadratic', start, n_steps=100, lr_scale=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "**Observation:** On the elongated quadratic, SGD oscillates because the gradient in the steep direction (y) is much larger. Momentum helps, but adaptive methods (RMSprop, Adam) handle different curvatures best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rosenbrock - the classic hard problem\n",
    "start = np.array([-1.0, 2.0])\n",
    "plot_trajectories(rosenbrock, rosenbrock_grad, (-2, 2), (-1, 3),\n",
    "                  'Rosenbrock Function', start, n_steps=500, lr_scale=0.01);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saddle point - most optimizers escape, SGD might not\n",
    "start = np.array([0.1, 0.1])  # Start near saddle\n",
    "plot_trajectories(saddle_point, saddle_point_grad, (-3, 3), (-3, 3),\n",
    "                  'Saddle Point', start, n_steps=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 4. Learning Rate Effects\n",
    "\n",
    "Let's see what happens with different learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_learning_rates(func, grad_func, xlim, ylim, start, lr_values):\n",
    "    \"\"\"Compare SGD with different learning rates.\"\"\"\n",
    "    fig, axes = plt.subplots(1, len(lr_values), figsize=(5*len(lr_values), 4))\n",
    "    \n",
    "    for ax, lr in zip(axes, lr_values):\n",
    "        plot_surface(func, xlim, ylim, f'SGD, lr={lr}', ax, levels=30)\n",
    "        \n",
    "        opt = SGD(lr=lr)\n",
    "        traj = optimize(opt, func, grad_func, start.copy(), n_steps=50)\n",
    "        \n",
    "        ax.plot(traj[:, 0], traj[:, 1], 'r-', linewidth=2, alpha=0.8)\n",
    "        ax.plot(traj[0, 0], traj[0, 1], 'ro', markersize=10)\n",
    "        ax.plot(traj[-1, 0], traj[-1, 1], 'rs', markersize=10)\n",
    "        \n",
    "        # Print final value\n",
    "        final_val = func(traj[-1])\n",
    "        ax.set_title(f'SGD, lr={lr}\\nFinal loss: {final_val:.4f}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compare learning rates on quadratic\n",
    "start = np.array([2.5, 2.5])\n",
    "compare_learning_rates(quadratic, quadratic_grad, (-3, 3), (-3, 3), \n",
    "                       start, [0.01, 0.1, 0.5, 0.9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "**Key insight:** \n",
    "- **Too small LR (0.01):** Slow convergence, might not reach minimum\n",
    "- **Good LR (0.1-0.5):** Reaches minimum efficiently\n",
    "- **Too large LR (0.9):** Overshoots, oscillates, may diverge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same for elongated quadratic - shows the problem more clearly\n",
    "start = np.array([2.5, 2.5])\n",
    "compare_learning_rates(elongated_quadratic, elongated_quadratic_grad, \n",
    "                       (-3, 3), (-3, 3), start, [0.01, 0.05, 0.1, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 5. Momentum Visualization\n",
    "\n",
    "Momentum helps overcome oscillations by accumulating velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_momentum(func, grad_func, xlim, ylim, start, beta_values, lr=0.05):\n",
    "    \"\"\"Compare different momentum coefficients.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    plot_surface(func, xlim, ylim, 'Effect of Momentum', ax)\n",
    "    \n",
    "    colors = plt.cm.coolwarm(np.linspace(0, 1, len(beta_values)))\n",
    "    \n",
    "    for beta, color in zip(beta_values, colors):\n",
    "        opt = Momentum(lr=lr, beta=beta)\n",
    "        traj = optimize(opt, func, grad_func, start.copy(), n_steps=100)\n",
    "        \n",
    "        ax.plot(traj[:, 0], traj[:, 1], '-', color=color, \n",
    "                linewidth=2, label=f'β={beta}', alpha=0.8)\n",
    "        ax.plot(traj[-1, 0], traj[-1, 1], 's', color=color, markersize=8)\n",
    "    \n",
    "    ax.plot(start[0], start[1], 'ko', markersize=10, label='Start')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "start = np.array([2.5, 2.5])\n",
    "compare_momentum(elongated_quadratic, elongated_quadratic_grad,\n",
    "                 (-3, 3), (-3, 3), start, [0.0, 0.5, 0.9, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "**Observation:** Higher momentum (β) smooths out oscillations but can overshoot. β=0.9 is a common default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 6. Animated Optimization\n",
    "\n",
    "Watch the optimizers in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_animation(func, grad_func, xlim, ylim, start, n_steps=100, lr_scale=1.0):\n",
    "    \"\"\"Create animated comparison of optimizers.\"\"\"\n",
    "    \n",
    "    # Get trajectories\n",
    "    optimizers = {\n",
    "        'SGD': SGD(lr=0.1 * lr_scale),\n",
    "        'Momentum': Momentum(lr=0.1 * lr_scale, beta=0.9),\n",
    "        'RMSprop': RMSprop(lr=0.1 * lr_scale),\n",
    "        'Adam': Adam(lr=0.1 * lr_scale),\n",
    "    }\n",
    "    \n",
    "    trajectories = {}\n",
    "    for name, opt in optimizers.items():\n",
    "        trajectories[name] = optimize(opt, func, grad_func, start.copy(), n_steps)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Plot contour\n",
    "    x = np.linspace(xlim[0], xlim[1], 100)\n",
    "    y = np.linspace(ylim[0], ylim[1], 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = np.array([[func(np.array([xi, yi])) for xi, yi in zip(row_x, row_y)] \n",
    "                  for row_x, row_y in zip(X, Y)])\n",
    "    Z_plot = np.log(Z + 1)\n",
    "    ax.contourf(X, Y, Z_plot, levels=30, cmap='viridis', alpha=0.5)\n",
    "    ax.contour(X, Y, Z_plot, levels=30, cmap='viridis', alpha=0.7)\n",
    "    \n",
    "    # Initialize lines and points\n",
    "    colors = {'SGD': 'red', 'Momentum': 'blue', 'RMSprop': 'green', 'Adam': 'orange'}\n",
    "    lines = {}\n",
    "    points = {}\n",
    "    \n",
    "    for name in optimizers:\n",
    "        lines[name], = ax.plot([], [], '-', color=colors[name], \n",
    "                               linewidth=2, label=name, alpha=0.8)\n",
    "        points[name], = ax.plot([], [], 'o', color=colors[name], markersize=10)\n",
    "    \n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    \n",
    "    # Animation function\n",
    "    def animate(frame):\n",
    "        for name, traj in trajectories.items():\n",
    "            lines[name].set_data(traj[:frame+1, 0], traj[:frame+1, 1])\n",
    "            points[name].set_data([traj[frame, 0]], [traj[frame, 1]])\n",
    "        ax.set_title(f'Step {frame}')\n",
    "        return list(lines.values()) + list(points.values())\n",
    "    \n",
    "    anim = animation.FuncAnimation(fig, animate, frames=n_steps,\n",
    "                                   interval=50, blit=True)\n",
    "    plt.close()\n",
    "    return anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animated optimization on elongated quadratic\n",
    "start = np.array([2.5, 2.5])\n",
    "anim = create_animation(elongated_quadratic, elongated_quadratic_grad,\n",
    "                        (-3, 3), (-3, 3), start, n_steps=100, lr_scale=0.3)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 7. Convergence Comparison\n",
    "\n",
    "Plot loss vs iterations for quantitative comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_convergence(func, grad_func, start, n_steps=100, lr_scale=1.0):\n",
    "    \"\"\"Plot loss curves for different optimizers.\"\"\"\n",
    "    \n",
    "    optimizers = {\n",
    "        'SGD': SGD(lr=0.1 * lr_scale),\n",
    "        'Momentum': Momentum(lr=0.1 * lr_scale, beta=0.9),\n",
    "        'RMSprop': RMSprop(lr=0.1 * lr_scale),\n",
    "        'Adam': Adam(lr=0.1 * lr_scale),\n",
    "    }\n",
    "    \n",
    "    colors = {'SGD': 'red', 'Momentum': 'blue', 'RMSprop': 'green', 'Adam': 'orange'}\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    for name, opt in optimizers.items():\n",
    "        traj = optimize(opt, func, grad_func, start.copy(), n_steps)\n",
    "        losses = [func(x) for x in traj]\n",
    "        ax.plot(losses, label=name, color=colors[name], linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Convergence Comparison')\n",
    "    ax.legend()\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Quadratic\n",
    "print(\"Quadratic function:\")\n",
    "start = np.array([2.5, 2.5])\n",
    "compare_convergence(quadratic, quadratic_grad, start, n_steps=50)\n",
    "\n",
    "# Elongated quadratic\n",
    "print(\"\\nElongated quadratic:\")\n",
    "compare_convergence(elongated_quadratic, elongated_quadratic_grad, start, \n",
    "                    n_steps=100, lr_scale=0.3)\n",
    "\n",
    "# Rosenbrock\n",
    "print(\"\\nRosenbrock:\")\n",
    "start = np.array([-1.0, 2.0])\n",
    "compare_convergence(rosenbrock, rosenbrock_grad, start, n_steps=500, lr_scale=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 8. Learning Rate Schedules\n",
    "\n",
    "Learning rate schedules can improve convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def constant_lr(step, initial_lr):\n",
    "    \"\"\"Constant learning rate.\"\"\"\n",
    "    return initial_lr\n",
    "\n",
    "def step_decay_lr(step, initial_lr, decay_rate=0.5, decay_steps=25):\n",
    "    \"\"\"Step decay: reduce by decay_rate every decay_steps.\"\"\"\n",
    "    return initial_lr * (decay_rate ** (step // decay_steps))\n",
    "\n",
    "def exponential_decay_lr(step, initial_lr, decay_rate=0.99):\n",
    "    \"\"\"Exponential decay: lr = initial_lr * decay_rate^step.\"\"\"\n",
    "    return initial_lr * (decay_rate ** step)\n",
    "\n",
    "def cosine_annealing_lr(step, initial_lr, total_steps):\n",
    "    \"\"\"Cosine annealing: smooth decay to 0.\"\"\"\n",
    "    return initial_lr * 0.5 * (1 + np.cos(np.pi * step / total_steps))\n",
    "\n",
    "def warmup_cosine_lr(step, initial_lr, warmup_steps, total_steps):\n",
    "    \"\"\"Linear warmup then cosine decay.\"\"\"\n",
    "    if step < warmup_steps:\n",
    "        return initial_lr * step / warmup_steps\n",
    "    else:\n",
    "        progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "        return initial_lr * 0.5 * (1 + np.cos(np.pi * progress))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learning rate schedules\n",
    "total_steps = 100\n",
    "initial_lr = 0.1\n",
    "steps = np.arange(total_steps)\n",
    "\n",
    "schedules = {\n",
    "    'Constant': [constant_lr(s, initial_lr) for s in steps],\n",
    "    'Step Decay': [step_decay_lr(s, initial_lr) for s in steps],\n",
    "    'Exponential': [exponential_decay_lr(s, initial_lr) for s in steps],\n",
    "    'Cosine': [cosine_annealing_lr(s, initial_lr, total_steps) for s in steps],\n",
    "    'Warmup+Cosine': [warmup_cosine_lr(s, initial_lr, 10, total_steps) for s in steps],\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for name, lrs in schedules.items():\n",
    "    ax.plot(steps, lrs, label=name, linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Learning Rate')\n",
    "ax.set_title('Learning Rate Schedules')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare schedules on a problem\n",
    "def optimize_with_schedule(schedule_fn, func, grad_func, start, n_steps, initial_lr, **schedule_kwargs):\n",
    "    \"\"\"Run SGD with a learning rate schedule.\"\"\"\n",
    "    x = start.copy()\n",
    "    trajectory = [x.copy()]\n",
    "    \n",
    "    for step in range(n_steps):\n",
    "        lr = schedule_fn(step, initial_lr, **schedule_kwargs)\n",
    "        grad = grad_func(x)\n",
    "        x = x - lr * grad\n",
    "        trajectory.append(x.copy())\n",
    "    \n",
    "    return np.array(trajectory)\n",
    "\n",
    "# Compare on elongated quadratic\n",
    "start = np.array([2.5, 2.5])\n",
    "n_steps = 100\n",
    "initial_lr = 0.1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "schedule_configs = {\n",
    "    'Constant': {'schedule_fn': constant_lr, 'kwargs': {}},\n",
    "    'Cosine': {'schedule_fn': cosine_annealing_lr, 'kwargs': {'total_steps': n_steps}},\n",
    "    'Warmup+Cosine': {'schedule_fn': warmup_cosine_lr, 'kwargs': {'warmup_steps': 10, 'total_steps': n_steps}},\n",
    "}\n",
    "\n",
    "for name, config in schedule_configs.items():\n",
    "    traj = optimize_with_schedule(\n",
    "        config['schedule_fn'], \n",
    "        elongated_quadratic, \n",
    "        elongated_quadratic_grad,\n",
    "        start.copy(), \n",
    "        n_steps, \n",
    "        initial_lr,\n",
    "        **config['kwargs']\n",
    "    )\n",
    "    losses = [elongated_quadratic(x) for x in traj]\n",
    "    ax.plot(losses, label=name, linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Effect of Learning Rate Schedules (SGD on Elongated Quadratic)')\n",
    "ax.legend()\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "| Optimizer | Key Idea | Best For | Hyperparameters |\n",
    "|-----------|----------|----------|------------------|\n",
    "| **SGD** | θ -= lr * grad | Simple problems | lr |\n",
    "| **Momentum** | Accumulate velocity | Oscillating gradients | lr, β |\n",
    "| **RMSprop** | Adaptive per-param LR | Different curvatures | lr, β, ε |\n",
    "| **Adam** | Momentum + RMSprop | General purpose | lr, β1, β2, ε |\n",
    "\n",
    "**Key takeaways:**\n",
    "\n",
    "1. **Learning rate** is the most important hyperparameter\n",
    "2. **Momentum** helps overcome oscillations in narrow valleys\n",
    "3. **Adaptive methods** (Adam, RMSprop) handle different curvatures automatically\n",
    "4. **Adam** is a good default for most problems\n",
    "5. **Learning rate schedules** can improve final convergence\n",
    "\n",
    "**Next:** [05-rnn-from-scratch.ipynb](05-rnn-from-scratch.ipynb) builds a character-level RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## 10. Exercises\n",
    "\n",
    "1. **Implement AdaGrad:** Similar to RMSprop but accumulates squared gradients without decay.\n",
    "\n",
    "2. **Nesterov momentum:** Implement \"lookahead\" momentum variant.\n",
    "\n",
    "3. **Find optimal learning rate:** For a given problem, implement learning rate finder (train for a few steps at increasing LR, plot loss).\n",
    "\n",
    "4. **Weight decay:** Add L2 regularization to Adam. How does it affect trajectories?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 starter: Implement AdaGrad\n",
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    AdaGrad: Accumulates all past squared gradients.\n",
    "    \n",
    "    s = s + gradient²\n",
    "    θ = θ - lr * gradient / √(s + ε)\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.1, eps=1e-8):\n",
    "        self.lr = lr\n",
    "        self.eps = eps\n",
    "        self.s = None\n",
    "    \n",
    "    def step(self, params, grads):\n",
    "        # Your implementation here\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
