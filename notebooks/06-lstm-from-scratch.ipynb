{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# LSTM from Scratch\n",
    "\n",
    "This notebook implements an LSTM (Long Short-Term Memory) cell from scratch. LSTMs solve the vanishing gradient problem that plagues vanilla RNNs by using gating mechanisms to control information flow.\n",
    "\n",
    "**Goal:** Understand how LSTM gates enable learning of long-range dependencies.\n",
    "\n",
    "**Prerequisites:** [lstms.md](../architectures/lstms.md), [05-rnn-from-scratch.ipynb](05-rnn-from-scratch.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. The LSTM Cell\n",
    "\n",
    "The LSTM has three gates controlling information flow:\n",
    "\n",
    "**Forget gate** - what to discard from cell state:\n",
    "$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n",
    "\n",
    "**Input gate** - what new information to store:\n",
    "$$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\n",
    "$$\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$$\n",
    "\n",
    "**Cell state update** - the \"memory\":\n",
    "$$C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$$\n",
    "\n",
    "**Output gate** - what to output:\n",
    "$$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$\n",
    "$$h_t = o_t \\odot \\tanh(C_t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"Numerically stable sigmoid.\"\"\"\n",
    "    return np.where(\n",
    "        x >= 0,\n",
    "        1 / (1 + np.exp(-x)),\n",
    "        np.exp(x) / (1 + np.exp(x))\n",
    "    )\n",
    "\n",
    "def sigmoid_derivative(s):\n",
    "    \"\"\"Derivative of sigmoid (given sigmoid output).\"\"\"\n",
    "    return s * (1 - s)\n",
    "\n",
    "def tanh_derivative(t):\n",
    "    \"\"\"Derivative of tanh (given tanh output).\"\"\"\n",
    "    return 1 - t**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell:\n",
    "    \"\"\"\n",
    "    Single LSTM cell.\n",
    "    \n",
    "    Processes one timestep, maintaining both hidden state (h) and cell state (c).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: Size of input vectors\n",
    "            hidden_dim: Size of hidden/cell state\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Combined input dimension: [h_{t-1}, x_t]\n",
    "        combined_dim = input_dim + hidden_dim\n",
    "        \n",
    "        # Initialize weights (Xavier)\n",
    "        scale = np.sqrt(1.0 / combined_dim)\n",
    "        \n",
    "        # Forget gate\n",
    "        self.W_f = np.random.randn(combined_dim, hidden_dim) * scale\n",
    "        self.b_f = np.ones(hidden_dim)  # Initialize to 1 (keep by default)\n",
    "        \n",
    "        # Input gate\n",
    "        self.W_i = np.random.randn(combined_dim, hidden_dim) * scale\n",
    "        self.b_i = np.zeros(hidden_dim)\n",
    "        \n",
    "        # Cell candidate\n",
    "        self.W_c = np.random.randn(combined_dim, hidden_dim) * scale\n",
    "        self.b_c = np.zeros(hidden_dim)\n",
    "        \n",
    "        # Output gate\n",
    "        self.W_o = np.random.randn(combined_dim, hidden_dim) * scale\n",
    "        self.b_o = np.zeros(hidden_dim)\n",
    "    \n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        \"\"\"\n",
    "        Forward pass for single timestep.\n",
    "        \n",
    "        Args:\n",
    "            x: Input [batch, input_dim]\n",
    "            h_prev: Previous hidden state [batch, hidden_dim]\n",
    "            c_prev: Previous cell state [batch, hidden_dim]\n",
    "            \n",
    "        Returns:\n",
    "            h: New hidden state\n",
    "            c: New cell state\n",
    "        \"\"\"\n",
    "        # Store inputs for backprop\n",
    "        self.x = x\n",
    "        self.h_prev = h_prev\n",
    "        self.c_prev = c_prev\n",
    "        \n",
    "        # Concatenate input and previous hidden state\n",
    "        self.concat = np.concatenate([h_prev, x], axis=-1)\n",
    "        \n",
    "        # Forget gate\n",
    "        self.f = sigmoid(self.concat @ self.W_f + self.b_f)\n",
    "        \n",
    "        # Input gate\n",
    "        self.i = sigmoid(self.concat @ self.W_i + self.b_i)\n",
    "        \n",
    "        # Cell candidate\n",
    "        self.c_tilde = np.tanh(self.concat @ self.W_c + self.b_c)\n",
    "        \n",
    "        # Output gate\n",
    "        self.o = sigmoid(self.concat @ self.W_o + self.b_o)\n",
    "        \n",
    "        # New cell state: forget old + add new\n",
    "        self.c = self.f * c_prev + self.i * self.c_tilde\n",
    "        \n",
    "        # New hidden state\n",
    "        self.tanh_c = np.tanh(self.c)\n",
    "        self.h = self.o * self.tanh_c\n",
    "        \n",
    "        return self.h, self.c\n",
    "    \n",
    "    def backward(self, dh, dc):\n",
    "        \"\"\"\n",
    "        Backward pass for single timestep.\n",
    "        \n",
    "        Args:\n",
    "            dh: Gradient w.r.t. hidden state\n",
    "            dc: Gradient w.r.t. cell state (from future)\n",
    "            \n",
    "        Returns:\n",
    "            dx: Gradient w.r.t. input\n",
    "            dh_prev: Gradient w.r.t. previous hidden state\n",
    "            dc_prev: Gradient w.r.t. previous cell state\n",
    "            grads: Dictionary of parameter gradients\n",
    "        \"\"\"\n",
    "        # Gradient from hidden state output\n",
    "        do = dh * self.tanh_c\n",
    "        dc_from_h = dh * self.o * tanh_derivative(self.tanh_c)\n",
    "        \n",
    "        # Total cell gradient\n",
    "        dc_total = dc + dc_from_h\n",
    "        \n",
    "        # Gradient through cell state update\n",
    "        df = dc_total * self.c_prev\n",
    "        di = dc_total * self.c_tilde\n",
    "        dc_tilde = dc_total * self.i\n",
    "        dc_prev = dc_total * self.f\n",
    "        \n",
    "        # Gradient through gates (sigmoid derivative)\n",
    "        df_raw = df * sigmoid_derivative(self.f)\n",
    "        di_raw = di * sigmoid_derivative(self.i)\n",
    "        dc_tilde_raw = dc_tilde * tanh_derivative(self.c_tilde)\n",
    "        do_raw = do * sigmoid_derivative(self.o)\n",
    "        \n",
    "        # Parameter gradients\n",
    "        dW_f = self.concat.T @ df_raw if len(df_raw.shape) > 1 else np.outer(self.concat, df_raw)\n",
    "        dW_i = self.concat.T @ di_raw if len(di_raw.shape) > 1 else np.outer(self.concat, di_raw)\n",
    "        dW_c = self.concat.T @ dc_tilde_raw if len(dc_tilde_raw.shape) > 1 else np.outer(self.concat, dc_tilde_raw)\n",
    "        dW_o = self.concat.T @ do_raw if len(do_raw.shape) > 1 else np.outer(self.concat, do_raw)\n",
    "        \n",
    "        db_f = df_raw.sum(axis=0) if len(df_raw.shape) > 1 else df_raw\n",
    "        db_i = di_raw.sum(axis=0) if len(di_raw.shape) > 1 else di_raw\n",
    "        db_c = dc_tilde_raw.sum(axis=0) if len(dc_tilde_raw.shape) > 1 else dc_tilde_raw\n",
    "        db_o = do_raw.sum(axis=0) if len(do_raw.shape) > 1 else do_raw\n",
    "        \n",
    "        # Gradient through concatenation\n",
    "        dconcat = (df_raw @ self.W_f.T + di_raw @ self.W_i.T + \n",
    "                   dc_tilde_raw @ self.W_c.T + do_raw @ self.W_o.T)\n",
    "        \n",
    "        # Split gradient for h_prev and x\n",
    "        dh_prev = dconcat[..., :self.hidden_dim]\n",
    "        dx = dconcat[..., self.hidden_dim:]\n",
    "        \n",
    "        grads = {\n",
    "            'W_f': dW_f, 'b_f': db_f,\n",
    "            'W_i': dW_i, 'b_i': db_i,\n",
    "            'W_c': dW_c, 'b_c': db_c,\n",
    "            'W_o': dW_o, 'b_o': db_o,\n",
    "        }\n",
    "        \n",
    "        return dx, dh_prev, dc_prev, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the LSTM cell\n",
    "cell = LSTMCell(input_dim=10, hidden_dim=20)\n",
    "\n",
    "x = np.random.randn(10)  # Single input\n",
    "h_prev = np.zeros(20)\n",
    "c_prev = np.zeros(20)\n",
    "\n",
    "h, c = cell.forward(x, h_prev, c_prev)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Hidden state shape: {h.shape}\")\n",
    "print(f\"Cell state shape: {c.shape}\")\n",
    "print(f\"\\nGate values (should be in [0,1]):\")\n",
    "print(f\"  Forget gate: [{cell.f.min():.3f}, {cell.f.max():.3f}]\")\n",
    "print(f\"  Input gate:  [{cell.i.min():.3f}, {cell.i.max():.3f}]\")\n",
    "print(f\"  Output gate: [{cell.o.min():.3f}, {cell.o.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 2. Character-Level LSTM\n",
    "\n",
    "Now let's build a full LSTM for character-level text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLSTM:\n",
    "    \"\"\"\n",
    "    Character-level LSTM for text generation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # LSTM cell\n",
    "        self.cell = LSTMCell(vocab_size, hidden_dim)\n",
    "        \n",
    "        # Output layer\n",
    "        scale = np.sqrt(1.0 / hidden_dim)\n",
    "        self.W_hy = np.random.randn(hidden_dim, vocab_size) * scale\n",
    "        self.b_y = np.zeros(vocab_size)\n",
    "    \n",
    "    def forward(self, inputs, h_init=None, c_init=None):\n",
    "        \"\"\"\n",
    "        Forward pass through sequence.\n",
    "        \"\"\"\n",
    "        seq_len = len(inputs)\n",
    "        \n",
    "        if h_init is None:\n",
    "            h_init = np.zeros(self.hidden_dim)\n",
    "        if c_init is None:\n",
    "            c_init = np.zeros(self.hidden_dim)\n",
    "        \n",
    "        # Storage\n",
    "        self.inputs = inputs\n",
    "        self.xs = {}\n",
    "        self.hs = {-1: h_init}\n",
    "        self.cs = {-1: c_init}\n",
    "        self.os = {}\n",
    "        self.ps = {}\n",
    "        \n",
    "        # Store cell states for backprop\n",
    "        self.cells = {}\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            # One-hot encode\n",
    "            x = np.zeros(self.vocab_size)\n",
    "            x[inputs[t]] = 1\n",
    "            self.xs[t] = x\n",
    "            \n",
    "            # Create fresh cell for this timestep (to store intermediate values)\n",
    "            self.cells[t] = LSTMCell(self.vocab_size, self.hidden_dim)\n",
    "            # Copy weights\n",
    "            for attr in ['W_f', 'b_f', 'W_i', 'b_i', 'W_c', 'b_c', 'W_o', 'b_o']:\n",
    "                setattr(self.cells[t], attr, getattr(self.cell, attr))\n",
    "            \n",
    "            # LSTM step\n",
    "            self.hs[t], self.cs[t] = self.cells[t].forward(x, self.hs[t-1], self.cs[t-1])\n",
    "            \n",
    "            # Output\n",
    "            self.os[t] = self.hs[t] @ self.W_hy + self.b_y\n",
    "            \n",
    "            # Softmax\n",
    "            exp_o = np.exp(self.os[t] - self.os[t].max())\n",
    "            self.ps[t] = exp_o / exp_o.sum()\n",
    "        \n",
    "        return self.ps, self.hs, self.cs\n",
    "    \n",
    "    def loss(self, targets):\n",
    "        \"\"\"Cross-entropy loss.\"\"\"\n",
    "        loss = 0\n",
    "        for t in range(len(targets)):\n",
    "            loss -= np.log(self.ps[t][targets[t]] + 1e-10)\n",
    "        return loss / len(targets)\n",
    "    \n",
    "    def backward(self, targets):\n",
    "        \"\"\"Backpropagation through time.\"\"\"\n",
    "        seq_len = len(targets)\n",
    "        \n",
    "        # Initialize gradients\n",
    "        dW_f = np.zeros_like(self.cell.W_f)\n",
    "        dW_i = np.zeros_like(self.cell.W_i)\n",
    "        dW_c = np.zeros_like(self.cell.W_c)\n",
    "        dW_o = np.zeros_like(self.cell.W_o)\n",
    "        db_f = np.zeros_like(self.cell.b_f)\n",
    "        db_i = np.zeros_like(self.cell.b_i)\n",
    "        db_c = np.zeros_like(self.cell.b_c)\n",
    "        db_o = np.zeros_like(self.cell.b_o)\n",
    "        dW_hy = np.zeros_like(self.W_hy)\n",
    "        db_y = np.zeros_like(self.b_y)\n",
    "        \n",
    "        # Gradients flowing back\n",
    "        dh_next = np.zeros(self.hidden_dim)\n",
    "        dc_next = np.zeros(self.hidden_dim)\n",
    "        \n",
    "        for t in reversed(range(seq_len)):\n",
    "            # Output gradient\n",
    "            do = self.ps[t].copy()\n",
    "            do[targets[t]] -= 1\n",
    "            \n",
    "            # Output layer gradients\n",
    "            dW_hy += np.outer(self.hs[t], do)\n",
    "            db_y += do\n",
    "            \n",
    "            # Gradient to hidden state\n",
    "            dh = do @ self.W_hy.T + dh_next\n",
    "            \n",
    "            # LSTM backward\n",
    "            dx, dh_next, dc_next, cell_grads = self.cells[t].backward(dh, dc_next)\n",
    "            \n",
    "            # Accumulate LSTM gradients\n",
    "            dW_f += cell_grads['W_f']\n",
    "            dW_i += cell_grads['W_i']\n",
    "            dW_c += cell_grads['W_c']\n",
    "            dW_o += cell_grads['W_o']\n",
    "            db_f += cell_grads['b_f']\n",
    "            db_i += cell_grads['b_i']\n",
    "            db_c += cell_grads['b_c']\n",
    "            db_o += cell_grads['b_o']\n",
    "        \n",
    "        # Clip gradients\n",
    "        for grad in [dW_f, dW_i, dW_c, dW_o, db_f, db_i, db_c, db_o, dW_hy, db_y]:\n",
    "            np.clip(grad, -5, 5, out=grad)\n",
    "        \n",
    "        return {\n",
    "            'W_f': dW_f, 'b_f': db_f,\n",
    "            'W_i': dW_i, 'b_i': db_i,\n",
    "            'W_c': dW_c, 'b_c': db_c,\n",
    "            'W_o': dW_o, 'b_o': db_o,\n",
    "            'W_hy': dW_hy, 'b_y': db_y,\n",
    "        }\n",
    "    \n",
    "    def update(self, grads, lr):\n",
    "        \"\"\"Update parameters.\"\"\"\n",
    "        self.cell.W_f -= lr * grads['W_f']\n",
    "        self.cell.W_i -= lr * grads['W_i']\n",
    "        self.cell.W_c -= lr * grads['W_c']\n",
    "        self.cell.W_o -= lr * grads['W_o']\n",
    "        self.cell.b_f -= lr * grads['b_f']\n",
    "        self.cell.b_i -= lr * grads['b_i']\n",
    "        self.cell.b_c -= lr * grads['b_c']\n",
    "        self.cell.b_o -= lr * grads['b_o']\n",
    "        self.W_hy -= lr * grads['W_hy']\n",
    "        self.b_y -= lr * grads['b_y']\n",
    "    \n",
    "    def sample(self, seed_char, length, temperature=1.0):\n",
    "        \"\"\"Generate text.\"\"\"\n",
    "        h = np.zeros(self.hidden_dim)\n",
    "        c = np.zeros(self.hidden_dim)\n",
    "        x_idx = seed_char\n",
    "        generated = [x_idx]\n",
    "        \n",
    "        for _ in range(length):\n",
    "            x = np.zeros(self.vocab_size)\n",
    "            x[x_idx] = 1\n",
    "            \n",
    "            h, c = self.cell.forward(x, h, c)\n",
    "            o = h @ self.W_hy + self.b_y\n",
    "            \n",
    "            o = o / temperature\n",
    "            exp_o = np.exp(o - o.max())\n",
    "            probs = exp_o / exp_o.sum()\n",
    "            \n",
    "            x_idx = np.random.choice(self.vocab_size, p=probs)\n",
    "            generated.append(x_idx)\n",
    "        \n",
    "        return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "text = \"\"\"To be, or not to be, that is the question:\n",
    "Whether 'tis nobler in the mind to suffer\n",
    "The slings and arrows of outrageous fortune,\n",
    "Or to take arms against a sea of troubles\n",
    "And by opposing end them. To die: to sleep;\n",
    "No more; and by a sleep to say we end\n",
    "The heart-ache and the thousand natural shocks\n",
    "That flesh is heir to. 'Tis a consummation\n",
    "Devoutly to be wish'd. To die, to sleep;\n",
    "To sleep: perchance to dream: ay, there's the rub.\"\"\"\n",
    "\n",
    "# Create vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "data = [char_to_idx[ch] for ch in text]\n",
    "\n",
    "print(f\"Text length: {len(text)}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(model, data, seq_length=25, epochs=100, lr=0.1, print_every=10):\n",
    "    \"\"\"Train the LSTM.\"\"\"\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        h_prev = np.zeros(model.hidden_dim)\n",
    "        c_prev = np.zeros(model.hidden_dim)\n",
    "        epoch_loss = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for i in range(0, len(data) - seq_length - 1, seq_length):\n",
    "            inputs = data[i:i+seq_length]\n",
    "            targets = data[i+1:i+seq_length+1]\n",
    "            \n",
    "            # Forward\n",
    "            probs, hidden_states, cell_states = model.forward(inputs, h_prev, c_prev)\n",
    "            loss = model.loss(targets)\n",
    "            \n",
    "            # Backward\n",
    "            grads = model.backward(targets)\n",
    "            model.update(grads, lr)\n",
    "            \n",
    "            # Carry state forward\n",
    "            h_prev = hidden_states[seq_length - 1].copy()\n",
    "            c_prev = cell_states[seq_length - 1].copy()\n",
    "            \n",
    "            epoch_loss += loss\n",
    "            n_batches += 1\n",
    "        \n",
    "        avg_loss = epoch_loss / n_batches\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        if epoch % print_every == 0:\n",
    "            print(f\"Epoch {epoch:3d}: Loss = {avg_loss:.4f}\")\n",
    "            sample_idx = model.sample(data[0], 100, temperature=0.8)\n",
    "            sample_text = ''.join([idx_to_char[i] for i in sample_idx])\n",
    "            print(f\"  Sample: {sample_text[:60]}...\\n\")\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM\n",
    "lstm_model = CharLSTM(vocab_size=vocab_size, hidden_dim=100)\n",
    "\n",
    "print(\"Training LSTM...\\n\")\n",
    "lstm_losses = train_lstm(lstm_model, data, seq_length=25, epochs=200, lr=0.1, print_every=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(lstm_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('LSTM Training Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 4. Visualizing the Gates\n",
    "\n",
    "Let's see how the gates behave when processing text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gates(model, text_sample, char_to_idx, idx_to_char):\n",
    "    \"\"\"Visualize gate activations for a text sample.\"\"\"\n",
    "    data_sample = [char_to_idx[ch] for ch in text_sample]\n",
    "    \n",
    "    # Forward pass\n",
    "    model.forward(data_sample)\n",
    "    \n",
    "    # Extract gate values\n",
    "    forget_gates = []\n",
    "    input_gates = []\n",
    "    output_gates = []\n",
    "    \n",
    "    for t in range(len(data_sample)):\n",
    "        forget_gates.append(model.cells[t].f)\n",
    "        input_gates.append(model.cells[t].i)\n",
    "        output_gates.append(model.cells[t].o)\n",
    "    \n",
    "    F = np.array(forget_gates)\n",
    "    I = np.array(input_gates)\n",
    "    O = np.array(output_gates)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "    \n",
    "    for ax, gate, name in zip(axes, [F, I, O], ['Forget Gate', 'Input Gate', 'Output Gate']):\n",
    "        im = ax.imshow(gate.T[:20], aspect='auto', cmap='RdYlGn', vmin=0, vmax=1)\n",
    "        ax.set_xlabel('Time step')\n",
    "        ax.set_ylabel('Hidden unit')\n",
    "        ax.set_title(name)\n",
    "        ax.set_xticks(range(len(text_sample)))\n",
    "        ax.set_xticklabels(list(text_sample), fontsize=8)\n",
    "        plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return F, I, O\n",
    "\n",
    "# Visualize on a short sample\n",
    "test_sample = \"To be, or not to be\"\n",
    "F, I, O = visualize_gates(lstm_model, test_sample, char_to_idx, idx_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "**Gate interpretations:**\n",
    "- **Forget gate near 1:** Keep this information\n",
    "- **Forget gate near 0:** Clear this information\n",
    "- **Input gate high:** Add new information here\n",
    "- **Output gate high:** This hidden unit should influence output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 5. Comparing Gradient Flow: RNN vs LSTM\n",
    "\n",
    "The key advantage of LSTMs is better gradient flow through the cell state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple RNN for comparison\n",
    "class SimpleRNN:\n",
    "    \"\"\"Vanilla RNN for comparison.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        scale_xh = np.sqrt(1.0 / vocab_size)\n",
    "        scale_hh = np.sqrt(1.0 / hidden_dim)\n",
    "        \n",
    "        self.W_xh = np.random.randn(vocab_size, hidden_dim) * scale_xh\n",
    "        self.W_hh = np.random.randn(hidden_dim, hidden_dim) * scale_hh\n",
    "        self.b_h = np.zeros(hidden_dim)\n",
    "        self.W_hy = np.random.randn(hidden_dim, vocab_size) * np.sqrt(1.0 / hidden_dim)\n",
    "        self.b_y = np.zeros(vocab_size)\n",
    "    \n",
    "    def forward(self, inputs, h_init=None):\n",
    "        if h_init is None:\n",
    "            h_init = np.zeros(self.hidden_dim)\n",
    "        \n",
    "        self.xs = {}\n",
    "        self.hs = {-1: h_init}\n",
    "        self.ps = {}\n",
    "        \n",
    "        for t in range(len(inputs)):\n",
    "            x = np.zeros(self.vocab_size)\n",
    "            x[inputs[t]] = 1\n",
    "            self.xs[t] = x\n",
    "            \n",
    "            self.hs[t] = np.tanh(x @ self.W_xh + self.hs[t-1] @ self.W_hh + self.b_h)\n",
    "            \n",
    "            o = self.hs[t] @ self.W_hy + self.b_y\n",
    "            exp_o = np.exp(o - o.max())\n",
    "            self.ps[t] = exp_o / exp_o.sum()\n",
    "        \n",
    "        return self.ps, self.hs\n",
    "    \n",
    "    def measure_gradient_flow(self, targets):\n",
    "        \"\"\"Measure gradient magnitude at each timestep.\"\"\"\n",
    "        seq_len = len(targets)\n",
    "        \n",
    "        dh_next = np.zeros(self.hidden_dim)\n",
    "        gradient_norms = []\n",
    "        \n",
    "        for t in reversed(range(seq_len)):\n",
    "            do = self.ps[t].copy()\n",
    "            do[targets[t]] -= 1\n",
    "            \n",
    "            dh = do @ self.W_hy.T + dh_next\n",
    "            dh_raw = dh * (1 - self.hs[t]**2)\n",
    "            \n",
    "            gradient_norms.append(np.linalg.norm(dh_raw))\n",
    "            \n",
    "            dh_next = dh_raw @ self.W_hh.T\n",
    "        \n",
    "        return gradient_norms[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare gradient flow\n",
    "seq_len = 50\n",
    "test_inputs = data[:seq_len]\n",
    "test_targets = data[1:seq_len+1]\n",
    "\n",
    "# RNN\n",
    "rnn = SimpleRNN(vocab_size, 100)\n",
    "rnn.forward(test_inputs)\n",
    "rnn_grads = rnn.measure_gradient_flow(test_targets)\n",
    "\n",
    "# LSTM - measure gradient through cell state pathway\n",
    "def measure_lstm_gradient_flow(model, inputs, targets):\n",
    "    \"\"\"Measure LSTM gradient flow.\"\"\"\n",
    "    model.forward(inputs)\n",
    "    \n",
    "    dh_next = np.zeros(model.hidden_dim)\n",
    "    dc_next = np.zeros(model.hidden_dim)\n",
    "    gradient_norms = []\n",
    "    \n",
    "    for t in reversed(range(len(targets))):\n",
    "        do = model.ps[t].copy()\n",
    "        do[targets[t]] -= 1\n",
    "        \n",
    "        dh = do @ model.W_hy.T + dh_next\n",
    "        \n",
    "        # Total gradient magnitude\n",
    "        gradient_norms.append(np.linalg.norm(dh) + np.linalg.norm(dc_next))\n",
    "        \n",
    "        dx, dh_next, dc_next, _ = model.cells[t].backward(dh, dc_next)\n",
    "    \n",
    "    return gradient_norms[::-1]\n",
    "\n",
    "lstm_grads = measure_lstm_gradient_flow(lstm_model, test_inputs, test_targets)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Normalize for comparison\n",
    "rnn_grads = np.array(rnn_grads)\n",
    "lstm_grads = np.array(lstm_grads)\n",
    "\n",
    "if rnn_grads[-1] > 0:\n",
    "    rnn_grads_norm = rnn_grads / rnn_grads[-1]\n",
    "else:\n",
    "    rnn_grads_norm = rnn_grads\n",
    "    \n",
    "if lstm_grads[-1] > 0:\n",
    "    lstm_grads_norm = lstm_grads / lstm_grads[-1]\n",
    "else:\n",
    "    lstm_grads_norm = lstm_grads\n",
    "\n",
    "plt.plot(rnn_grads_norm, label='RNN', linewidth=2)\n",
    "plt.plot(lstm_grads_norm, label='LSTM', linewidth=2)\n",
    "plt.xlabel('Position in sequence')\n",
    "plt.ylabel('Relative gradient magnitude')\n",
    "plt.title('Gradient Flow: RNN vs LSTM')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Gradient at position 0 relative to final:\")\n",
    "print(f\"  RNN:  {rnn_grads_norm[0]:.2e}\")\n",
    "print(f\"  LSTM: {lstm_grads_norm[0]:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "**Key insight:** The LSTM maintains much stronger gradients for early timesteps. The cell state provides a \"gradient highway\" that bypasses the multiplicative vanishing of vanilla RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 6. Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate with different temperatures\n",
    "print(\"Generated text:\\n\")\n",
    "\n",
    "for temp in [0.5, 0.8, 1.0, 1.5]:\n",
    "    seed = char_to_idx['T']\n",
    "    sample_idx = lstm_model.sample(seed, 150, temperature=temp)\n",
    "    sample_text = ''.join([idx_to_char[i] for i in sample_idx])\n",
    "    print(f\"Temperature {temp}:\")\n",
    "    print(f\"  {sample_text}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "| Component | Formula | Purpose |\n",
    "|-----------|---------|--------|\n",
    "| **Forget gate** | $f = \\sigma(W_f[h,x] + b_f)$ | What to forget |\n",
    "| **Input gate** | $i = \\sigma(W_i[h,x] + b_i)$ | What to add |\n",
    "| **Cell candidate** | $\\tilde{C} = \\tanh(W_c[h,x] + b_c)$ | What to add |\n",
    "| **Cell state** | $C = f \\odot C_{prev} + i \\odot \\tilde{C}$ | Long-term memory |\n",
    "| **Output gate** | $o = \\sigma(W_o[h,x] + b_o)$ | What to output |\n",
    "| **Hidden state** | $h = o \\odot \\tanh(C)$ | Short-term output |\n",
    "\n",
    "**Key takeaways:**\n",
    "\n",
    "1. **Gates control information flow** - multiplicative gating is powerful\n",
    "2. **Cell state is the key** - provides a \"highway\" for gradient flow\n",
    "3. **Forget gate initialized to 1** - default is to remember\n",
    "4. **More parameters than RNN** - but much better at long sequences\n",
    "\n",
    "**Historical note:** LSTMs (1997) dominated sequence modeling until transformers (2017). Many modern systems still use LSTMs for smaller-scale tasks.\n",
    "\n",
    "**Next:** [07-attention-from-scratch.ipynb](07-attention-from-scratch.ipynb) introduces attention, which eventually replaced LSTMs for most NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 8. Exercises\n",
    "\n",
    "1. **GRU:** Implement a GRU cell (simpler gating than LSTM).\n",
    "\n",
    "2. **Peephole connections:** Add connections from cell state to gates.\n",
    "\n",
    "3. **Bidirectional LSTM:** Process sequence in both directions.\n",
    "\n",
    "4. **Longer sequences:** Train on longer sequences. How does LSTM compare to RNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 starter: Implement GRU\n",
    "class GRUCell:\n",
    "    \"\"\"\n",
    "    Gated Recurrent Unit.\n",
    "    \n",
    "    Simpler than LSTM: 2 gates instead of 3.\n",
    "    \n",
    "    Reset gate: r = sigmoid(W_r[h,x])\n",
    "    Update gate: z = sigmoid(W_z[h,x])  \n",
    "    Candidate: h_tilde = tanh(W_h[r*h, x])\n",
    "    Output: h = (1-z)*h_prev + z*h_tilde\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        # Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, h_prev):\n",
    "        # Your implementation here\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
