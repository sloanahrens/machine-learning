{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention from Scratch\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Implement scaled dot-product attention in NumPy\n",
    "2. Build intuition for query, key, value through visualizations\n",
    "3. Visualize attention weights as heatmaps\n",
    "4. Implement causal (masked) attention for autoregressive models\n",
    "5. Compare attention to RNN hidden state bottleneck\n",
    "\n",
    "**Prerequisites:** [attention](../transformers/attention.md), [self-attention](../transformers/self-attention.md)\n",
    "\n",
    "**Key Insight:** Attention replaces sequential processing with content-based routing. Every position can directly access every other position, weighted by learned relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Attention Equation\n",
    "\n",
    "The core attention formula:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Breaking this down:\n",
    "- **Q (Query):** What we're looking for\n",
    "- **K (Key):** What's available to match against\n",
    "- **V (Value):** What we actually retrieve\n",
    "- **Scaling by sqrt(d_k):** Prevents dot products from getting too large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=-1):\n",
    "    \"\"\"Numerically stable softmax.\"\"\"\n",
    "    exp_x = np.exp(x - x.max(axis=axis, keepdims=True))\n",
    "    return exp_x / exp_x.sum(axis=axis, keepdims=True)\n",
    "\n",
    "def attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Queries (seq_len_q, d_k)\n",
    "        K: Keys (seq_len_k, d_k)\n",
    "        V: Values (seq_len_k, d_v)\n",
    "        mask: Optional mask (seq_len_q, seq_len_k), True = attend\n",
    "    \n",
    "    Returns:\n",
    "        output: (seq_len_q, d_v)\n",
    "        weights: (seq_len_q, seq_len_k)\n",
    "    \"\"\"\n",
    "    d_k = K.shape[-1]\n",
    "    \n",
    "    # Step 1: Compute similarity scores\n",
    "    scores = Q @ K.T / np.sqrt(d_k)  # (seq_len_q, seq_len_k)\n",
    "    \n",
    "    # Step 2: Apply mask if provided\n",
    "    if mask is not None:\n",
    "        scores = np.where(mask, scores, -np.inf)\n",
    "    \n",
    "    # Step 3: Convert to probabilities\n",
    "    weights = softmax(scores, axis=-1)\n",
    "    \n",
    "    # Step 4: Weighted sum of values\n",
    "    output = weights @ V\n",
    "    \n",
    "    return output, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Why Scale by sqrt(d_k)?\n",
    "\n",
    "Without scaling, as dimension increases, dot products grow in variance. This pushes softmax into near-one-hot outputs where gradients vanish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the scaling problem\n",
    "dimensions = [8, 32, 128, 512, 2048]\n",
    "\n",
    "fig, axes = plt.subplots(2, len(dimensions), figsize=(15, 5))\n",
    "fig.suptitle('Effect of Dimension on Dot Product Distribution', fontsize=12)\n",
    "\n",
    "for i, d in enumerate(dimensions):\n",
    "    # Random unit vectors\n",
    "    q = np.random.randn(1000, d)\n",
    "    k = np.random.randn(1000, d)\n",
    "    \n",
    "    # Unscaled dot products\n",
    "    unscaled = np.sum(q * k, axis=1)\n",
    "    \n",
    "    # Scaled dot products\n",
    "    scaled = unscaled / np.sqrt(d)\n",
    "    \n",
    "    # Plot unscaled\n",
    "    axes[0, i].hist(unscaled, bins=30, density=True, alpha=0.7, color='red')\n",
    "    axes[0, i].set_title(f'd={d}\\nVar={unscaled.var():.1f}')\n",
    "    axes[0, i].set_xlim(-60, 60)\n",
    "    if i == 0:\n",
    "        axes[0, i].set_ylabel('Unscaled')\n",
    "    \n",
    "    # Plot scaled\n",
    "    axes[1, i].hist(scaled, bins=30, density=True, alpha=0.7, color='blue')\n",
    "    axes[1, i].set_title(f'Var={scaled.var():.2f}')\n",
    "    axes[1, i].set_xlim(-4, 4)\n",
    "    if i == 0:\n",
    "        axes[1, i].set_ylabel('Scaled by sqrt(d)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nWithout scaling: variance grows with dimension\")\n",
    "print(\"With scaling: variance stays around 1 regardless of dimension\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show effect on softmax\n",
    "d = 512\n",
    "seq_len = 10\n",
    "\n",
    "Q = np.random.randn(seq_len, d)\n",
    "K = np.random.randn(seq_len, d)\n",
    "\n",
    "# Unscaled scores\n",
    "scores_unscaled = Q @ K.T\n",
    "weights_unscaled = softmax(scores_unscaled, axis=-1)\n",
    "\n",
    "# Scaled scores\n",
    "scores_scaled = scores_unscaled / np.sqrt(d)\n",
    "weights_scaled = softmax(scores_scaled, axis=-1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Unscaled\n",
    "im1 = axes[0].imshow(weights_unscaled, cmap='Blues', vmin=0, vmax=1)\n",
    "axes[0].set_title(f'Unscaled: Near one-hot\\nMax weight: {weights_unscaled.max():.4f}')\n",
    "axes[0].set_xlabel('Key Position')\n",
    "axes[0].set_ylabel('Query Position')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Scaled\n",
    "im2 = axes[1].imshow(weights_scaled, cmap='Blues', vmin=0, vmax=1)\n",
    "axes[1].set_title(f'Scaled: Smoother distribution\\nMax weight: {weights_scaled.max():.4f}')\n",
    "axes[1].set_xlabel('Key Position')\n",
    "axes[1].set_ylabel('Query Position')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Query-Key-Value Intuition\n",
    "\n",
    "Think of attention like a **database lookup**:\n",
    "- **Query:** Your search term\n",
    "- **Keys:** Index entries\n",
    "- **Values:** The actual data stored\n",
    "\n",
    "Unlike a database, attention returns a *weighted combination* of all values, not just the best match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a \"memory retrieval\" system\n",
    "# Memories: stored facts with associated vectors\n",
    "\n",
    "memories = [\n",
    "    (\"Paris is the capital of France\", np.array([0.9, 0.1, 0.0, 0.0])),\n",
    "    (\"Berlin is the capital of Germany\", np.array([0.8, 0.2, 0.0, 0.0])),\n",
    "    (\"The Eiffel Tower is in Paris\", np.array([0.7, 0.0, 0.3, 0.0])),\n",
    "    (\"French cuisine includes croissants\", np.array([0.6, 0.0, 0.0, 0.4])),\n",
    "    (\"German cars are well-engineered\", np.array([0.0, 0.9, 0.1, 0.0])),\n",
    "]\n",
    "\n",
    "# Keys: semantic vectors for matching\n",
    "# Values: the actual facts (represented as one-hot for retrieval)\n",
    "K = np.array([m[1] for m in memories])\n",
    "V = np.eye(len(memories))  # One-hot encoding of each memory\n",
    "\n",
    "# Query: \"Tell me about France\"\n",
    "Q_france = np.array([[0.85, 0.0, 0.1, 0.05]])\n",
    "\n",
    "# Query: \"Tell me about German engineering\"\n",
    "Q_german = np.array([[0.0, 0.8, 0.2, 0.0]])\n",
    "\n",
    "def show_retrieval(query, query_name):\n",
    "    output, weights = attention(query, K, V)\n",
    "    \n",
    "    print(f\"\\n{query_name}:\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, (fact, _) in enumerate(memories):\n",
    "        bar = \"#\" * int(weights[0, i] * 40)\n",
    "        print(f\"{weights[0, i]:.3f} {bar}\")\n",
    "        print(f\"      {fact}\")\n",
    "\n",
    "show_retrieval(Q_france, \"Query: 'Tell me about France'\")\n",
    "show_retrieval(Q_german, \"Query: 'Tell me about German engineering'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Self-Attention: Sequence Talks to Itself\n",
    "\n",
    "In self-attention, Q, K, and V all come from the same sequence. Each position asks: \"Which other positions are relevant to me?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention:\n",
    "    \"\"\"Self-attention with learnable projections.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, d_k=None, d_v=None):\n",
    "        d_k = d_k or d_model\n",
    "        d_v = d_v or d_model\n",
    "        \n",
    "        # Learnable projection matrices\n",
    "        self.W_Q = np.random.randn(d_model, d_k) / np.sqrt(d_model)\n",
    "        self.W_K = np.random.randn(d_model, d_k) / np.sqrt(d_model)\n",
    "        self.W_V = np.random.randn(d_model, d_v) / np.sqrt(d_model)\n",
    "        self.d_k = d_k\n",
    "    \n",
    "    def forward(self, X, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: Input sequence (seq_len, d_model)\n",
    "            mask: Optional mask (seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        Q = X @ self.W_Q\n",
    "        K = X @ self.W_K\n",
    "        V = X @ self.W_V\n",
    "        \n",
    "        return attention(Q, K, V, mask)\n",
    "\n",
    "# Test self-attention\n",
    "d_model = 32\n",
    "seq_len = 6\n",
    "\n",
    "sa = SelfAttention(d_model)\n",
    "X = np.random.randn(seq_len, d_model)\n",
    "\n",
    "output, weights = sa.forward(X)\n",
    "\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nAttention weights (each row sums to 1):\")\n",
    "print(weights.round(3))\n",
    "print(f\"\\nRow sums: {weights.sum(axis=1).round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing Attention on Real Text\n",
    "\n",
    "Let's create a simple example where we can interpret what the attention is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_heatmap(tokens, weights, title=\"Attention Weights\"):\n",
    "    \"\"\"\n",
    "    Visualize attention weights as a heatmap.\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of token strings\n",
    "        weights: Attention matrix (n, n)\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    im = ax.imshow(weights, cmap='Blues', aspect='auto')\n",
    "    \n",
    "    # Set ticks\n",
    "    ax.set_xticks(np.arange(len(tokens)))\n",
    "    ax.set_yticks(np.arange(len(tokens)))\n",
    "    ax.set_xticklabels(tokens, fontsize=10)\n",
    "    ax.set_yticklabels(tokens, fontsize=10)\n",
    "    \n",
    "    # Rotate x labels\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens)):\n",
    "            text = ax.text(j, i, f\"{weights[i, j]:.2f}\",\n",
    "                          ha=\"center\", va=\"center\", \n",
    "                          color=\"white\" if weights[i, j] > 0.5 else \"black\",\n",
    "                          fontsize=8)\n",
    "    \n",
    "    ax.set_xlabel(\"Keys (attending to)\")\n",
    "    ax.set_ylabel(\"Queries (from)\")\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    plt.colorbar(im, ax=ax, label=\"Attention Weight\")\n",
    "    plt.tight_layout()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate learned attention patterns\n",
    "# Create embeddings where similar tokens have similar vectors\n",
    "\n",
    "tokens = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "\n",
    "# Create embeddings with semantic structure\n",
    "d_model = 16\n",
    "embeddings = {\n",
    "    \"The\": np.array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
    "    \"the\": np.array([0.9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
    "    \"cat\": np.array([0, 1, 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
    "    \"sat\": np.array([0, 0, 0, 1, 0, 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
    "    \"on\":  np.array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
    "    \"mat\": np.array([0, 0.5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
    "}\n",
    "\n",
    "# Build input matrix\n",
    "X = np.array([embeddings.get(t, embeddings.get(t.lower(), np.random.randn(d_model))) for t in tokens])\n",
    "X = X + np.random.randn(*X.shape) * 0.1  # Add noise\n",
    "\n",
    "# Apply self-attention\n",
    "sa = SelfAttention(d_model, d_k=8)\n",
    "output, weights = sa.forward(X)\n",
    "\n",
    "plot_attention_heatmap(tokens, weights, \"Self-Attention: 'The cat sat on the mat'\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNotice: Each row shows which tokens that query position attends to.\")\n",
    "print(\"The pattern depends on the learned W_Q, W_K, W_V matrices.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Causal (Masked) Attention\n",
    "\n",
    "For autoregressive models (like GPT), position i can only attend to positions <= i. We enforce this with a causal mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"Create lower-triangular causal mask.\"\"\"\n",
    "    return np.tril(np.ones((seq_len, seq_len), dtype=bool))\n",
    "\n",
    "# Visualize the mask\n",
    "seq_len = 6\n",
    "mask = create_causal_mask(seq_len)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "ax.imshow(mask, cmap='Greens', aspect='auto')\n",
    "ax.set_title('Causal Mask\\n(True = can attend, False = blocked)')\n",
    "ax.set_xlabel('Key Position (can see)')\n",
    "ax.set_ylabel('Query Position')\n",
    "\n",
    "for i in range(seq_len):\n",
    "    for j in range(seq_len):\n",
    "        symbol = \"Y\" if mask[i, j] else \"N\"\n",
    "        color = \"white\" if mask[i, j] else \"red\"\n",
    "        ax.text(j, i, symbol, ha=\"center\", va=\"center\", fontsize=14, color=color)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Position 0 can only see itself\")\n",
    "print(\"Position 3 can see positions 0, 1, 2, 3\")\n",
    "print(\"Position 5 can see all positions 0-5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply causal masking\n",
    "tokens = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "mask = create_causal_mask(len(tokens))\n",
    "\n",
    "output, weights_causal = sa.forward(X, mask=mask)\n",
    "\n",
    "plot_attention_heatmap(tokens, weights_causal, \"Causal Self-Attention (can't see future)\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNotice: The upper triangle is all zeros.\")\n",
    "print(\"Each position can only attend to itself and previous positions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare causal vs bidirectional\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bidirectional (no mask)\n",
    "_, weights_bi = sa.forward(X)\n",
    "im1 = axes[0].imshow(weights_bi, cmap='Blues', aspect='auto')\n",
    "axes[0].set_title('Bidirectional Attention\\n(like BERT)')\n",
    "axes[0].set_xticks(range(len(tokens)))\n",
    "axes[0].set_yticks(range(len(tokens)))\n",
    "axes[0].set_xticklabels(tokens)\n",
    "axes[0].set_yticklabels(tokens)\n",
    "axes[0].set_xlabel('Keys')\n",
    "axes[0].set_ylabel('Queries')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Causal (with mask)\n",
    "im2 = axes[1].imshow(weights_causal, cmap='Blues', aspect='auto')\n",
    "axes[1].set_title('Causal Attention\\n(like GPT)')\n",
    "axes[1].set_xticks(range(len(tokens)))\n",
    "axes[1].set_yticks(range(len(tokens)))\n",
    "axes[1].set_xticklabels(tokens)\n",
    "axes[1].set_yticklabels(tokens)\n",
    "axes[1].set_xlabel('Keys')\n",
    "axes[1].set_ylabel('Queries')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"BERT uses bidirectional attention (sees entire context)\")\n",
    "print(\"GPT uses causal attention (can only see past)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Attention vs RNN: The Bottleneck Problem\n",
    "\n",
    "RNNs compress all information into a fixed-size hidden state. Attention provides direct access to all positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_rnn_information_flow(seq_len, decay=0.7):\n",
    "    \"\"\"\n",
    "    Simulate how information from position i reaches position j in an RNN.\n",
    "    With each step, information decays (vanishing gradient).\n",
    "    \"\"\"\n",
    "    flow = np.zeros((seq_len, seq_len))\n",
    "    \n",
    "    for i in range(seq_len):\n",
    "        for j in range(i, seq_len):\n",
    "            # Information from i to j decays with distance\n",
    "            distance = j - i\n",
    "            flow[j, i] = decay ** distance\n",
    "    \n",
    "    return flow\n",
    "\n",
    "def simulate_attention_information_flow(seq_len):\n",
    "    \"\"\"\n",
    "    In attention, any position can directly access any other.\n",
    "    Distance doesn't matter.\n",
    "    \"\"\"\n",
    "    # Uniform attention as baseline (each position attends equally)\n",
    "    flow = np.ones((seq_len, seq_len)) / seq_len\n",
    "    return flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 10\n",
    "\n",
    "rnn_flow = simulate_rnn_information_flow(seq_len, decay=0.7)\n",
    "attn_flow = simulate_attention_information_flow(seq_len)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# RNN\n",
    "im1 = axes[0].imshow(rnn_flow, cmap='YlOrRd', aspect='auto')\n",
    "axes[0].set_title('RNN Information Flow\\n(exponential decay with distance)')\n",
    "axes[0].set_xlabel('Source Position')\n",
    "axes[0].set_ylabel('Target Position')\n",
    "plt.colorbar(im1, ax=axes[0], label='Information preserved')\n",
    "\n",
    "# Attention\n",
    "im2 = axes[1].imshow(attn_flow, cmap='YlOrRd', aspect='auto')\n",
    "axes[1].set_title('Attention Information Flow\\n(direct access, no decay)')\n",
    "axes[1].set_xlabel('Source Position')\n",
    "axes[1].set_ylabel('Target Position')\n",
    "plt.colorbar(im2, ax=axes[1], label='Information preserved')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information preservation at different distances\n",
    "distances = np.arange(0, 20)\n",
    "decay_rates = [0.9, 0.7, 0.5]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "for decay in decay_rates:\n",
    "    rnn_preservation = decay ** distances\n",
    "    plt.plot(distances, rnn_preservation, 'o-', label=f'RNN (decay={decay})', alpha=0.7)\n",
    "\n",
    "# Attention baseline (constant)\n",
    "plt.axhline(y=1.0, color='green', linestyle='--', linewidth=2, label='Attention (direct)')\n",
    "\n",
    "plt.xlabel('Distance between positions')\n",
    "plt.ylabel('Information Preserved')\n",
    "plt.title('RNN vs Attention: Long-Range Dependencies')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0, 1.1)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insight: RNN information decays exponentially with distance.\")\n",
    "print(\"Attention maintains constant O(1) path length between any two positions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Attention Patterns\n",
    "\n",
    "Different attention patterns emerge for different relationships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attention_pattern(pattern_type, seq_len):\n",
    "    \"\"\"Create example attention patterns.\"\"\"\n",
    "    \n",
    "    if pattern_type == \"diagonal\":\n",
    "        # Self-attention (attend to self)\n",
    "        weights = np.eye(seq_len)\n",
    "        \n",
    "    elif pattern_type == \"previous\":\n",
    "        # Attend to previous token\n",
    "        weights = np.zeros((seq_len, seq_len))\n",
    "        for i in range(seq_len):\n",
    "            weights[i, max(0, i-1)] = 1.0\n",
    "            \n",
    "    elif pattern_type == \"first\":\n",
    "        # Attend to first token (like [CLS])\n",
    "        weights = np.zeros((seq_len, seq_len))\n",
    "        weights[:, 0] = 1.0\n",
    "        \n",
    "    elif pattern_type == \"uniform\":\n",
    "        # Attend uniformly\n",
    "        weights = np.ones((seq_len, seq_len)) / seq_len\n",
    "        \n",
    "    elif pattern_type == \"local\":\n",
    "        # Attend to nearby tokens\n",
    "        weights = np.zeros((seq_len, seq_len))\n",
    "        for i in range(seq_len):\n",
    "            for j in range(max(0, i-2), min(seq_len, i+3)):\n",
    "                weights[i, j] = 1.0\n",
    "        weights = weights / weights.sum(axis=1, keepdims=True)\n",
    "        \n",
    "    return weights\n",
    "\n",
    "# Visualize different patterns\n",
    "patterns = [\"diagonal\", \"previous\", \"first\", \"uniform\", \"local\"]\n",
    "titles = [\n",
    "    \"Diagonal\\n(attend to self)\",\n",
    "    \"Previous\\n(attend to prior token)\",\n",
    "    \"First Token\\n(like [CLS] aggregation)\",\n",
    "    \"Uniform\\n(average all)\",\n",
    "    \"Local\\n(nearby window)\"\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(18, 3.5))\n",
    "\n",
    "for i, (pattern, title) in enumerate(zip(patterns, titles)):\n",
    "    weights = create_attention_pattern(pattern, 8)\n",
    "    im = axes[i].imshow(weights, cmap='Blues', aspect='auto', vmin=0, vmax=1)\n",
    "    axes[i].set_title(title, fontsize=10)\n",
    "    axes[i].set_xlabel('Key')\n",
    "    if i == 0:\n",
    "        axes[i].set_ylabel('Query')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThese patterns emerge naturally during training.\")\n",
    "print(\"Different attention heads often specialize in different patterns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Multi-Head Attention\n",
    "\n",
    "Multiple attention heads learn different patterns in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    \"\"\"Multi-head self-attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads):\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Combined projections for efficiency\n",
    "        self.W_Q = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "        self.W_K = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "        self.W_V = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "        self.W_O = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "    \n",
    "    def forward(self, X, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: Input (seq_len, d_model)\n",
    "            mask: Optional mask (seq_len, seq_len)\n",
    "        Returns:\n",
    "            output: (seq_len, d_model)\n",
    "            weights: (n_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        seq_len = X.shape[0]\n",
    "        \n",
    "        # Project and reshape for heads\n",
    "        Q = (X @ self.W_Q).reshape(seq_len, self.n_heads, self.d_k).transpose(1, 0, 2)\n",
    "        K = (X @ self.W_K).reshape(seq_len, self.n_heads, self.d_k).transpose(1, 0, 2)\n",
    "        V = (X @ self.W_V).reshape(seq_len, self.n_heads, self.d_k).transpose(1, 0, 2)\n",
    "        \n",
    "        # Attention for each head: (n_heads, seq_len, seq_len)\n",
    "        scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = np.where(mask[None, :, :], scores, -np.inf)\n",
    "        \n",
    "        weights = softmax(scores, axis=-1)\n",
    "        \n",
    "        # Weighted values: (n_heads, seq_len, d_k)\n",
    "        head_outputs = np.matmul(weights, V)\n",
    "        \n",
    "        # Concatenate and project: (seq_len, d_model)\n",
    "        concat = head_outputs.transpose(1, 0, 2).reshape(seq_len, self.d_model)\n",
    "        output = concat @ self.W_O\n",
    "        \n",
    "        return output, weights\n",
    "\n",
    "# Test multi-head attention\n",
    "d_model = 64\n",
    "n_heads = 8\n",
    "seq_len = 6\n",
    "\n",
    "mha = MultiHeadAttention(d_model, n_heads)\n",
    "X = np.random.randn(seq_len, d_model)\n",
    "\n",
    "output, all_weights = mha.forward(X)\n",
    "\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {all_weights.shape}\")\n",
    "print(f\"  ({n_heads} heads, each with {seq_len}x{seq_len} attention matrix)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention patterns across heads\n",
    "tokens = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for head_idx in range(min(8, n_heads)):\n",
    "    im = axes[head_idx].imshow(all_weights[head_idx], cmap='Blues', aspect='auto')\n",
    "    axes[head_idx].set_title(f'Head {head_idx}', fontsize=10)\n",
    "    axes[head_idx].set_xticks(range(len(tokens)))\n",
    "    axes[head_idx].set_yticks(range(len(tokens)))\n",
    "    axes[head_idx].set_xticklabels(tokens, fontsize=8, rotation=45)\n",
    "    axes[head_idx].set_yticklabels(tokens, fontsize=8)\n",
    "\n",
    "plt.suptitle('Multi-Head Attention: Different Heads Learn Different Patterns', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEach head can specialize in different relationships:\")\n",
    "print(\"- Syntactic patterns (subject-verb)\")\n",
    "print(\"- Positional patterns (previous token, next token)\")\n",
    "print(\"- Semantic patterns (related concepts)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Positional Encoding\n",
    "\n",
    "Attention is **permutation equivariant** — it doesn't know token order. We add positional information explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_position_encoding(seq_len, d_model):\n",
    "    \"\"\"\n",
    "    Sinusoidal positional encoding from \"Attention Is All You Need\".\n",
    "    \n",
    "    PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
    "    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "    \"\"\"\n",
    "    position = np.arange(seq_len)[:, np.newaxis]\n",
    "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "    \n",
    "    pe = np.zeros((seq_len, d_model))\n",
    "    pe[:, 0::2] = np.sin(position * div_term)\n",
    "    pe[:, 1::2] = np.cos(position * div_term)\n",
    "    \n",
    "    return pe\n",
    "\n",
    "# Visualize positional encodings\n",
    "seq_len = 50\n",
    "d_model = 64\n",
    "\n",
    "pe = sinusoidal_position_encoding(seq_len, d_model)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(pe, cmap='RdBu', aspect='auto')\n",
    "plt.xlabel('Dimension')\n",
    "plt.ylabel('Position')\n",
    "plt.title('Sinusoidal Positional Encoding')\n",
    "plt.colorbar(label='Value')\n",
    "plt.show()\n",
    "\n",
    "print(\"Each position has a unique encoding.\")\n",
    "print(\"Low dimensions change quickly (high frequency).\")\n",
    "print(\"High dimensions change slowly (low frequency).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show that nearby positions have similar encodings\n",
    "def positional_similarity(pe):\n",
    "    \"\"\"Compute cosine similarity between position encodings.\"\"\"\n",
    "    norm = np.linalg.norm(pe, axis=1, keepdims=True)\n",
    "    pe_normalized = pe / norm\n",
    "    return pe_normalized @ pe_normalized.T\n",
    "\n",
    "pe = sinusoidal_position_encoding(50, 64)\n",
    "sim = positional_similarity(pe)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(sim, cmap='RdYlGn', aspect='auto')\n",
    "plt.xlabel('Position j')\n",
    "plt.ylabel('Position i')\n",
    "plt.title('Positional Encoding Similarity\\n(nearby positions are more similar)')\n",
    "plt.colorbar(label='Cosine Similarity')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey property: Similarity decreases smoothly with distance.\")\n",
    "print(\"This gives the model information about relative positions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Computational Complexity\n",
    "\n",
    "Attention is O(n^2) in sequence length — both a strength and a limitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_attention(seq_lengths, d_model=64):\n",
    "    \"\"\"Measure attention computation time vs sequence length.\"\"\"\n",
    "    times = []\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        X = np.random.randn(seq_len, d_model)\n",
    "        sa = SelfAttention(d_model, d_k=32)\n",
    "        \n",
    "        # Warm up\n",
    "        _ = sa.forward(X)\n",
    "        \n",
    "        # Time multiple runs\n",
    "        n_runs = 10\n",
    "        start = time.time()\n",
    "        for _ in range(n_runs):\n",
    "            _ = sa.forward(X)\n",
    "        elapsed = (time.time() - start) / n_runs\n",
    "        times.append(elapsed)\n",
    "    \n",
    "    return times\n",
    "\n",
    "seq_lengths = [16, 32, 64, 128, 256, 512]\n",
    "times = benchmark_attention(seq_lengths)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(seq_lengths, times, 'bo-', markersize=8)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.title('Attention Computation Time')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.loglog(seq_lengths, times, 'bo-', markersize=8, label='Measured')\n",
    "# Theoretical O(n^2) line\n",
    "theoretical = [(s/seq_lengths[0])**2 * times[0] for s in seq_lengths]\n",
    "plt.loglog(seq_lengths, theoretical, 'r--', label='O(n^2) theoretical')\n",
    "plt.xlabel('Sequence Length (log)')\n",
    "plt.ylabel('Time (log)')\n",
    "plt.title('Log-Log Scale (slope ~ 2)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nImplication: Doubling sequence length quadruples compute time.\")\n",
    "print(\"This is why long-context models use efficient attention variants.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Concept | Formula | Purpose |\n",
    "|---------|---------|---------||\n",
    "| Attention | softmax(QK^T/sqrt(d_k))V | Content-based routing |\n",
    "| Scaling | / sqrt(d_k) | Stabilize softmax gradients |\n",
    "| Causal Mask | Lower triangular | Prevent seeing future |\n",
    "| Multi-Head | Parallel attention | Learn different patterns |\n",
    "| Position Encoding | X + PE | Inject order information |\n",
    "\n",
    "**Key Insights:**\n",
    "1. Attention provides **O(1) path length** between any positions (vs O(n) for RNN)\n",
    "2. Self-attention is **permutation equivariant** — needs positional encoding\n",
    "3. **Multi-head attention** learns diverse patterns in parallel\n",
    "4. **Causal masking** enables autoregressive generation\n",
    "5. **O(n^2) complexity** limits sequence length; efficient variants exist\n",
    "\n",
    "**Next:** [08-minimal-transformer.ipynb](08-minimal-transformer.ipynb) builds a complete transformer block with attention, feedforward, and residual connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Temperature in Attention:** Modify the attention function to accept a temperature parameter T. What happens with T < 1 (sharper)? T > 1 (softer)?\n",
    "\n",
    "2. **Relative Position:** Implement relative positional encoding where attention scores include a bias based on position difference (i - j).\n",
    "\n",
    "3. **Sparse Attention:** Implement a local attention pattern where each position only attends to a window of k neighbors. How does this reduce complexity?\n",
    "\n",
    "4. **Cross-Attention:** Modify the code to support cross-attention where Q comes from one sequence and K, V from another (like encoder-decoder attention)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
