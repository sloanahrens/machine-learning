{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal GPT from Scratch\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Build a decoder-only (autoregressive) transformer\n",
    "2. Implement causal masking for left-to-right generation\n",
    "3. Train on character-level Shakespeare text\n",
    "4. Generate text samples during training\n",
    "5. Track loss and perplexity curves\n",
    "\n",
    "**Prerequisites:** [gpt](../transformers/gpt.md), [08-minimal-transformer](08-minimal-transformer.ipynb)\n",
    "\n",
    "**Key Insight:** GPT is just a transformer decoder with causal masking. The model predicts the next token given all previous tokens.\n",
    "\n",
    "**Reference:** Inspired by Karpathy's nanoGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Shakespeare Dataset\n",
    "\n",
    "We'll use a small Shakespeare corpus for character-level language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Shakespeare text\n",
    "data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "data_path = 'shakespeare.txt'\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    print(\"Downloading Shakespeare text...\")\n",
    "    urllib.request.urlretrieve(data_url, data_path)\n",
    "\n",
    "with open(data_path, 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Total characters: {len(text):,}\")\n",
    "print(f\"\\nFirst 500 characters:\")\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create character-level vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Characters: {''.join(chars)}\")\n",
    "\n",
    "# Character to index mapping\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Encode/decode functions\n",
    "def encode(s):\n",
    "    return [char_to_idx[c] for c in s]\n",
    "\n",
    "def decode(indices):\n",
    "    return ''.join([idx_to_char[i] for i in indices])\n",
    "\n",
    "# Test encoding\n",
    "test_str = \"Hello\"\n",
    "encoded = encode(test_str)\n",
    "decoded = decode(encoded)\n",
    "print(f\"\\n'{test_str}' -> {encoded} -> '{decoded}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode entire dataset\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "\n",
    "# Train/val split\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(f\"Train: {len(train_data):,} tokens\")\n",
    "print(f\"Val: {len(val_data):,} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset and Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \"\"\"Character-level text dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, data, block_size):\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get a chunk of text\n",
    "        x = self.data[idx:idx + self.block_size]\n",
    "        y = self.data[idx + 1:idx + self.block_size + 1]\n",
    "        return x, y\n",
    "\n",
    "# Hyperparameters\n",
    "block_size = 64  # Context length\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = TextDataset(train_data, block_size)\n",
    "val_dataset = TextDataset(val_data, block_size)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Show example batch\n",
    "x, y = train_dataset[0]\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nInput: {decode(x.tolist())}\")\n",
    "print(f\"\\nTarget (shifted by 1): {decode(y.tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GPT Model Architecture\n",
    "\n",
    "GPT is a **decoder-only** transformer:\n",
    "- Uses **causal masking** (each position can only attend to earlier positions)\n",
    "- **No encoder** - just stacked decoder blocks\n",
    "- Predicts next token given all previous tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"Causal (masked) self-attention for autoregressive modeling.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Combined QKV projection for efficiency\n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model)\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.proj_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Causal mask: lower triangular\n",
    "        # Position i can only attend to positions <= i\n",
    "        mask = torch.tril(torch.ones(block_size, block_size))\n",
    "        self.register_buffer('mask', mask.view(1, 1, block_size, block_size))\n",
    "        \n",
    "        self.scale = 1.0 / np.sqrt(self.d_k)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape  # batch, seq_len, d_model\n",
    "        \n",
    "        # Compute Q, K, V\n",
    "        qkv = self.qkv(x)  # (B, T, 3*d_model)\n",
    "        q, k, v = qkv.split(self.d_model, dim=2)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(B, T, self.n_heads, self.d_k).transpose(1, 2)  # (B, n_heads, T, d_k)\n",
    "        k = k.view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Attention scores\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale  # (B, n_heads, T, T)\n",
    "        \n",
    "        # Apply causal mask\n",
    "        attn = attn.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "        \n",
    "        # Weighted sum of values\n",
    "        out = attn @ v  # (B, n_heads, T, d_k)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)  # (B, T, d_model)\n",
    "        \n",
    "        return self.proj_dropout(self.proj(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise feedforward network.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"GPT-style transformer block (pre-norm).\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = CausalSelfAttention(d_model, n_heads, block_size, dropout)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = FeedForward(d_model, dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\"GPT Language Model.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        \n",
    "        # Token and position embeddings\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(block_size, d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, block_size, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        \n",
    "        # Weight tying: share weights between embedding and output\n",
    "        self.tok_emb.weight = self.head.weight\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx: (B, T) token indices\n",
    "            targets: (B, T) target token indices (optional)\n",
    "        Returns:\n",
    "            logits: (B, T, vocab_size)\n",
    "            loss: scalar if targets provided\n",
    "        \"\"\"\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.block_size, f\"Sequence length {T} exceeds block size {self.block_size}\"\n",
    "        \n",
    "        # Get embeddings\n",
    "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)  # (1, T)\n",
    "        tok_emb = self.tok_emb(idx)  # (B, T, d_model)\n",
    "        pos_emb = self.pos_emb(pos)  # (1, T, d_model)\n",
    "        x = self.drop(tok_emb + pos_emb)\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Output projection\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)  # (B, T, vocab_size)\n",
    "        \n",
    "        # Compute loss if targets provided\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Generate new tokens autoregressively.\n",
    "        \n",
    "        Args:\n",
    "            idx: (B, T) starting token indices\n",
    "            max_new_tokens: number of tokens to generate\n",
    "            temperature: sampling temperature (higher = more random)\n",
    "            top_k: if set, only sample from top k most likely tokens\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop to block_size if needed\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            \n",
    "            # Get predictions\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature  # (B, vocab_size)\n",
    "            \n",
    "            # Optional top-k filtering\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, top_k)\n",
    "                logits[logits < v[:, [-1]]] = float('-inf')\n",
    "            \n",
    "            # Sample from distribution\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Append to sequence\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "        \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model_config = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'd_model': 128,\n",
    "    'n_heads': 4,\n",
    "    'n_layers': 4,\n",
    "    'block_size': block_size,\n",
    "    'dropout': 0.1,\n",
    "}\n",
    "\n",
    "model = GPT(**model_config).to(device)\n",
    "\n",
    "# Count parameters\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {n_params:,}\")\n",
    "print(f\"\\nConfig: {model_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, loader, device, max_batches=None):\n",
    "    \"\"\"Compute average loss on a dataset.\"\"\"\n",
    "    model.train(False)  # Set to inference mode\n",
    "    total_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(loader):\n",
    "            if max_batches and i >= max_batches:\n",
    "                break\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            _, loss = model(x, y)\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "    \n",
    "    return total_loss / n_batches\n",
    "\n",
    "def generate_sample(model, start_text, max_tokens=100, temperature=0.8):\n",
    "    \"\"\"Generate a text sample.\"\"\"\n",
    "    model.train(False)  # Set to inference mode\n",
    "    \n",
    "    idx = torch.tensor([encode(start_text)], device=device)\n",
    "    generated = model.generate(idx, max_new_tokens=max_tokens, temperature=temperature)\n",
    "    return decode(generated[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "learning_rate = 3e-4\n",
    "n_epochs = 10\n",
    "log_interval = 100\n",
    "sample_interval = 500\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training history\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "steps = []\n",
    "\n",
    "# Initial loss\n",
    "print(f\"Initial val loss: {compute_loss(model, val_loader, device, max_batches=50):.4f}\")\n",
    "print(f\"Random baseline: {np.log(vocab_size):.4f} (= ln({vocab_size}))\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "step = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, loss = model(x, y)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        # Log progress\n",
    "        if step % log_interval == 0:\n",
    "            val_loss = compute_loss(model, val_loader, device, max_batches=20)\n",
    "            train_losses.append(loss.item())\n",
    "            val_losses.append(val_loss)\n",
    "            steps.append(step)\n",
    "            \n",
    "            perplexity = np.exp(val_loss)\n",
    "            print(f\"Epoch {epoch+1} | Step {step:5d} | Train Loss: {loss.item():.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} | Perplexity: {perplexity:.2f}\")\n",
    "            \n",
    "            model.train()  # Back to training mode\n",
    "        \n",
    "        # Generate sample\n",
    "        if step % sample_interval == 0:\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"Generated sample:\")\n",
    "            sample = generate_sample(model, \"\\n\", max_tokens=150, temperature=0.8)\n",
    "            print(sample)\n",
    "            print(\"=\"*50 + \"\\n\")\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss curve\n",
    "axes[0].plot(steps, train_losses, label='Train', alpha=0.7)\n",
    "axes[0].plot(steps, val_losses, label='Validation')\n",
    "axes[0].axhline(y=np.log(vocab_size), color='gray', linestyle='--', label='Random baseline')\n",
    "axes[0].set_xlabel('Step')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Perplexity curve\n",
    "perplexities = [np.exp(l) for l in val_losses]\n",
    "axes[1].plot(steps, perplexities)\n",
    "axes[1].axhline(y=vocab_size, color='gray', linestyle='--', label='Random baseline')\n",
    "axes[1].set_xlabel('Step')\n",
    "axes[1].set_ylabel('Perplexity')\n",
    "axes[1].set_title('Validation Perplexity')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal validation loss: {val_losses[-1]:.4f}\")\n",
    "print(f\"Final perplexity: {perplexities[-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate with different temperatures\n",
    "print(\"=\"*60)\n",
    "print(\"GENERATION WITH DIFFERENT TEMPERATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "prompt = \"\\nTo be, or not to be\"\n",
    "\n",
    "for temp in [0.5, 0.8, 1.0, 1.5]:\n",
    "    print(f\"\\n--- Temperature {temp} ---\")\n",
    "    sample = generate_sample(model, prompt, max_tokens=100, temperature=temp)\n",
    "    print(sample)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Lower temperature = more deterministic, repetitive\")\n",
    "print(\"Higher temperature = more random, creative\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate longer samples\n",
    "print(\"=\"*60)\n",
    "print(\"LONGER GENERATION (300 characters)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "prompts = [\n",
    "    \"\\nROMEO:\\n\",\n",
    "    \"\\nThe king doth keep\",\n",
    "    \"\\nWhat light through yonder\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\n--- Prompt: {repr(prompt)} ---\")\n",
    "    sample = generate_sample(model, prompt, max_tokens=300, temperature=0.8)\n",
    "    print(sample)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Understanding Perplexity\n",
    "\n",
    "Perplexity measures how \"surprised\" the model is by the data:\n",
    "\n",
    "$$\\text{Perplexity} = e^{\\text{loss}} = e^{-\\frac{1}{N}\\sum_{i=1}^N \\log p(x_i)}$$\n",
    "\n",
    "- **Lower is better**\n",
    "- Perplexity of $k$ means the model is as uncertain as if choosing uniformly among $k$ options\n",
    "- Random baseline: perplexity = vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-character probabilities\n",
    "model.train(False)  # Set to inference mode\n",
    "\n",
    "test_text = \"\\nTo be, or not to be, that is the question:\\n\"\n",
    "test_tokens = torch.tensor([encode(test_text)], device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits, _ = model(test_tokens)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "# Get probability of each actual next character\n",
    "print(\"Character-by-character probabilities:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i in range(min(30, len(test_text) - 1)):\n",
    "    char = test_text[i]\n",
    "    next_char = test_text[i + 1]\n",
    "    next_idx = encode(next_char)[0]\n",
    "    prob = probs[0, i, next_idx].item()\n",
    "    \n",
    "    # Get top prediction\n",
    "    top_idx = probs[0, i].argmax().item()\n",
    "    top_char = idx_to_char[top_idx]\n",
    "    \n",
    "    bar = \"#\" * int(prob * 40)\n",
    "    print(f\"'{char}' -> '{next_char}' (p={prob:.3f}) {bar}\")\n",
    "    if top_char != next_char:\n",
    "        print(f\"      (top prediction: '{top_char}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizing Attention Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified attention layer to capture weights\n",
    "def get_attention_weights(model, text):\n",
    "    \"\"\"Get attention weights for a piece of text.\"\"\"\n",
    "    model.train(False)  # Set to inference mode\n",
    "    \n",
    "    tokens = torch.tensor([encode(text)], device=device)\n",
    "    \n",
    "    # Hook to capture attention weights\n",
    "    attention_weights = []\n",
    "    \n",
    "    def hook(module, input, output):\n",
    "        # Get attention scores before softmax\n",
    "        B, T, C = input[0].shape\n",
    "        qkv = module.qkv(input[0])\n",
    "        q, k, v = qkv.split(module.d_model, dim=2)\n",
    "        \n",
    "        q = q.view(B, T, module.n_heads, module.d_k).transpose(1, 2)\n",
    "        k = k.view(B, T, module.n_heads, module.d_k).transpose(1, 2)\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * module.scale\n",
    "        attn = attn.masked_fill(module.mask[:, :, :T, :T] == 0, float('-inf'))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        \n",
    "        attention_weights.append(attn.detach().cpu())\n",
    "    \n",
    "    # Register hooks\n",
    "    hooks = []\n",
    "    for block in model.blocks:\n",
    "        h = block.attn.register_forward_hook(hook)\n",
    "        hooks.append(h)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        _ = model(tokens)\n",
    "    \n",
    "    # Remove hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "    \n",
    "    return attention_weights, text\n",
    "\n",
    "# Get attention weights\n",
    "test_text = \"To be or not\"\n",
    "attn_weights, text = get_attention_weights(model, test_text)\n",
    "\n",
    "print(f\"Text: '{text}'\")\n",
    "print(f\"Number of layers: {len(attn_weights)}\")\n",
    "print(f\"Attention shape per layer: {attn_weights[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention patterns\n",
    "def plot_attention(attn_weights, text, layer=0, head=0):\n",
    "    \"\"\"Plot attention pattern for a specific layer and head.\"\"\"\n",
    "    attn = attn_weights[layer][0, head].numpy()  # (T, T)\n",
    "    chars = list(text)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    im = ax.imshow(attn, cmap='Blues')\n",
    "    \n",
    "    ax.set_xticks(range(len(chars)))\n",
    "    ax.set_yticks(range(len(chars)))\n",
    "    ax.set_xticklabels(chars)\n",
    "    ax.set_yticklabels(chars)\n",
    "    \n",
    "    ax.set_xlabel('Key (attending to)')\n",
    "    ax.set_ylabel('Query (from)')\n",
    "    ax.set_title(f'Attention - Layer {layer}, Head {head}')\n",
    "    \n",
    "    plt.colorbar(im, ax=ax)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Plot different layers and heads\n",
    "fig = plot_attention(attn_weights, test_text, layer=0, head=0)\n",
    "plt.show()\n",
    "\n",
    "fig = plot_attention(attn_weights, test_text, layer=3, head=0)\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: The causal mask makes the upper triangle all zeros.\")\n",
    "print(\"Each position can only attend to itself and earlier positions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all heads in a layer\n",
    "layer = 0\n",
    "n_heads = model_config['n_heads']\n",
    "\n",
    "fig, axes = plt.subplots(1, n_heads, figsize=(4 * n_heads, 4))\n",
    "\n",
    "for head in range(n_heads):\n",
    "    attn = attn_weights[layer][0, head].numpy()\n",
    "    im = axes[head].imshow(attn, cmap='Blues')\n",
    "    axes[head].set_title(f'Head {head}')\n",
    "    axes[head].set_xticks(range(len(test_text)))\n",
    "    axes[head].set_yticks(range(len(test_text)))\n",
    "    axes[head].set_xticklabels(list(test_text), fontsize=8)\n",
    "    axes[head].set_yticklabels(list(test_text), fontsize=8)\n",
    "\n",
    "plt.suptitle(f'All Attention Heads - Layer {layer}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter breakdown\n",
    "def count_params(model):\n",
    "    \"\"\"Count parameters by component.\"\"\"\n",
    "    counts = {\n",
    "        'embeddings': model.tok_emb.weight.numel() + model.pos_emb.weight.numel(),\n",
    "        'attention': sum(sum(p.numel() for p in block.attn.parameters()) for block in model.blocks),\n",
    "        'feedforward': sum(sum(p.numel() for p in block.ffn.parameters()) for block in model.blocks),\n",
    "        'layer_norms': sum(\n",
    "            sum(p.numel() for p in block.ln1.parameters()) +\n",
    "            sum(p.numel() for p in block.ln2.parameters())\n",
    "            for block in model.blocks\n",
    "        ) + sum(p.numel() for p in model.ln_f.parameters()),\n",
    "    }\n",
    "    # Note: output head shares weights with embedding\n",
    "    return counts\n",
    "\n",
    "counts = count_params(model)\n",
    "total = sum(counts.values())\n",
    "\n",
    "print(\"Parameter Breakdown:\")\n",
    "print(\"-\" * 40)\n",
    "for name, count in counts.items():\n",
    "    pct = count / total * 100\n",
    "    print(f\"{name:20s} {count:10,d} ({pct:5.1f}%)\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Total':20s} {total:10,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to theoretical FLOPS\n",
    "d = model_config['d_model']\n",
    "T = model_config['block_size']\n",
    "L = model_config['n_layers']\n",
    "V = model_config['vocab_size']\n",
    "\n",
    "print(\"Approximate FLOPS per forward pass:\")\n",
    "print(f\"  Embedding lookup: O(T * d) = {T * d:,}\")\n",
    "print(f\"  Attention (per layer): O(T^2 * d) = {T * T * d:,}\")\n",
    "print(f\"  FFN (per layer): O(T * d^2) = {T * d * d * 4:,}\")\n",
    "print(f\"  Output projection: O(T * V * d) = {T * V * d:,}\")\n",
    "print()\n",
    "print(f\"Total per layer: ~{T * T * d + T * d * d * 4:,}\")\n",
    "print(f\"Total for {L} layers: ~{L * (T * T * d + T * d * d * 4):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Component | Purpose |\n",
    "|-----------|--------|\n",
    "| Token Embedding | Convert tokens to vectors |\n",
    "| Position Embedding | Inject position information |\n",
    "| Causal Attention | Attend only to past tokens |\n",
    "| Weight Tying | Share embedding/output weights |\n",
    "\n",
    "**Key Insights:**\n",
    "1. GPT is just a transformer decoder with **causal masking**\n",
    "2. **Weight tying** between embedding and output reduces parameters\n",
    "3. **Temperature** controls generation randomness\n",
    "4. **Perplexity** measures model uncertainty (lower is better)\n",
    "5. Even tiny models can learn character patterns from Shakespeare\n",
    "\n",
    "**Scaling:** Real GPT models use:\n",
    "- Billions of parameters (we have ~500K)\n",
    "- Much longer context (8K-128K vs our 64)\n",
    "- Trained on trillions of tokens (we have ~1M)\n",
    "\n",
    "**Next:** [10-fine-tuning-basics.ipynb](10-fine-tuning-basics.ipynb) explores fine-tuning pretrained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Larger Model:** Increase d_model to 256 and n_layers to 6. How does this affect training speed and quality?\n",
    "\n",
    "2. **Top-k Sampling:** Implement top-k sampling in the generate method. Compare outputs with k=5, 10, 50.\n",
    "\n",
    "3. **Learning Rate Schedule:** Implement cosine learning rate decay. Does it improve final perplexity?\n",
    "\n",
    "4. **Different Corpus:** Train on a different text (song lyrics, code, poetry). How does the output style change?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
