{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12: KV Cache Demo\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Understand why naive autoregressive generation is slow\n",
    "2. Implement KV (Key-Value) caching from scratch\n",
    "3. Benchmark the speedup from caching\n",
    "4. Visualize what gets cached and why\n",
    "5. Explore memory vs speed tradeoffs\n",
    "\n",
    "**Prerequisites:** [attention](../transformers/attention.md), [GPT](../transformers/gpt.md), [inference](../modern-llms/inference.md)\n",
    "\n",
    "**Framework:** PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Problem - Redundant Computation\n",
    "\n",
    "In autoregressive generation, each new token requires attending to ALL previous tokens. Without caching, we recompute K and V for every previous token at every step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_redundant_computation():\n",
    "    \"\"\"Visualize the redundant work in naive generation.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Left: Naive approach\n",
    "    ax = axes[0]\n",
    "    seq_len = 6\n",
    "    \n",
    "    # Create a matrix showing what's computed at each step\n",
    "    computation = np.zeros((seq_len, seq_len))\n",
    "    for step in range(seq_len):\n",
    "        for j in range(step + 1):\n",
    "            computation[step, j] = step + 1  # Color by step\n",
    "    \n",
    "    im = ax.imshow(computation, cmap='Blues', aspect='auto')\n",
    "    ax.set_xlabel('Token Position')\n",
    "    ax.set_ylabel('Generation Step')\n",
    "    ax.set_title('Naive: Recompute K,V for ALL tokens each step\\n(Darker = computed at that step)')\n",
    "    ax.set_xticks(range(seq_len))\n",
    "    ax.set_yticks(range(seq_len))\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            if computation[i, j] > 0:\n",
    "                ax.text(j, i, 'K,V', ha='center', va='center', fontsize=8, color='white' if computation[i,j] > 3 else 'black')\n",
    "    \n",
    "    # Right: With KV cache\n",
    "    ax = axes[1]\n",
    "    \n",
    "    # Only new tokens are computed\n",
    "    cached = np.zeros((seq_len, seq_len))\n",
    "    for step in range(seq_len):\n",
    "        cached[step, step] = step + 1  # Only diagonal\n",
    "        # Show cached values as lighter\n",
    "        for j in range(step):\n",
    "            cached[step, j] = 0.3  # Cached\n",
    "    \n",
    "    im = ax.imshow(cached, cmap='Greens', aspect='auto', vmin=0, vmax=seq_len)\n",
    "    ax.set_xlabel('Token Position')\n",
    "    ax.set_ylabel('Generation Step')\n",
    "    ax.set_title('KV Cache: Only compute NEW token, reuse cached\\n(Dark = new, Light = cached)')\n",
    "    ax.set_xticks(range(seq_len))\n",
    "    ax.set_yticks(range(seq_len))\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            if j == i:\n",
    "                ax.text(j, i, 'NEW', ha='center', va='center', fontsize=8, color='white')\n",
    "            elif j < i:\n",
    "                ax.text(j, i, 'cache', ha='center', va='center', fontsize=7, color='darkgreen')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('kv_cache_concept.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Compute complexity comparison\n",
    "    n = 1000  # sequence length\n",
    "    d = 768   # hidden dimension\n",
    "    \n",
    "    naive_ops = sum((i+1) for i in range(n))  # 1 + 2 + 3 + ... + n = n(n+1)/2\n",
    "    cached_ops = n  # 1 + 1 + 1 + ... = n\n",
    "    \n",
    "    print(f\"\\nComplexity comparison (n={n} tokens):\")\n",
    "    print(f\"  Naive: O(n^2) = {naive_ops:,} K,V computations\")\n",
    "    print(f\"  KV Cache: O(n) = {cached_ops:,} K,V computations\")\n",
    "    print(f\"  Speedup: {naive_ops / cached_ops:.1f}x fewer computations\")\n",
    "\n",
    "visualize_redundant_computation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implementing a GPT with KV Cache\n",
    "\n",
    "We'll build a small GPT model with both naive and cached generation modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size: int = 1000\n",
    "    d_model: int = 256\n",
    "    n_heads: int = 4\n",
    "    n_layers: int = 4\n",
    "    max_seq_len: int = 512\n",
    "    dropout: float = 0.1\n",
    "\n",
    "\n",
    "class KVCache:\n",
    "    \"\"\"Key-Value cache for efficient autoregressive generation.\"\"\"\n",
    "    \n",
    "    def __init__(self, batch_size: int, max_seq_len: int, n_layers: int, \n",
    "                 n_heads: int, head_dim: int, device: torch.device):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.n_layers = n_layers\n",
    "        self.current_len = 0\n",
    "        \n",
    "        # Pre-allocate cache tensors for all layers\n",
    "        # Shape: [batch, n_heads, max_seq_len, head_dim]\n",
    "        self.cache_k = torch.zeros(\n",
    "            n_layers, batch_size, n_heads, max_seq_len, head_dim,\n",
    "            device=device\n",
    "        )\n",
    "        self.cache_v = torch.zeros(\n",
    "            n_layers, batch_size, n_heads, max_seq_len, head_dim,\n",
    "            device=device\n",
    "        )\n",
    "    \n",
    "    def update(self, layer_idx: int, k: torch.Tensor, v: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Update cache with new K, V and return full cached K, V.\n",
    "        \n",
    "        Args:\n",
    "            k, v: [batch, n_heads, seq_len, head_dim] - new K, V to add\n",
    "        Returns:\n",
    "            Full K, V including new values: [batch, n_heads, total_len, head_dim]\n",
    "        \"\"\"\n",
    "        seq_len = k.shape[2]\n",
    "        start = self.current_len\n",
    "        end = start + seq_len\n",
    "        \n",
    "        # Store new K, V in cache\n",
    "        self.cache_k[layer_idx, :, :, start:end, :] = k\n",
    "        self.cache_v[layer_idx, :, :, start:end, :] = v\n",
    "        \n",
    "        # Return full cached K, V up to current position\n",
    "        return self.cache_k[layer_idx, :, :, :end, :], self.cache_v[layer_idx, :, :, :end, :]\n",
    "    \n",
    "    def increment(self, n: int = 1):\n",
    "        \"\"\"Increment the sequence length counter.\"\"\"\n",
    "        self.current_len += n\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the cache for a new sequence.\"\"\"\n",
    "        self.current_len = 0\n",
    "        self.cache_k.zero_()\n",
    "        self.cache_v.zero_()\n",
    "    \n",
    "    def memory_usage(self) -> float:\n",
    "        \"\"\"Return memory usage in MB.\"\"\"\n",
    "        k_mem = self.cache_k.numel() * self.cache_k.element_size()\n",
    "        v_mem = self.cache_v.numel() * self.cache_v.element_size()\n",
    "        return (k_mem + v_mem) / (1024 ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention with optional KV caching.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        assert config.d_model % config.n_heads == 0\n",
    "        \n",
    "        self.n_heads = config.n_heads\n",
    "        self.head_dim = config.d_model // config.n_heads\n",
    "        self.d_model = config.d_model\n",
    "        \n",
    "        # Combined Q, K, V projection\n",
    "        self.qkv = nn.Linear(config.d_model, 3 * config.d_model, bias=False)\n",
    "        self.proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # Causal mask\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.tril(torch.ones(config.max_seq_len, config.max_seq_len))\n",
    "            .view(1, 1, config.max_seq_len, config.max_seq_len)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, cache: Optional[KVCache] = None, \n",
    "                layer_idx: int = 0) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass with optional KV caching.\n",
    "        \n",
    "        Args:\n",
    "            x: [batch, seq_len, d_model]\n",
    "            cache: Optional KV cache for generation\n",
    "            layer_idx: Layer index for cache lookup\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # Compute Q, K, V\n",
    "        qkv = self.qkv(x)  # [B, T, 3*C]\n",
    "        q, k, v = qkv.split(self.d_model, dim=-1)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)  # [B, nh, T, hd]\n",
    "        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Use KV cache if provided\n",
    "        if cache is not None:\n",
    "            k, v = cache.update(layer_idx, k, v)\n",
    "            # k, v now have shape [B, nh, cache_len + T, hd]\n",
    "        \n",
    "        # Compute attention\n",
    "        # Q has T positions, K has (cache_len + T) positions\n",
    "        scores = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        # Apply causal mask (only for new tokens attending to past)\n",
    "        kv_len = k.shape[2]  # Total length including cache\n",
    "        if cache is not None:\n",
    "            # During generation, Q only has 1 token but can attend to all cached K\n",
    "            # The causal constraint is automatically satisfied since we only \n",
    "            # generate one token at a time\n",
    "            pass  # No explicit masking needed for single-token generation\n",
    "        else:\n",
    "            # During training/prefill, apply causal mask\n",
    "            scores = scores.masked_fill(\n",
    "                self.mask[:, :, :T, :T] == 0,\n",
    "                float('-inf')\n",
    "            )\n",
    "        \n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = weights @ v  # [B, nh, T, hd]\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        return self.proj(out)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config.d_model, 4 * config.d_model)\n",
    "        self.fc2 = nn.Linear(4 * config.d_model, config.d_model)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.fc2(F.gelu(self.fc1(x))))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.d_model)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln2 = nn.LayerNorm(config.d_model)\n",
    "        self.ffn = FeedForward(config)\n",
    "    \n",
    "    def forward(self, x, cache=None, layer_idx=0):\n",
    "        x = x + self.attn(self.ln1(x), cache, layer_idx)\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\"GPT model with support for KV caching.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.tok_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.pos_emb = nn.Embedding(config.max_seq_len, config.d_model)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(config) for _ in range(config.n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.ln_f = nn.LayerNorm(config.d_model)\n",
    "        self.head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        \n",
    "        # Weight tying\n",
    "        self.tok_emb.weight = self.head.weight\n",
    "    \n",
    "    def forward(self, idx: torch.Tensor, cache: Optional[KVCache] = None,\n",
    "                start_pos: int = 0) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            idx: [batch, seq_len] token indices\n",
    "            cache: Optional KV cache\n",
    "            start_pos: Starting position for positional embeddings (for cached generation)\n",
    "        \"\"\"\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        # Embeddings\n",
    "        pos = torch.arange(start_pos, start_pos + T, device=idx.device)\n",
    "        x = self.dropout(self.tok_emb(idx) + self.pos_emb(pos))\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for layer_idx, block in enumerate(self.blocks):\n",
    "            x = block(x, cache, layer_idx)\n",
    "        \n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate_naive(self, idx: torch.Tensor, max_new_tokens: int,\n",
    "                       temperature: float = 1.0) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Naive generation: recompute everything at each step.\n",
    "        \"\"\"\n",
    "        self.train(False)\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            # Truncate if needed\n",
    "            idx_cond = idx[:, -self.config.max_seq_len:]\n",
    "            \n",
    "            # Forward pass over ENTIRE sequence\n",
    "            logits = self(idx_cond)\n",
    "            \n",
    "            # Sample next token\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            idx = torch.cat([idx, next_token], dim=1)\n",
    "        \n",
    "        return idx\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate_cached(self, idx: torch.Tensor, max_new_tokens: int,\n",
    "                        temperature: float = 1.0) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Cached generation: only compute new token at each step.\n",
    "        \"\"\"\n",
    "        self.train(False)\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        # Initialize KV cache\n",
    "        cache = KVCache(\n",
    "            batch_size=B,\n",
    "            max_seq_len=self.config.max_seq_len,\n",
    "            n_layers=self.config.n_layers,\n",
    "            n_heads=self.config.n_heads,\n",
    "            head_dim=self.config.d_model // self.config.n_heads,\n",
    "            device=idx.device\n",
    "        )\n",
    "        \n",
    "        # Prefill: process entire prompt\n",
    "        logits = self(idx, cache, start_pos=0)\n",
    "        cache.increment(T)\n",
    "        \n",
    "        # Sample first new token\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_token], dim=1)\n",
    "        \n",
    "        # Decode: one token at a time using cache\n",
    "        for i in range(max_new_tokens - 1):\n",
    "            # Only process the NEW token\n",
    "            logits = self(next_token, cache, start_pos=T + i)\n",
    "            cache.increment(1)\n",
    "            \n",
    "            # Sample next token\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            idx = torch.cat([idx, next_token], dim=1)\n",
    "        \n",
    "        return idx\n",
    "\n",
    "\n",
    "# Create model\n",
    "config = GPTConfig()\n",
    "model = GPT(config).to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Benchmarking Naive vs Cached Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_generation(model, prompt_len, gen_len, n_runs=5):\n",
    "    \"\"\"\n",
    "    Benchmark naive vs cached generation.\n",
    "    \n",
    "    Returns:\n",
    "        naive_time, cached_time (both in seconds)\n",
    "    \"\"\"\n",
    "    # Create prompt\n",
    "    prompt = torch.randint(0, config.vocab_size, (1, prompt_len), device=device)\n",
    "    \n",
    "    # Warmup\n",
    "    _ = model.generate_naive(prompt.clone(), 5)\n",
    "    _ = model.generate_cached(prompt.clone(), 5)\n",
    "    \n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark naive\n",
    "    naive_times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.perf_counter()\n",
    "        _ = model.generate_naive(prompt.clone(), gen_len)\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        naive_times.append(time.perf_counter() - start)\n",
    "    \n",
    "    # Benchmark cached\n",
    "    cached_times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.perf_counter()\n",
    "        _ = model.generate_cached(prompt.clone(), gen_len)\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        cached_times.append(time.perf_counter() - start)\n",
    "    \n",
    "    return np.mean(naive_times), np.mean(cached_times)\n",
    "\n",
    "\n",
    "# Benchmark for different generation lengths\n",
    "print(\"Benchmarking generation speed...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "prompt_len = 32\n",
    "gen_lengths = [8, 16, 32, 64, 128]\n",
    "results = []\n",
    "\n",
    "for gen_len in gen_lengths:\n",
    "    naive_t, cached_t = benchmark_generation(model, prompt_len, gen_len)\n",
    "    speedup = naive_t / cached_t\n",
    "    results.append({\n",
    "        'gen_len': gen_len,\n",
    "        'naive': naive_t,\n",
    "        'cached': cached_t,\n",
    "        'speedup': speedup\n",
    "    })\n",
    "    print(f\"Gen {gen_len:3d} tokens: Naive={naive_t:.3f}s, Cached={cached_t:.3f}s, Speedup={speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize benchmark results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "gen_lens = [r['gen_len'] for r in results]\n",
    "naive_times = [r['naive'] for r in results]\n",
    "cached_times = [r['cached'] for r in results]\n",
    "speedups = [r['speedup'] for r in results]\n",
    "\n",
    "# Time comparison\n",
    "axes[0].plot(gen_lens, naive_times, 'r-o', label='Naive', linewidth=2, markersize=8)\n",
    "axes[0].plot(gen_lens, cached_times, 'g-o', label='KV Cache', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Tokens Generated')\n",
    "axes[0].set_ylabel('Time (seconds)')\n",
    "axes[0].set_title('Generation Time Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Speedup\n",
    "bars = axes[1].bar(gen_lens, speedups, color='steelblue', width=10)\n",
    "axes[1].axhline(y=1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[1].set_xlabel('Tokens Generated')\n",
    "axes[1].set_ylabel('Speedup (x)')\n",
    "axes[1].set_title('KV Cache Speedup Factor')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar, speedup in zip(bars, speedups):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n",
    "                 f'{speedup:.1f}x', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('kv_cache_speedup.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAverage speedup: {np.mean(speedups):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Scaling Analysis - How Speedup Grows with Sequence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theoretical_speedup(prompt_len, gen_len):\n",
    "    \"\"\"\n",
    "    Compute theoretical speedup from KV caching.\n",
    "    \n",
    "    Naive: Each new token requires attention over all previous tokens\n",
    "    - Token 1: attend to 1+prompt_len tokens\n",
    "    - Token 2: attend to 2+prompt_len tokens\n",
    "    - ...\n",
    "    - Token n: attend to n+prompt_len tokens\n",
    "    Total: sum of (prompt_len + i) for i in 1..gen_len\n",
    "           = gen_len * prompt_len + gen_len * (gen_len + 1) / 2\n",
    "    \n",
    "    Cached: \n",
    "    - Prefill: prompt_len^2 (full attention over prompt)\n",
    "    - Each decode: attend to (prompt_len + i) tokens, but only compute Q for 1 token\n",
    "    - Total attention computations: prompt_len^2 + sum of (prompt_len + i) for i in 1..gen_len\n",
    "    - BUT K,V computations are only gen_len (one per new token)\n",
    "    \n",
    "    The key savings is in K,V projection, not attention.\n",
    "    \"\"\"\n",
    "    # K,V computation counts\n",
    "    naive_kv = sum(prompt_len + i for i in range(1, gen_len + 1))\n",
    "    cached_kv = prompt_len + gen_len  # Prefill once, then one per token\n",
    "    \n",
    "    return naive_kv / cached_kv\n",
    "\n",
    "\n",
    "# Visualize theoretical speedup\n",
    "prompt_lens = [16, 32, 64, 128]\n",
    "gen_lens = np.arange(1, 257, 4)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "for prompt_len in prompt_lens:\n",
    "    speedups = [theoretical_speedup(prompt_len, g) for g in gen_lens]\n",
    "    plt.plot(gen_lens, speedups, label=f'Prompt={prompt_len}', linewidth=2)\n",
    "\n",
    "plt.xlabel('Tokens Generated')\n",
    "plt.ylabel('Theoretical Speedup (K,V computations)')\n",
    "plt.title('KV Cache Theoretical Speedup\\n(Grows approximately linearly with generation length)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('kv_cache_theoretical_speedup.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: Speedup grows roughly linearly with generation length.\")\n",
    "print(\"For long generations, KV caching provides massive speedups.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Memory Usage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kv_cache_memory(batch_size, seq_len, n_layers, d_model, n_heads, dtype_bytes=4):\n",
    "    \"\"\"\n",
    "    Compute KV cache memory in bytes.\n",
    "    \n",
    "    For each layer:\n",
    "        K: [batch, n_heads, seq_len, head_dim]\n",
    "        V: [batch, n_heads, seq_len, head_dim]\n",
    "    \"\"\"\n",
    "    head_dim = d_model // n_heads\n",
    "    per_layer = 2 * batch_size * n_heads * seq_len * head_dim * dtype_bytes\n",
    "    total = n_layers * per_layer\n",
    "    return total\n",
    "\n",
    "\n",
    "def visualize_memory_tradeoff():\n",
    "    \"\"\"Show memory vs speed tradeoff.\"\"\"\n",
    "    \n",
    "    # Simulate a larger model\n",
    "    batch_sizes = [1, 2, 4, 8, 16, 32]\n",
    "    seq_lens = [512, 1024, 2048, 4096, 8192]\n",
    "    \n",
    "    # Model config (simulating a ~7B model)\n",
    "    d_model = 4096\n",
    "    n_heads = 32\n",
    "    n_layers = 32\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Memory vs sequence length (batch=1)\n",
    "    ax = axes[0]\n",
    "    memory_gb = [compute_kv_cache_memory(1, s, n_layers, d_model, n_heads) / 1e9 for s in seq_lens]\n",
    "    \n",
    "    ax.bar([str(s) for s in seq_lens], memory_gb, color='steelblue')\n",
    "    ax.set_xlabel('Sequence Length')\n",
    "    ax.set_ylabel('KV Cache Memory (GB)')\n",
    "    ax.set_title('KV Cache Memory vs Sequence Length\\n(7B model, batch=1, fp32)')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for i, (s, m) in enumerate(zip(seq_lens, memory_gb)):\n",
    "        ax.text(i, m, f'{m:.1f} GB', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Memory vs batch size (seq=2048)\n",
    "    ax = axes[1]\n",
    "    memory_gb = [compute_kv_cache_memory(b, 2048, n_layers, d_model, n_heads) / 1e9 for b in batch_sizes]\n",
    "    \n",
    "    ax.bar([str(b) for b in batch_sizes], memory_gb, color='green')\n",
    "    ax.set_xlabel('Batch Size')\n",
    "    ax.set_ylabel('KV Cache Memory (GB)')\n",
    "    ax.set_title('KV Cache Memory vs Batch Size\\n(7B model, seq=2048, fp32)')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for i, (b, m) in enumerate(zip(batch_sizes, memory_gb)):\n",
    "        ax.text(i, m, f'{m:.1f} GB', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('kv_cache_memory.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nMemory analysis for 7B model (32 layers, d=4096):\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Model weights (fp16): ~14 GB\")\n",
    "    print(f\"KV cache (seq=4096, batch=1, fp16): ~{compute_kv_cache_memory(1, 4096, 32, 4096, 32, 2)/1e9:.1f} GB\")\n",
    "    print(f\"KV cache (seq=4096, batch=8, fp16): ~{compute_kv_cache_memory(8, 4096, 32, 4096, 32, 2)/1e9:.1f} GB\")\n",
    "    print(\"\\nKey insight: KV cache memory scales with batch_size * seq_len\")\n",
    "    print(\"For long sequences or large batches, cache can exceed model size!\")\n",
    "\n",
    "visualize_memory_tradeoff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: What's Actually Being Cached?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_cache_contents(model, prompt_len=16, gen_len=8):\n",
    "    \"\"\"Visualize what gets stored in the KV cache.\"\"\"\n",
    "    \n",
    "    # Generate with cache and capture cache state\n",
    "    prompt = torch.randint(0, config.vocab_size, (1, prompt_len), device=device)\n",
    "    \n",
    "    # Initialize cache\n",
    "    cache = KVCache(\n",
    "        batch_size=1,\n",
    "        max_seq_len=config.max_seq_len,\n",
    "        n_layers=config.n_layers,\n",
    "        n_heads=config.n_heads,\n",
    "        head_dim=config.d_model // config.n_heads,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    model.train(False)\n",
    "    \n",
    "    # Prefill\n",
    "    with torch.no_grad():\n",
    "        _ = model(prompt, cache, start_pos=0)\n",
    "    cache.increment(prompt_len)\n",
    "    \n",
    "    # Capture cache state after prefill\n",
    "    prefill_k = cache.cache_k[0, 0, 0, :prompt_len, :8].cpu().numpy()  # First layer, first head, first 8 dims\n",
    "    prefill_v = cache.cache_v[0, 0, 0, :prompt_len, :8].cpu().numpy()\n",
    "    \n",
    "    # Continue generation\n",
    "    next_token = torch.randint(0, config.vocab_size, (1, 1), device=device)\n",
    "    for i in range(gen_len):\n",
    "        with torch.no_grad():\n",
    "            _ = model(next_token, cache, start_pos=prompt_len + i)\n",
    "        cache.increment(1)\n",
    "        next_token = torch.randint(0, config.vocab_size, (1, 1), device=device)\n",
    "    \n",
    "    # Capture full cache state\n",
    "    total_len = prompt_len + gen_len\n",
    "    full_k = cache.cache_k[0, 0, 0, :total_len, :8].cpu().numpy()\n",
    "    full_v = cache.cache_v[0, 0, 0, :total_len, :8].cpu().numpy()\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "    \n",
    "    # K cache after prefill\n",
    "    im1 = axes[0, 0].imshow(prefill_k.T, aspect='auto', cmap='RdBu', vmin=-2, vmax=2)\n",
    "    axes[0, 0].set_xlabel('Token Position')\n",
    "    axes[0, 0].set_ylabel('Key Dimension')\n",
    "    axes[0, 0].set_title(f'Key Cache After Prefill (Layer 0, Head 0)\\n({prompt_len} prompt tokens)')\n",
    "    axes[0, 0].axvline(x=prompt_len-0.5, color='green', linestyle='--', linewidth=2)\n",
    "    plt.colorbar(im1, ax=axes[0, 0])\n",
    "    \n",
    "    # K cache after generation\n",
    "    im2 = axes[0, 1].imshow(full_k.T, aspect='auto', cmap='RdBu', vmin=-2, vmax=2)\n",
    "    axes[0, 1].set_xlabel('Token Position')\n",
    "    axes[0, 1].set_ylabel('Key Dimension')\n",
    "    axes[0, 1].set_title(f'Key Cache After Generation\\n({prompt_len} prompt + {gen_len} generated)')\n",
    "    axes[0, 1].axvline(x=prompt_len-0.5, color='green', linestyle='--', linewidth=2, label='Prompt/Gen boundary')\n",
    "    axes[0, 1].legend(loc='upper right')\n",
    "    plt.colorbar(im2, ax=axes[0, 1])\n",
    "    \n",
    "    # V cache after prefill\n",
    "    im3 = axes[1, 0].imshow(prefill_v.T, aspect='auto', cmap='RdBu', vmin=-2, vmax=2)\n",
    "    axes[1, 0].set_xlabel('Token Position')\n",
    "    axes[1, 0].set_ylabel('Value Dimension')\n",
    "    axes[1, 0].set_title(f'Value Cache After Prefill (Layer 0, Head 0)')\n",
    "    axes[1, 0].axvline(x=prompt_len-0.5, color='green', linestyle='--', linewidth=2)\n",
    "    plt.colorbar(im3, ax=axes[1, 0])\n",
    "    \n",
    "    # V cache after generation\n",
    "    im4 = axes[1, 1].imshow(full_v.T, aspect='auto', cmap='RdBu', vmin=-2, vmax=2)\n",
    "    axes[1, 1].set_xlabel('Token Position')\n",
    "    axes[1, 1].set_ylabel('Value Dimension')\n",
    "    axes[1, 1].set_title(f'Value Cache After Generation')\n",
    "    axes[1, 1].axvline(x=prompt_len-0.5, color='green', linestyle='--', linewidth=2)\n",
    "    plt.colorbar(im4, ax=axes[1, 1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('kv_cache_contents.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nCache statistics:\")\n",
    "    print(f\"  Total K entries: {total_len}\")\n",
    "    print(f\"  Total V entries: {total_len}\")\n",
    "    print(f\"  Memory used: {cache.memory_usage():.2f} MB\")\n",
    "\n",
    "visualize_cache_contents(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Summary and Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "KV CACHING: SUMMARY\n",
    "===================\n",
    "\n",
    "What is KV Caching?\n",
    "-------------------\n",
    "During autoregressive generation, cache the Key and Value tensors\n",
    "from previous tokens. When generating the next token, only compute\n",
    "K and V for the new token, reusing cached values for attention.\n",
    "\n",
    "Why It Works:\n",
    "-------------\n",
    "- Attention: Q_new @ K_all.T @ V_all\n",
    "- Without cache: Recompute K_all and V_all every step\n",
    "- With cache: K_all and V_all already stored, just append new K,V\n",
    "\n",
    "Complexity:\n",
    "-----------\n",
    "- Naive generation of n tokens: O(n^2) K,V computations\n",
    "- Cached generation of n tokens: O(n) K,V computations\n",
    "- Speedup grows roughly linearly with generation length\n",
    "\n",
    "Memory Cost:\n",
    "------------\n",
    "- Per layer: 2 * batch * seq_len * d_model * dtype_size\n",
    "- Total: n_layers * (above)\n",
    "- For 7B model (32 layers, d=4096), seq=4096, batch=1:\n",
    "  - fp16: ~4 GB\n",
    "  - fp32: ~8 GB\n",
    "\n",
    "Best Practices:\n",
    "---------------\n",
    "1. Pre-allocate cache to max_seq_len to avoid dynamic allocation\n",
    "2. Use fp16 for cache to reduce memory by 2x\n",
    "3. For very long sequences, consider chunked/paged attention\n",
    "4. Batch generation requests when possible to amortize prefill\n",
    "\n",
    "Advanced Techniques:\n",
    "--------------------\n",
    "- PagedAttention (vLLM): Virtual memory for KV cache\n",
    "- Grouped-Query Attention: Share K,V across query heads\n",
    "- Multi-Query Attention: Single K,V for all heads\n",
    "- Sliding Window: Only cache recent N tokens\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Batch generation**: Modify the cached generation to handle multiple prompts simultaneously. How does batch size affect speedup?\n",
    "\n",
    "2. **Memory optimization**: Implement a version that uses fp16 for the cache. Measure memory savings.\n",
    "\n",
    "3. **Sliding window**: Implement a sliding window cache that only stores the last N tokens. Compare quality vs memory.\n",
    "\n",
    "4. **Grouped-query attention**: Implement GQA where K,V are shared across groups of query heads. How much memory is saved?\n",
    "\n",
    "5. **Prefill chunking**: For very long prompts, implement chunked prefill to avoid memory spikes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Concept | Key Point |\n",
    "|---------|----------|\n",
    "| Problem | Naive generation recomputes K,V for all tokens each step |\n",
    "| Solution | Cache K,V tensors, only compute for new tokens |\n",
    "| Speedup | Roughly linear with generation length (n^2 -> n) |\n",
    "| Memory | Grows with batch_size * seq_len * n_layers * d_model |\n",
    "| Prefill | Process entire prompt once, populate cache |\n",
    "| Decode | One token at a time, append to cache |\n",
    "\n",
    "**Key insight:** KV caching is essential for practical LLM inference. Without it, generating 1000 tokens would require ~500x more computation than necessary. The memory cost is the tradeoffâ€”for long sequences or large batches, the cache can become substantial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
