{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11: LoRA from Scratch\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Understand why fine-tuning weight changes are low-rank\n",
    "2. Implement LoRA (Low-Rank Adaptation) from first principles\n",
    "3. Compare parameter counts: full fine-tuning vs LoRA\n",
    "4. Train LoRA adapters on a classification task\n",
    "5. Demonstrate merging LoRA weights for zero-overhead inference\n",
    "\n",
    "**Prerequisites:** [fine-tuning](../modern-llms/fine-tuning.md), [efficient-adaptation](../modern-llms/efficient-adaptation.md)\n",
    "\n",
    "**Framework:** PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Intuition - Weight Changes Are Low-Rank\n",
    "\n",
    "When we fine-tune a model, how much do the weights actually change? Let's simulate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_weight_change_rank(d_in, d_out, simulated_rank=8):\n",
    "    \"\"\"\n",
    "    Demonstrate that fine-tuning changes often have low effective rank.\n",
    "    \n",
    "    In practice, fine-tuning updates W -> W + delta_W where delta_W is low-rank.\n",
    "    \"\"\"\n",
    "    # Simulate a weight change that's inherently low-rank\n",
    "    # (In real fine-tuning, this emerges from the optimization process)\n",
    "    \n",
    "    # True low-rank structure: delta_W = B @ A where B is d_out x r, A is r x d_in\n",
    "    true_B = torch.randn(d_out, simulated_rank) * 0.1\n",
    "    true_A = torch.randn(simulated_rank, d_in) * 0.1\n",
    "    delta_W = true_B @ true_A  # This is our \"weight change\"\n",
    "    \n",
    "    # SVD to analyze the rank structure\n",
    "    U, S, Vh = torch.linalg.svd(delta_W, full_matrices=False)\n",
    "    \n",
    "    # How much variance is captured by top-k singular values?\n",
    "    total_variance = (S ** 2).sum().item()\n",
    "    cumulative_variance = torch.cumsum(S ** 2, dim=0) / total_variance\n",
    "    \n",
    "    return S.numpy(), cumulative_variance.numpy()\n",
    "\n",
    "\n",
    "# Analyze\n",
    "d_in, d_out = 768, 768  # Typical transformer hidden dim\n",
    "singular_values, cumvar = analyze_weight_change_rank(d_in, d_out, simulated_rank=8)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Singular values\n",
    "axes[0].bar(range(min(50, len(singular_values))), singular_values[:50], color='steelblue')\n",
    "axes[0].set_xlabel('Singular Value Index')\n",
    "axes[0].set_ylabel('Singular Value')\n",
    "axes[0].set_title('Singular Values of Weight Change (delta_W)\\n(First 50 shown)')\n",
    "axes[0].axvline(x=8, color='red', linestyle='--', label='True rank=8')\n",
    "axes[0].legend()\n",
    "\n",
    "# Cumulative variance\n",
    "axes[1].plot(range(1, min(50, len(cumvar))+1), cumvar[:50], 'b-', linewidth=2)\n",
    "axes[1].axhline(y=0.99, color='gray', linestyle='--', alpha=0.7, label='99% variance')\n",
    "axes[1].axvline(x=8, color='red', linestyle='--', label='Rank 8')\n",
    "axes[1].set_xlabel('Number of Components')\n",
    "axes[1].set_ylabel('Cumulative Variance Explained')\n",
    "axes[1].set_title('Variance Captured by Top-k Singular Values')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lora_rank_intuition.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nKey insight: With true rank {8}, first 8 components capture {cumvar[7]*100:.1f}% of variance\")\n",
    "print(\"This means we can represent delta_W with far fewer parameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implementing LoRA from Scratch\n",
    "\n",
    "LoRA replaces the weight update $\\Delta W$ with a low-rank factorization:\n",
    "\n",
    "$$W' = W + \\Delta W = W + BA$$\n",
    "\n",
    "Where:\n",
    "- $W \\in \\mathbb{R}^{d_{out} \\times d_{in}}$ is the frozen pretrained weight\n",
    "- $B \\in \\mathbb{R}^{d_{out} \\times r}$ is trainable\n",
    "- $A \\in \\mathbb{R}^{r \\times d_{in}}$ is trainable\n",
    "- $r \\ll \\min(d_{out}, d_{in})$ is the rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear layer with LoRA adaptation.\n",
    "    \n",
    "    The original weight W is frozen. We learn low-rank matrices A and B\n",
    "    such that the effective weight becomes W + (alpha/r) * B @ A.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, original_linear: nn.Linear, rank: int = 8, alpha: float = 16.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.original = original_linear\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        d_out, d_in = original_linear.weight.shape\n",
    "        \n",
    "        # Freeze original weights\n",
    "        self.original.weight.requires_grad = False\n",
    "        if self.original.bias is not None:\n",
    "            self.original.bias.requires_grad = False\n",
    "        \n",
    "        # LoRA matrices\n",
    "        # A is initialized with small random values\n",
    "        # B is initialized with zeros so that delta_W starts at zero\n",
    "        self.lora_A = nn.Parameter(torch.randn(rank, d_in) * 0.01)\n",
    "        self.lora_B = nn.Parameter(torch.zeros(d_out, rank))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Original output (frozen path)\n",
    "        base_output = self.original(x)\n",
    "        \n",
    "        # LoRA output (trainable path)\n",
    "        # x @ A.T @ B.T = x @ (B @ A).T\n",
    "        lora_output = F.linear(F.linear(x, self.lora_A), self.lora_B)\n",
    "        \n",
    "        return base_output + self.scaling * lora_output\n",
    "    \n",
    "    def merge_weights(self):\n",
    "        \"\"\"Merge LoRA weights into original for inference (no overhead).\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # W' = W + scaling * B @ A\n",
    "            merged = self.original.weight + self.scaling * (self.lora_B @ self.lora_A)\n",
    "        return merged\n",
    "    \n",
    "    @property\n",
    "    def num_trainable_params(self):\n",
    "        return self.lora_A.numel() + self.lora_B.numel()\n",
    "    \n",
    "    @property\n",
    "    def num_total_params(self):\n",
    "        return self.original.weight.numel() + self.num_trainable_params\n",
    "\n",
    "\n",
    "# Demonstrate parameter savings\n",
    "d_in, d_out = 768, 768\n",
    "rank = 8\n",
    "\n",
    "original = nn.Linear(d_in, d_out, bias=False)\n",
    "lora = LoRALinear(original, rank=rank)\n",
    "\n",
    "full_params = d_in * d_out\n",
    "lora_params = lora.num_trainable_params\n",
    "\n",
    "print(f\"Full fine-tuning parameters: {full_params:,}\")\n",
    "print(f\"LoRA parameters (rank={rank}): {lora_params:,}\")\n",
    "print(f\"Parameter reduction: {100 * (1 - lora_params / full_params):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that output starts the same (B initialized to zeros)\n",
    "x = torch.randn(2, 10, d_in)  # [batch, seq, features]\n",
    "\n",
    "with torch.no_grad():\n",
    "    original_out = original(x)\n",
    "    lora_out = lora(x)\n",
    "\n",
    "print(\"Initial outputs match (B=0 means delta_W=0):\")\n",
    "print(f\"  Max difference: {(original_out - lora_out).abs().max().item():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Building a LoRA-Enabled Transformer\n",
    "\n",
    "Let's create a small transformer and add LoRA to its attention projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention without LoRA.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        Q = self.W_q(x).view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        K = self.W_k(x).view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        V = self.W_v(x).view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        \n",
    "        # Attention\n",
    "        scores = (Q @ K.transpose(-2, -1)) / (self.d_head ** 0.5)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "        \n",
    "        # Output\n",
    "        out = (weights @ V).transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.W_o(out)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(F.gelu(self.linear1(x))))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.ffn = FeedForward(d_model, dropout=dropout)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        x = x + self.dropout(self.attn(self.ln1(x), mask))\n",
    "        x = x + self.dropout(self.ffn(self.ln2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class SmallTransformer(nn.Module):\n",
    "    \"\"\"Small transformer for classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers, num_classes, max_len=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        B, T = input_ids.shape\n",
    "        \n",
    "        # Embeddings\n",
    "        pos = torch.arange(T, device=input_ids.device)\n",
    "        x = self.dropout(self.tok_emb(input_ids) + self.pos_emb(pos))\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.ln_f(x)\n",
    "        \n",
    "        # Pool and classify (use last token)\n",
    "        if attention_mask is not None:\n",
    "            seq_lengths = attention_mask.sum(dim=1) - 1\n",
    "            pooled = x[torch.arange(B, device=x.device), seq_lengths]\n",
    "        else:\n",
    "            pooled = x[:, -1]\n",
    "        \n",
    "        return self.classifier(pooled)\n",
    "\n",
    "\n",
    "# Create model\n",
    "vocab_size = 1000\n",
    "d_model = 256\n",
    "n_heads = 4\n",
    "n_layers = 4\n",
    "num_classes = 2\n",
    "\n",
    "model = SmallTransformer(vocab_size, d_model, n_heads, n_layers, num_classes).to(device)\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lora_to_model(model, rank=8, alpha=16.0, target_modules=['W_q', 'W_v']):\n",
    "    \"\"\"\n",
    "    Add LoRA adapters to specified modules in a model.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to modify\n",
    "        rank: LoRA rank\n",
    "        alpha: LoRA scaling factor\n",
    "        target_modules: List of module names to add LoRA to\n",
    "    \n",
    "    Returns:\n",
    "        Modified model with LoRA layers\n",
    "    \"\"\"\n",
    "    lora_layers = []\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, MultiHeadAttention):\n",
    "            for target in target_modules:\n",
    "                if hasattr(module, target):\n",
    "                    original_linear = getattr(module, target)\n",
    "                    lora_linear = LoRALinear(original_linear, rank=rank, alpha=alpha)\n",
    "                    setattr(module, target, lora_linear)\n",
    "                    lora_layers.append(lora_linear)\n",
    "    \n",
    "    return model, lora_layers\n",
    "\n",
    "\n",
    "def count_parameters(model, only_trainable=False):\n",
    "    \"\"\"Count model parameters.\"\"\"\n",
    "    if only_trainable:\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\n",
    "def freeze_non_lora_params(model):\n",
    "    \"\"\"Freeze all parameters except LoRA.\"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lora_' not in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "# Create a fresh model and add LoRA\n",
    "lora_model = SmallTransformer(vocab_size, d_model, n_heads, n_layers, num_classes).to(device)\n",
    "\n",
    "print(\"Before LoRA:\")\n",
    "print(f\"  Total params: {count_parameters(lora_model):,}\")\n",
    "print(f\"  Trainable params: {count_parameters(lora_model, only_trainable=True):,}\")\n",
    "\n",
    "# Add LoRA to Q and V projections\n",
    "lora_model, lora_layers = add_lora_to_model(lora_model, rank=8, alpha=16.0, target_modules=['W_q', 'W_v'])\n",
    "freeze_non_lora_params(lora_model)\n",
    "\n",
    "print(\"\\nAfter LoRA (rank=8, Q and V only):\")\n",
    "print(f\"  Total params: {count_parameters(lora_model):,}\")\n",
    "print(f\"  Trainable params: {count_parameters(lora_model, only_trainable=True):,}\")\n",
    "print(f\"  LoRA layers added: {len(lora_layers)}\")\n",
    "\n",
    "# Compare to full fine-tuning\n",
    "full_model = SmallTransformer(vocab_size, d_model, n_heads, n_layers, num_classes).to(device)\n",
    "print(f\"\\nFull fine-tuning would train: {count_parameters(full_model):,} params\")\n",
    "print(f\"LoRA trains only: {count_parameters(lora_model, only_trainable=True):,} params\")\n",
    "print(f\"Reduction: {100 * (1 - count_parameters(lora_model, only_trainable=True) / count_parameters(full_model)):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Training LoRA vs Full Fine-Tuning\n",
    "\n",
    "Let's compare LoRA and full fine-tuning on a simple classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic classification dataset\n",
    "def create_synthetic_data(num_samples, vocab_size, seq_len, num_classes=2):\n",
    "    \"\"\"\n",
    "    Create synthetic data where class is determined by token patterns.\n",
    "    Class 0: sequences with more low-id tokens (0-vocab_size/2)\n",
    "    Class 1: sequences with more high-id tokens (vocab_size/2-vocab_size)\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    mid = vocab_size // 2\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # Decide class\n",
    "        label = np.random.randint(0, num_classes)\n",
    "        \n",
    "        if label == 0:\n",
    "            # More low tokens\n",
    "            tokens = np.random.randint(0, mid, size=seq_len)\n",
    "            # Sprinkle some high tokens\n",
    "            noise_idx = np.random.choice(seq_len, size=seq_len//4, replace=False)\n",
    "            tokens[noise_idx] = np.random.randint(mid, vocab_size, size=len(noise_idx))\n",
    "        else:\n",
    "            # More high tokens\n",
    "            tokens = np.random.randint(mid, vocab_size, size=seq_len)\n",
    "            # Sprinkle some low tokens\n",
    "            noise_idx = np.random.choice(seq_len, size=seq_len//4, replace=False)\n",
    "            tokens[noise_idx] = np.random.randint(0, mid, size=len(noise_idx))\n",
    "        \n",
    "        data.append((tokens.tolist(), label))\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "class SyntheticDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens, label = self.data[idx]\n",
    "        return {\n",
    "            'input_ids': torch.tensor(tokens, dtype=torch.long),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "seq_len = 32\n",
    "train_data = create_synthetic_data(1000, vocab_size, seq_len)\n",
    "test_data = create_synthetic_data(200, vocab_size, seq_len)\n",
    "\n",
    "train_dataset = SyntheticDataset(train_data)\n",
    "test_dataset = SyntheticDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, epochs=10, lr=1e-4, description=\"\"):\n",
    "    \"\"\"Train a model and track metrics.\"\"\"\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=lr,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    metrics = {'train_loss': [], 'train_acc': [], 'test_acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids)\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        \n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        # Evaluation\n",
    "        model.train(False)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                logits = model(input_ids)\n",
    "                preds = logits.argmax(dim=-1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        \n",
    "        test_acc = correct / total\n",
    "        \n",
    "        metrics['train_loss'].append(train_loss)\n",
    "        metrics['train_acc'].append(train_acc)\n",
    "        metrics['test_acc'].append(test_acc)\n",
    "        \n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            print(f\"{description} Epoch {epoch+1}: Loss={train_loss:.4f}, Train Acc={train_acc:.3f}, Test Acc={test_acc:.3f}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with full fine-tuning\n",
    "print(\"=\" * 60)\n",
    "print(\"FULL FINE-TUNING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "full_model = SmallTransformer(vocab_size, d_model, n_heads, n_layers, num_classes).to(device)\n",
    "full_trainable = count_parameters(full_model, only_trainable=True)\n",
    "print(f\"Trainable parameters: {full_trainable:,}\")\n",
    "\n",
    "full_metrics = train_model(full_model, train_loader, test_loader, epochs=10, lr=1e-4, description=\"[Full]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with LoRA\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LORA FINE-TUNING (rank=8)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "lora_model = SmallTransformer(vocab_size, d_model, n_heads, n_layers, num_classes).to(device)\n",
    "lora_model, _ = add_lora_to_model(lora_model, rank=8, alpha=16.0, target_modules=['W_q', 'W_v'])\n",
    "freeze_non_lora_params(lora_model)\n",
    "\n",
    "lora_trainable = count_parameters(lora_model, only_trainable=True)\n",
    "print(f\"Trainable parameters: {lora_trainable:,} ({100*lora_trainable/full_trainable:.1f}% of full)\")\n",
    "\n",
    "lora_metrics = train_model(lora_model, train_loader, test_loader, epochs=10, lr=1e-3, description=\"[LoRA]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with different LoRA ranks\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LORA RANK COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "rank_results = {}\n",
    "\n",
    "for rank in [2, 4, 8, 16, 32]:\n",
    "    model = SmallTransformer(vocab_size, d_model, n_heads, n_layers, num_classes).to(device)\n",
    "    model, _ = add_lora_to_model(model, rank=rank, alpha=2*rank, target_modules=['W_q', 'W_v'])\n",
    "    freeze_non_lora_params(model)\n",
    "    \n",
    "    trainable = count_parameters(model, only_trainable=True)\n",
    "    metrics = train_model(model, train_loader, test_loader, epochs=10, lr=1e-3, description=f\"[r={rank}]\")\n",
    "    \n",
    "    rank_results[rank] = {\n",
    "        'params': trainable,\n",
    "        'final_test_acc': metrics['test_acc'][-1],\n",
    "        'metrics': metrics\n",
    "    }\n",
    "    print(f\"Rank {rank}: {trainable:,} params, Final Test Acc: {metrics['test_acc'][-1]:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "epochs = range(1, 11)\n",
    "\n",
    "# Test accuracy over training\n",
    "axes[0].plot(epochs, full_metrics['test_acc'], 'k-', label='Full fine-tune', linewidth=2)\n",
    "axes[0].plot(epochs, lora_metrics['test_acc'], 'b-', label='LoRA (r=8)', linewidth=2)\n",
    "for rank, data in rank_results.items():\n",
    "    if rank != 8:\n",
    "        axes[0].plot(epochs, data['metrics']['test_acc'], '--', alpha=0.6, label=f'LoRA (r={rank})')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Test Accuracy')\n",
    "axes[0].set_title('Learning Curves')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Final accuracy vs parameters\n",
    "ranks = list(rank_results.keys())\n",
    "params = [rank_results[r]['params'] for r in ranks]\n",
    "accs = [rank_results[r]['final_test_acc'] for r in ranks]\n",
    "\n",
    "axes[1].scatter(params, accs, s=100, c='steelblue', zorder=5)\n",
    "for r, p, a in zip(ranks, params, accs):\n",
    "    axes[1].annotate(f'r={r}', (p, a), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "# Add full fine-tuning point\n",
    "axes[1].scatter([full_trainable], [full_metrics['test_acc'][-1]], s=150, c='red', marker='*', zorder=5, label='Full fine-tune')\n",
    "axes[1].annotate('Full', (full_trainable, full_metrics['test_acc'][-1]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "axes[1].set_xlabel('Trainable Parameters')\n",
    "axes[1].set_ylabel('Final Test Accuracy')\n",
    "axes[1].set_title('Accuracy vs Parameter Count')\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Parameter comparison bar chart\n",
    "methods = ['Full'] + [f'LoRA\\nr={r}' for r in ranks]\n",
    "param_counts = [full_trainable] + params\n",
    "colors = ['red'] + ['steelblue'] * len(ranks)\n",
    "\n",
    "bars = axes[2].bar(methods, param_counts, color=colors)\n",
    "axes[2].set_ylabel('Trainable Parameters')\n",
    "axes[2].set_title('Parameter Count Comparison')\n",
    "axes[2].set_yscale('log')\n",
    "\n",
    "# Add text labels\n",
    "for bar, count in zip(bars, param_counts):\n",
    "    axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
    "                 f'{count:,}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('lora_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Merging LoRA Weights\n",
    "\n",
    "One key advantage of LoRA: you can merge the adapters into the base weights for zero-overhead inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_lora_weights(model):\n",
    "    \"\"\"\n",
    "    Merge LoRA weights into the original weights.\n",
    "    After merging, the model can run without LoRA overhead.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, LoRALinear):\n",
    "            # Merge: W' = W + scaling * B @ A\n",
    "            merged_weight = module.merge_weights()\n",
    "            module.original.weight.data = merged_weight\n",
    "            \n",
    "            # Reset LoRA matrices to zero (effectively disabling them)\n",
    "            module.lora_A.data.zero_()\n",
    "            module.lora_B.data.zero_()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Demonstrate merging\n",
    "print(\"Before merging:\")\n",
    "sample_input = torch.randint(0, vocab_size, (2, seq_len)).to(device)\n",
    "\n",
    "lora_model.train(False)\n",
    "with torch.no_grad():\n",
    "    output_before = lora_model(sample_input)\n",
    "\n",
    "# Check a LoRA layer\n",
    "for name, module in lora_model.named_modules():\n",
    "    if isinstance(module, LoRALinear):\n",
    "        print(f\"  LoRA A norm: {module.lora_A.norm().item():.4f}\")\n",
    "        print(f\"  LoRA B norm: {module.lora_B.norm().item():.4f}\")\n",
    "        break\n",
    "\n",
    "# Merge\n",
    "print(\"\\nMerging LoRA weights...\")\n",
    "merge_lora_weights(lora_model)\n",
    "\n",
    "print(\"\\nAfter merging:\")\n",
    "with torch.no_grad():\n",
    "    output_after = lora_model(sample_input)\n",
    "\n",
    "for name, module in lora_model.named_modules():\n",
    "    if isinstance(module, LoRALinear):\n",
    "        print(f\"  LoRA A norm: {module.lora_A.norm().item():.4f}\")\n",
    "        print(f\"  LoRA B norm: {module.lora_B.norm().item():.4f}\")\n",
    "        break\n",
    "\n",
    "# Verify outputs match\n",
    "diff = (output_before - output_after).abs().max().item()\n",
    "print(f\"\\nOutput difference after merge: {diff:.2e}\")\n",
    "print(\"(Should be ~0, meaning merge preserved behavior)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: LoRA Hyperparameters\n",
    "\n",
    "Key hyperparameters and their effects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_lora_hyperparameters():\n",
    "    \"\"\"Visualize the effect of LoRA hyperparameters.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Rank vs Parameters\n",
    "    d = 256  # Hidden dimension\n",
    "    ranks = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "    \n",
    "    # For Q and V projections in 4 layers\n",
    "    lora_params = [2 * 4 * (d * r + r * d) for r in ranks]\n",
    "    full_params = 2 * 4 * d * d  # Just Q and V\n",
    "    \n",
    "    axes[0].plot(ranks, lora_params, 'bo-', linewidth=2, markersize=8)\n",
    "    axes[0].axhline(y=full_params, color='red', linestyle='--', label=f'Full fine-tune Q+V ({full_params:,})')\n",
    "    axes[0].set_xlabel('LoRA Rank')\n",
    "    axes[0].set_ylabel('Trainable Parameters')\n",
    "    axes[0].set_title('Parameters vs LoRA Rank')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].set_xscale('log', base=2)\n",
    "    axes[0].set_yscale('log')\n",
    "    \n",
    "    # Alpha/Rank ratio (scaling)\n",
    "    ranks_demo = [4, 8, 16]\n",
    "    alphas = [4, 8, 16, 32, 64]\n",
    "    \n",
    "    for r in ranks_demo:\n",
    "        scalings = [a / r for a in alphas]\n",
    "        axes[1].plot(alphas, scalings, 'o-', label=f'rank={r}')\n",
    "    \n",
    "    axes[1].set_xlabel('Alpha')\n",
    "    axes[1].set_ylabel('Scaling Factor (alpha/rank)')\n",
    "    axes[1].set_title('LoRA Scaling Factor')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].axhline(y=1.0, color='gray', linestyle=':', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lora_hyperparameters.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"LoRA Hyperparameters:\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Rank (r):\")\n",
    "    print(\"  - Higher = more capacity, more parameters\")\n",
    "    print(\"  - Typical values: 4, 8, 16, 32, 64\")\n",
    "    print(\"  - Start with 8, increase if underfitting\")\n",
    "    print(\"\")\n",
    "    print(\"Alpha (a):\")\n",
    "    print(\"  - Controls magnitude of LoRA update\")\n",
    "    print(\"  - Effective scaling = alpha / rank\")\n",
    "    print(\"  - Common practice: alpha = 2 * rank\")\n",
    "    print(\"\")\n",
    "    print(\"Target modules:\")\n",
    "    print(\"  - Q, V only: minimal parameters\")\n",
    "    print(\"  - Q, K, V: more capacity\")\n",
    "    print(\"  - Q, K, V, O: full attention adaptation\")\n",
    "    print(\"  - + FFN: maximum capacity\")\n",
    "\n",
    "visualize_lora_hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Summary\n",
    "\n",
    "Key takeaways about LoRA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\"\"\n",
    "LoRA: Low-Rank Adaptation\n",
    "=========================\n",
    "\n",
    "Core Idea:\n",
    "  W' = W + (alpha/r) * B @ A\n",
    "  \n",
    "  - W: Frozen pretrained weight [d_out, d_in]\n",
    "  - B: Trainable [d_out, r]\n",
    "  - A: Trainable [r, d_in]\n",
    "  - r: Low rank (typically 4-64)\n",
    "\n",
    "Key Benefits:\n",
    "  1. Parameter efficiency: ~0.1-1% of full fine-tuning\n",
    "  2. Memory efficiency: No optimizer states for frozen params\n",
    "  3. Zero inference overhead: Merge weights after training\n",
    "  4. Task switching: Swap adapters without reloading base model\n",
    "\n",
    "When to Use:\n",
    "  - Memory constrained: Can't fit full fine-tuning\n",
    "  - Multiple tasks: Need different adaptations of same base\n",
    "  - Fast iteration: Train adapters quickly\n",
    "  - Production: Need efficient inference\n",
    "\n",
    "Common Configuration:\n",
    "  - Rank: 8 (start here, increase if needed)\n",
    "  - Alpha: 16 (or 2 * rank)\n",
    "  - Targets: Q and V projections minimum\n",
    "  - Learning rate: 1e-4 to 3e-4 (higher than full FT)\n",
    "\n",
    "Historical Note:\n",
    "  LoRA (Hu et al., 2021) showed that weight updates during\n",
    "  fine-tuning have low intrinsic rank, enabling dramatic\n",
    "  parameter reduction with minimal quality loss.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Different target modules**: Try adding LoRA to K and O projections. Does performance improve?\n",
    "\n",
    "2. **FFN adaptation**: Add LoRA to the feedforward layers. Compare to attention-only LoRA.\n",
    "\n",
    "3. **Multi-task LoRA**: Train separate LoRA adapters for two different tasks. Show how to swap them.\n",
    "\n",
    "4. **Rank analysis**: After training, SVD the merged weight change. Is it actually low-rank?\n",
    "\n",
    "5. **QLoRA simulation**: Quantize the base weights to int8, keep LoRA in float32. Compare accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Concept | Key Point |\n",
    "|---------|----------|\n",
    "| Low-rank insight | Fine-tuning changes live in a low-dimensional subspace |\n",
    "| LoRA formula | W' = W + (alpha/r) * B @ A |\n",
    "| Parameter savings | 99%+ reduction vs full fine-tuning |\n",
    "| Initialization | A random small, B zeros (start with no change) |\n",
    "| Merging | Add B @ A to W for zero inference overhead |\n",
    "| Target modules | Q and V minimum, add more for capacity |\n",
    "\n",
    "**Key insight:** Full fine-tuning is wastefulâ€”the actual task-specific changes occupy a tiny subspace of the full parameter space. LoRA parameterizes exactly this subspace with a low-rank factorization, achieving nearly identical results with a fraction of the parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
