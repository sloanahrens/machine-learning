{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# RNN from Scratch\n",
    "\n",
    "This notebook builds a character-level RNN from scratch using only NumPy. We'll implement the forward pass, backpropagation through time (BPTT), and train it to generate text.\n",
    "\n",
    "**Goal:** Understand how RNNs process sequences and why they struggle with long-range dependencies.\n",
    "\n",
    "**Prerequisites:** [rnns.md](../architectures/rnns.md), [backpropagation.md](../neural-networks/backpropagation.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. The RNN Cell\n",
    "\n",
    "The core RNN update rule:\n",
    "\n",
    "$$h_t = \\tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$$\n",
    "\n",
    "$$y_t = W_{hy} h_t + b_y$$\n",
    "\n",
    "Where:\n",
    "- $h_t$ is the hidden state at time $t$\n",
    "- $x_t$ is the input at time $t$\n",
    "- $y_t$ is the output at time $t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell:\n",
    "    \"\"\"\n",
    "    Single RNN cell: processes one timestep.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: Size of input vectors\n",
    "            hidden_dim: Size of hidden state\n",
    "        \"\"\"\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Xavier initialization\n",
    "        scale_xh = np.sqrt(1.0 / input_dim)\n",
    "        scale_hh = np.sqrt(1.0 / hidden_dim)\n",
    "        \n",
    "        self.W_xh = np.random.randn(input_dim, hidden_dim) * scale_xh\n",
    "        self.W_hh = np.random.randn(hidden_dim, hidden_dim) * scale_hh\n",
    "        self.b_h = np.zeros(hidden_dim)\n",
    "    \n",
    "    def forward(self, x, h_prev):\n",
    "        \"\"\"\n",
    "        Forward pass for single timestep.\n",
    "        \n",
    "        Args:\n",
    "            x: Input at current timestep [batch, input_dim]\n",
    "            h_prev: Hidden state from previous timestep [batch, hidden_dim]\n",
    "            \n",
    "        Returns:\n",
    "            h: New hidden state [batch, hidden_dim]\n",
    "        \"\"\"\n",
    "        # Store for backprop\n",
    "        self.x = x\n",
    "        self.h_prev = h_prev\n",
    "        \n",
    "        # h = tanh(W_xh @ x + W_hh @ h_prev + b)\n",
    "        self.z = x @ self.W_xh + h_prev @ self.W_hh + self.b_h\n",
    "        h = np.tanh(self.z)\n",
    "        \n",
    "        return h\n",
    "    \n",
    "    def backward(self, dh):\n",
    "        \"\"\"\n",
    "        Backward pass for single timestep.\n",
    "        \n",
    "        Args:\n",
    "            dh: Gradient w.r.t. hidden state [batch, hidden_dim]\n",
    "            \n",
    "        Returns:\n",
    "            dx: Gradient w.r.t. input\n",
    "            dh_prev: Gradient w.r.t. previous hidden state\n",
    "            grads: Dictionary of parameter gradients\n",
    "        \"\"\"\n",
    "        # Gradient through tanh: d/dz tanh(z) = 1 - tanh(z)^2\n",
    "        dz = dh * (1 - np.tanh(self.z)**2)\n",
    "        \n",
    "        # Parameter gradients\n",
    "        dW_xh = self.x.T @ dz\n",
    "        dW_hh = self.h_prev.T @ dz\n",
    "        db_h = dz.sum(axis=0)\n",
    "        \n",
    "        # Input gradients\n",
    "        dx = dz @ self.W_xh.T\n",
    "        dh_prev = dz @ self.W_hh.T\n",
    "        \n",
    "        grads = {'W_xh': dW_xh, 'W_hh': dW_hh, 'b_h': db_h}\n",
    "        return dx, dh_prev, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the cell\n",
    "cell = RNNCell(input_dim=10, hidden_dim=20)\n",
    "\n",
    "batch_size = 4\n",
    "x = np.random.randn(batch_size, 10)\n",
    "h_prev = np.zeros((batch_size, 20))\n",
    "\n",
    "h = cell.forward(x, h_prev)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Hidden shape: {h.shape}\")\n",
    "print(f\"Hidden range: [{h.min():.3f}, {h.max():.3f}] (should be in [-1, 1] due to tanh)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Character-Level Language Model\n",
    "\n",
    "Now let's build a full RNN that:\n",
    "1. Takes characters as input (one-hot encoded)\n",
    "2. Predicts the next character\n",
    "3. Trains on text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    \"\"\"\n",
    "    Character-level RNN for text generation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: Number of unique characters\n",
    "            hidden_dim: Size of hidden state\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Initialize weights\n",
    "        scale_xh = np.sqrt(1.0 / vocab_size)\n",
    "        scale_hh = np.sqrt(1.0 / hidden_dim)\n",
    "        scale_hy = np.sqrt(1.0 / hidden_dim)\n",
    "        \n",
    "        self.W_xh = np.random.randn(vocab_size, hidden_dim) * scale_xh\n",
    "        self.W_hh = np.random.randn(hidden_dim, hidden_dim) * scale_hh\n",
    "        self.W_hy = np.random.randn(hidden_dim, vocab_size) * scale_hy\n",
    "        self.b_h = np.zeros(hidden_dim)\n",
    "        self.b_y = np.zeros(vocab_size)\n",
    "        \n",
    "    def forward(self, inputs, h_init=None):\n",
    "        \"\"\"\n",
    "        Forward pass through sequence.\n",
    "        \n",
    "        Args:\n",
    "            inputs: List of character indices [seq_len]\n",
    "            h_init: Initial hidden state [hidden_dim]\n",
    "            \n",
    "        Returns:\n",
    "            outputs: Output logits at each timestep\n",
    "            hidden_states: Hidden state at each timestep\n",
    "        \"\"\"\n",
    "        seq_len = len(inputs)\n",
    "        \n",
    "        if h_init is None:\n",
    "            h_init = np.zeros(self.hidden_dim)\n",
    "        \n",
    "        # Storage for backprop\n",
    "        self.inputs = inputs\n",
    "        self.xs = {}  # One-hot inputs\n",
    "        self.hs = {-1: h_init}  # Hidden states\n",
    "        self.os = {}  # Output logits\n",
    "        self.ps = {}  # Softmax probabilities\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            # One-hot encode input\n",
    "            x = np.zeros(self.vocab_size)\n",
    "            x[inputs[t]] = 1\n",
    "            self.xs[t] = x\n",
    "            \n",
    "            # RNN step: h_t = tanh(W_xh @ x + W_hh @ h_{t-1} + b_h)\n",
    "            self.hs[t] = np.tanh(\n",
    "                x @ self.W_xh + self.hs[t-1] @ self.W_hh + self.b_h\n",
    "            )\n",
    "            \n",
    "            # Output: y_t = W_hy @ h_t + b_y\n",
    "            self.os[t] = self.hs[t] @ self.W_hy + self.b_y\n",
    "            \n",
    "            # Softmax for probabilities\n",
    "            exp_o = np.exp(self.os[t] - self.os[t].max())\n",
    "            self.ps[t] = exp_o / exp_o.sum()\n",
    "        \n",
    "        return self.ps, self.hs\n",
    "    \n",
    "    def loss(self, targets):\n",
    "        \"\"\"\n",
    "        Compute cross-entropy loss.\n",
    "        \n",
    "        Args:\n",
    "            targets: List of target character indices [seq_len]\n",
    "            \n",
    "        Returns:\n",
    "            loss: Average cross-entropy loss\n",
    "        \"\"\"\n",
    "        loss = 0\n",
    "        for t in range(len(targets)):\n",
    "            loss -= np.log(self.ps[t][targets[t]] + 1e-10)\n",
    "        return loss / len(targets)\n",
    "    \n",
    "    def backward(self, targets):\n",
    "        \"\"\"\n",
    "        Backpropagation through time (BPTT).\n",
    "        \n",
    "        Args:\n",
    "            targets: List of target character indices [seq_len]\n",
    "            \n",
    "        Returns:\n",
    "            grads: Dictionary of parameter gradients\n",
    "        \"\"\"\n",
    "        seq_len = len(targets)\n",
    "        \n",
    "        # Initialize gradients\n",
    "        dW_xh = np.zeros_like(self.W_xh)\n",
    "        dW_hh = np.zeros_like(self.W_hh)\n",
    "        dW_hy = np.zeros_like(self.W_hy)\n",
    "        db_h = np.zeros_like(self.b_h)\n",
    "        db_y = np.zeros_like(self.b_y)\n",
    "        \n",
    "        # Gradient flowing back through hidden states\n",
    "        dh_next = np.zeros(self.hidden_dim)\n",
    "        \n",
    "        # Go backwards through time\n",
    "        for t in reversed(range(seq_len)):\n",
    "            # Output gradient: softmax + cross-entropy\n",
    "            do = self.ps[t].copy()\n",
    "            do[targets[t]] -= 1  # d(loss)/d(output)\n",
    "            \n",
    "            # Output layer gradients\n",
    "            dW_hy += np.outer(self.hs[t], do)\n",
    "            db_y += do\n",
    "            \n",
    "            # Hidden state gradient (from output AND from next timestep)\n",
    "            dh = do @ self.W_hy.T + dh_next\n",
    "            \n",
    "            # Gradient through tanh\n",
    "            dh_raw = dh * (1 - self.hs[t]**2)\n",
    "            \n",
    "            # RNN parameter gradients\n",
    "            dW_xh += np.outer(self.xs[t], dh_raw)\n",
    "            dW_hh += np.outer(self.hs[t-1], dh_raw)\n",
    "            db_h += dh_raw\n",
    "            \n",
    "            # Pass gradient to previous timestep\n",
    "            dh_next = dh_raw @ self.W_hh.T\n",
    "        \n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        for grad in [dW_xh, dW_hh, dW_hy, db_h, db_y]:\n",
    "            np.clip(grad, -5, 5, out=grad)\n",
    "        \n",
    "        return {\n",
    "            'W_xh': dW_xh, 'W_hh': dW_hh, 'W_hy': dW_hy,\n",
    "            'b_h': db_h, 'b_y': db_y\n",
    "        }\n",
    "    \n",
    "    def update(self, grads, lr):\n",
    "        \"\"\"Update parameters with gradient descent.\"\"\"\n",
    "        self.W_xh -= lr * grads['W_xh']\n",
    "        self.W_hh -= lr * grads['W_hh']\n",
    "        self.W_hy -= lr * grads['W_hy']\n",
    "        self.b_h -= lr * grads['b_h']\n",
    "        self.b_y -= lr * grads['b_y']\n",
    "    \n",
    "    def sample(self, seed_char, length, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Generate text by sampling from the model.\n",
    "        \n",
    "        Args:\n",
    "            seed_char: Starting character index\n",
    "            length: Number of characters to generate\n",
    "            temperature: Higher = more random, lower = more deterministic\n",
    "            \n",
    "        Returns:\n",
    "            List of generated character indices\n",
    "        \"\"\"\n",
    "        h = np.zeros(self.hidden_dim)\n",
    "        x = seed_char\n",
    "        generated = [x]\n",
    "        \n",
    "        for _ in range(length):\n",
    "            # One-hot encode\n",
    "            x_vec = np.zeros(self.vocab_size)\n",
    "            x_vec[x] = 1\n",
    "            \n",
    "            # Forward step\n",
    "            h = np.tanh(x_vec @ self.W_xh + h @ self.W_hh + self.b_h)\n",
    "            o = h @ self.W_hy + self.b_y\n",
    "            \n",
    "            # Sample with temperature\n",
    "            o = o / temperature\n",
    "            exp_o = np.exp(o - o.max())\n",
    "            probs = exp_o / exp_o.sum()\n",
    "            \n",
    "            x = np.random.choice(self.vocab_size, p=probs)\n",
    "            generated.append(x)\n",
    "        \n",
    "        return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Prepare Training Data\n",
    "\n",
    "We'll train on a small text dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text (you can use any text)\n",
    "text = \"\"\"To be, or not to be, that is the question:\n",
    "Whether 'tis nobler in the mind to suffer\n",
    "The slings and arrows of outrageous fortune,\n",
    "Or to take arms against a sea of troubles\n",
    "And by opposing end them. To die: to sleep;\n",
    "No more; and by a sleep to say we end\n",
    "The heart-ache and the thousand natural shocks\n",
    "That flesh is heir to. 'Tis a consummation\n",
    "Devoutly to be wish'd. To die, to sleep;\n",
    "To sleep: perchance to dream: ay, there's the rub;\n",
    "For in that sleep of death what dreams may come\n",
    "When we have shuffled off this mortal coil,\n",
    "Must give us pause.\"\"\"\n",
    "\n",
    "# Create character vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "print(f\"Text length: {len(text)} characters\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Vocabulary: {''.join(chars)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to indices\n",
    "data = [char_to_idx[ch] for ch in text]\n",
    "\n",
    "print(f\"First 50 characters: {text[:50]}\")\n",
    "print(f\"As indices: {data[:50]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 4. Training\n",
    "\n",
    "Train the RNN with truncated BPTT (process sequences in chunks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(model, data, seq_length=25, epochs=100, lr=0.1, print_every=10):\n",
    "    \"\"\"\n",
    "    Train the RNN on character data.\n",
    "    \n",
    "    Args:\n",
    "        model: CharRNN model\n",
    "        data: List of character indices\n",
    "        seq_length: Length of sequences for BPTT\n",
    "        epochs: Number of passes through data\n",
    "        lr: Learning rate\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    smooth_loss = -np.log(1.0 / model.vocab_size) * seq_length\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        h_prev = np.zeros(model.hidden_dim)\n",
    "        epoch_loss = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        # Go through data in chunks\n",
    "        for i in range(0, len(data) - seq_length - 1, seq_length):\n",
    "            inputs = data[i:i+seq_length]\n",
    "            targets = data[i+1:i+seq_length+1]\n",
    "            \n",
    "            # Forward pass\n",
    "            probs, hidden_states = model.forward(inputs, h_prev)\n",
    "            loss = model.loss(targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            grads = model.backward(targets)\n",
    "            \n",
    "            # Update\n",
    "            model.update(grads, lr)\n",
    "            \n",
    "            # Carry hidden state forward (detached)\n",
    "            h_prev = hidden_states[seq_length - 1].copy()\n",
    "            \n",
    "            epoch_loss += loss\n",
    "            n_batches += 1\n",
    "            \n",
    "            smooth_loss = 0.999 * smooth_loss + 0.001 * loss * seq_length\n",
    "        \n",
    "        avg_loss = epoch_loss / n_batches\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        if epoch % print_every == 0:\n",
    "            print(f\"Epoch {epoch:3d}: Loss = {avg_loss:.4f}\")\n",
    "            \n",
    "            # Generate sample\n",
    "            sample_idx = model.sample(data[0], 100, temperature=0.8)\n",
    "            sample_text = ''.join([idx_to_char[i] for i in sample_idx])\n",
    "            print(f\"  Sample: {sample_text[:60]}...\\n\")\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train model\n",
    "model = CharRNN(vocab_size=vocab_size, hidden_dim=100)\n",
    "\n",
    "print(\"Training RNN...\\n\")\n",
    "losses = train_rnn(model, data, seq_length=25, epochs=200, lr=0.1, print_every=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 5. Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate with different temperatures\n",
    "print(\"Generated text at different temperatures:\\n\")\n",
    "\n",
    "for temp in [0.5, 0.8, 1.0, 1.5]:\n",
    "    seed = char_to_idx['T']\n",
    "    sample_idx = model.sample(seed, 150, temperature=temp)\n",
    "    sample_text = ''.join([idx_to_char[i] for i in sample_idx])\n",
    "    print(f\"Temperature {temp}:\")\n",
    "    print(f\"  {sample_text}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "**Temperature effects:**\n",
    "- **Low (0.5):** More conservative, repeats common patterns\n",
    "- **Medium (0.8-1.0):** Balanced creativity and coherence\n",
    "- **High (1.5):** More random, might produce nonsense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 6. Visualizing Hidden States\n",
    "\n",
    "Let's see what the RNN \"remembers\" as it processes text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a sentence and visualize hidden states\n",
    "test_text = \"To be, or not to be\"\n",
    "test_data = [char_to_idx[ch] for ch in test_text]\n",
    "\n",
    "# Forward pass\n",
    "probs, hidden_states = model.forward(test_data)\n",
    "\n",
    "# Extract hidden states into array\n",
    "H = np.array([hidden_states[t] for t in range(len(test_data))])\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Heatmap of hidden states\n",
    "ax = axes[0]\n",
    "im = ax.imshow(H.T, aspect='auto', cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "ax.set_xlabel('Time step')\n",
    "ax.set_ylabel('Hidden unit')\n",
    "ax.set_title('Hidden State Evolution')\n",
    "ax.set_xticks(range(len(test_text)))\n",
    "ax.set_xticklabels(list(test_text))\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# Plot a few hidden units over time\n",
    "ax = axes[1]\n",
    "for i in [0, 10, 20, 30, 40]:\n",
    "    ax.plot(H[:, i], label=f'Unit {i}')\n",
    "ax.set_xlabel('Time step')\n",
    "ax.set_ylabel('Activation')\n",
    "ax.set_title('Selected Hidden Units Over Time')\n",
    "ax.set_xticks(range(len(test_text)))\n",
    "ax.set_xticklabels(list(test_text))\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 7. The Vanishing Gradient Problem\n",
    "\n",
    "Let's visualize how gradients decay over long sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_gradient_flow(model, data, seq_lengths):\n",
    "    \"\"\"\n",
    "    Measure how gradients flow back through different sequence lengths.\n",
    "    \"\"\"\n",
    "    gradient_norms = []\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        if seq_len >= len(data):\n",
    "            break\n",
    "            \n",
    "        inputs = data[:seq_len]\n",
    "        targets = data[1:seq_len+1]\n",
    "        \n",
    "        # Forward\n",
    "        model.forward(inputs)\n",
    "        \n",
    "        # Backward and measure gradient at first timestep\n",
    "        seq_len_actual = len(targets)\n",
    "        \n",
    "        # We'll track gradient magnitude at each timestep\n",
    "        dh_next = np.zeros(model.hidden_dim)\n",
    "        grad_magnitudes = []\n",
    "        \n",
    "        for t in reversed(range(seq_len_actual)):\n",
    "            do = model.ps[t].copy()\n",
    "            do[targets[t]] -= 1\n",
    "            \n",
    "            dh = do @ model.W_hy.T + dh_next\n",
    "            dh_raw = dh * (1 - model.hs[t]**2)\n",
    "            \n",
    "            grad_magnitudes.append(np.linalg.norm(dh_raw))\n",
    "            \n",
    "            dh_next = dh_raw @ model.W_hh.T\n",
    "        \n",
    "        # Reverse to get chronological order\n",
    "        grad_magnitudes = grad_magnitudes[::-1]\n",
    "        gradient_norms.append(grad_magnitudes)\n",
    "    \n",
    "    return gradient_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure gradient flow for different sequence lengths\n",
    "seq_lengths = [10, 25, 50, 100]\n",
    "gradient_flows = measure_gradient_flow(model, data, seq_lengths)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for seq_len, grads in zip(seq_lengths[:len(gradient_flows)], gradient_flows):\n",
    "    # Normalize to show relative decay\n",
    "    grads = np.array(grads)\n",
    "    if grads[-1] > 0:\n",
    "        grads = grads / grads[-1]  # Normalize to final (most recent) gradient\n",
    "    ax.plot(range(len(grads)), grads, label=f'Seq length {seq_len}')\n",
    "\n",
    "ax.set_xlabel('Position in sequence (from start)')\n",
    "ax.set_ylabel('Relative gradient magnitude')\n",
    "ax.set_title('Gradient Flow Through Time (Vanishing Gradient Problem)')\n",
    "ax.legend()\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGradient at position 0 vs position -1 (final):\")\n",
    "for seq_len, grads in zip(seq_lengths[:len(gradient_flows)], gradient_flows):\n",
    "    if len(grads) > 0 and grads[-1] > 0:\n",
    "        ratio = grads[0] / grads[-1]\n",
    "        print(f\"  Seq {seq_len}: {ratio:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "**Key insight:** Gradients decay exponentially as they flow back through time. For long sequences, the gradient at early timesteps becomes vanishingly small, making it hard to learn long-range dependencies.\n",
    "\n",
    "This is why LSTMs were invented (next notebook) and why transformers with attention (later notebooks) work even better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| **RNN Cell** | h_t = tanh(W_xh @ x_t + W_hh @ h_{t-1} + b) |\n",
    "| **Output** | y_t = W_hy @ h_t + b_y |\n",
    "| **BPTT** | Unroll through time, sum gradients |\n",
    "| **Gradient clipping** | Prevent exploding gradients |\n",
    "\n",
    "**Key takeaways:**\n",
    "\n",
    "1. RNNs process sequences one step at a time, maintaining a hidden state\n",
    "2. BPTT computes gradients by unrolling the network through time\n",
    "3. **Vanishing gradients** make it hard to learn long-range dependencies\n",
    "4. **Gradient clipping** prevents exploding gradients but doesn't solve vanishing\n",
    "5. Character-level models learn structure (words, punctuation) from raw characters\n",
    "\n",
    "**Next:** [06-lstm-from-scratch.ipynb](06-lstm-from-scratch.ipynb) solves the vanishing gradient problem with gating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 9. Exercises\n",
    "\n",
    "1. **Deeper RNN:** Add a second hidden layer. Does it help?\n",
    "\n",
    "2. **Different text:** Train on different text (song lyrics, code, etc.). What patterns does it learn?\n",
    "\n",
    "3. **Bidirectional:** Implement a bidirectional RNN that processes the sequence both forwards and backwards.\n",
    "\n",
    "4. **Gradient analysis:** Track which positions in the input have the most influence on the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 starter: Train on different text\n",
    "new_text = \"\"\"def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "    return True\n",
    "\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "def multiply(a, b):\n",
    "    return a * b\n",
    "\"\"\"\n",
    "\n",
    "# Your code here to train on this Python code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
